{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9X6j0HbaR6Y",
        "outputId": "cfa6302c-41d2-4e2e-f668-09bd7b1c06a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 0 Loss: 2.344553232192993\n",
            "Epoch: 0 Step: 1000 Loss: 0.019925987347960472\n",
            "Epoch: 0 Step: 2000 Loss: 0.05456148087978363\n",
            "Epoch: 0 Step: 3000 Loss: 0.05925726890563965\n",
            "Epoch: 0 Step: 4000 Loss: 0.07700435817241669\n",
            "Epoch: 0 Step: 5000 Loss: 0.054198257625103\n",
            "Epoch: 0 Step: 6000 Loss: 0.03575216233730316\n",
            "Epoch: 0 Step: 7000 Loss: 0.09703902900218964\n",
            "Epoch: 0 Step: 8000 Loss: 0.17704366147518158\n",
            "Epoch: 0 Step: 9000 Loss: 0.022009745240211487\n",
            "Epoch: 0 Step: 10000 Loss: 1.1845033168792725\n",
            "Epoch: 0 Step: 11000 Loss: 0.005539680831134319\n",
            "Epoch: 0 Step: 12000 Loss: 0.09148440510034561\n",
            "Epoch: 0 Step: 13000 Loss: 0.39787980914115906\n",
            "Epoch: 0 Step: 14000 Loss: 0.013335236348211765\n",
            "Accuracy: 0.9773\n",
            "Test Loss: 0.07461589605547488\n",
            "Epoch: 1 Step: 0 Loss: 0.006511128507554531\n",
            "Epoch: 1 Step: 1000 Loss: 0.0029558632522821426\n",
            "Epoch: 1 Step: 2000 Loss: 0.004235273692756891\n",
            "Epoch: 1 Step: 3000 Loss: 0.017132079228758812\n",
            "Epoch: 1 Step: 4000 Loss: 0.005290993023663759\n",
            "Epoch: 1 Step: 5000 Loss: 0.00298552424646914\n",
            "Epoch: 1 Step: 6000 Loss: 0.0684744343161583\n",
            "Epoch: 1 Step: 7000 Loss: 0.5526543855667114\n",
            "Epoch: 1 Step: 8000 Loss: 0.02253834158182144\n",
            "Epoch: 1 Step: 9000 Loss: 0.16881024837493896\n",
            "Epoch: 1 Step: 10000 Loss: 0.385405570268631\n",
            "Epoch: 1 Step: 11000 Loss: 0.07228785753250122\n",
            "Epoch: 1 Step: 12000 Loss: 0.0009998992318287492\n",
            "Epoch: 1 Step: 13000 Loss: 0.0010898824548348784\n",
            "Epoch: 1 Step: 14000 Loss: 0.0051566073670983315\n",
            "Accuracy: 0.9804\n",
            "Test Loss: 0.06231395327016944\n",
            "Epoch: 2 Step: 0 Loss: 0.005444832146167755\n",
            "Epoch: 2 Step: 1000 Loss: 0.00041717197746038437\n",
            "Epoch: 2 Step: 2000 Loss: 0.00040615658508613706\n",
            "Epoch: 2 Step: 3000 Loss: 0.03717915713787079\n",
            "Epoch: 2 Step: 4000 Loss: 0.0004316962440498173\n",
            "Epoch: 2 Step: 5000 Loss: 0.00047074988833628595\n",
            "Epoch: 2 Step: 6000 Loss: 0.0021393324714154005\n",
            "Epoch: 2 Step: 7000 Loss: 0.32626980543136597\n",
            "Epoch: 2 Step: 8000 Loss: 0.024496888741850853\n",
            "Epoch: 2 Step: 9000 Loss: 0.26053348183631897\n",
            "Epoch: 2 Step: 10000 Loss: 0.03188352286815643\n",
            "Epoch: 2 Step: 11000 Loss: 0.0010687339818105102\n",
            "Epoch: 2 Step: 12000 Loss: 0.07077422738075256\n",
            "Epoch: 2 Step: 13000 Loss: 0.0003283314872533083\n",
            "Epoch: 2 Step: 14000 Loss: 0.003984414041042328\n",
            "Accuracy: 0.9833\n",
            "Test Loss: 0.055456962812002165\n",
            "Epoch: 3 Step: 0 Loss: 0.0003284531703684479\n",
            "Epoch: 3 Step: 1000 Loss: 0.0028862999752163887\n",
            "Epoch: 3 Step: 2000 Loss: 0.00023465172853320837\n",
            "Epoch: 3 Step: 3000 Loss: 0.0019259101245552301\n",
            "Epoch: 3 Step: 4000 Loss: 0.0036656896118074656\n",
            "Epoch: 3 Step: 5000 Loss: 0.00028534166631288826\n",
            "Epoch: 3 Step: 6000 Loss: 0.15374159812927246\n",
            "Epoch: 3 Step: 7000 Loss: 0.007126759737730026\n",
            "Epoch: 3 Step: 8000 Loss: 0.0018016195390373468\n",
            "Epoch: 3 Step: 9000 Loss: 0.09848625212907791\n",
            "Epoch: 3 Step: 10000 Loss: 0.0007029642583802342\n",
            "Epoch: 3 Step: 11000 Loss: 0.000658454024232924\n",
            "Epoch: 3 Step: 12000 Loss: 0.007927736267447472\n",
            "Epoch: 3 Step: 13000 Loss: 0.004446759819984436\n",
            "Epoch: 3 Step: 14000 Loss: 0.0010047670220956206\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.04977163575062005\n",
            "Epoch: 4 Step: 0 Loss: 8.299470209749416e-05\n",
            "Epoch: 4 Step: 1000 Loss: 0.00038243766175583005\n",
            "Epoch: 4 Step: 2000 Loss: 0.08022464066743851\n",
            "Epoch: 4 Step: 3000 Loss: 0.009822632186114788\n",
            "Epoch: 4 Step: 4000 Loss: 0.00010167842992814258\n",
            "Epoch: 4 Step: 5000 Loss: 0.005525784566998482\n",
            "Epoch: 4 Step: 6000 Loss: 0.23228612542152405\n",
            "Epoch: 4 Step: 7000 Loss: 0.0014865633565932512\n",
            "Epoch: 4 Step: 8000 Loss: 0.0013747852062806487\n",
            "Epoch: 4 Step: 9000 Loss: 0.0035714309196919203\n",
            "Epoch: 4 Step: 10000 Loss: 0.0005934576038271189\n",
            "Epoch: 4 Step: 11000 Loss: 0.0015352844493463635\n",
            "Epoch: 4 Step: 12000 Loss: 0.010636543855071068\n",
            "Epoch: 4 Step: 13000 Loss: 0.021273238584399223\n",
            "Epoch: 4 Step: 14000 Loss: 0.004532936029136181\n",
            "Accuracy: 0.9863\n",
            "Test Loss: 0.04253492645258884\n",
            "Epoch: 5 Step: 0 Loss: 0.01360282488167286\n",
            "Epoch: 5 Step: 1000 Loss: 0.0010789813240990043\n",
            "Epoch: 5 Step: 2000 Loss: 0.0013172358740121126\n",
            "Epoch: 5 Step: 3000 Loss: 0.324545294046402\n",
            "Epoch: 5 Step: 4000 Loss: 0.003150620497763157\n",
            "Epoch: 5 Step: 5000 Loss: 0.00022362865274772048\n",
            "Epoch: 5 Step: 6000 Loss: 0.0002659168094396591\n",
            "Epoch: 5 Step: 7000 Loss: 0.00040006107883527875\n",
            "Epoch: 5 Step: 8000 Loss: 0.000628459791187197\n",
            "Epoch: 5 Step: 9000 Loss: 0.00034582344233058393\n",
            "Epoch: 5 Step: 10000 Loss: 1.754440188407898\n",
            "Epoch: 5 Step: 11000 Loss: 0.05536073073744774\n",
            "Epoch: 5 Step: 12000 Loss: 0.0012096189893782139\n",
            "Epoch: 5 Step: 13000 Loss: 0.0003197787737008184\n",
            "Epoch: 5 Step: 14000 Loss: 0.015931928530335426\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.04693542118980477\n",
            "Epoch: 6 Step: 0 Loss: 8.51406148285605e-05\n",
            "Epoch: 6 Step: 1000 Loss: 1.166308045387268\n",
            "Epoch: 6 Step: 2000 Loss: 0.010384229943156242\n",
            "Epoch: 6 Step: 3000 Loss: 0.010944518260657787\n",
            "Epoch: 6 Step: 4000 Loss: 0.0006784826400689781\n",
            "Epoch: 6 Step: 5000 Loss: 0.0042254021391272545\n",
            "Epoch: 6 Step: 6000 Loss: 0.00011261412873864174\n",
            "Epoch: 6 Step: 7000 Loss: 0.0003341560368426144\n",
            "Epoch: 6 Step: 8000 Loss: 0.001194898970425129\n",
            "Epoch: 6 Step: 9000 Loss: 0.0001180656545329839\n",
            "Epoch: 6 Step: 10000 Loss: 0.0003059926675632596\n",
            "Epoch: 6 Step: 11000 Loss: 0.0007705499301664531\n",
            "Epoch: 6 Step: 12000 Loss: 0.000559261126909405\n",
            "Epoch: 6 Step: 13000 Loss: 6.943653716007248e-05\n",
            "Epoch: 6 Step: 14000 Loss: 0.011759378015995026\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.050065988243764147\n",
            "Epoch: 7 Step: 0 Loss: 0.011856733821332455\n",
            "Epoch: 7 Step: 1000 Loss: 0.00023160123964771628\n",
            "Epoch: 7 Step: 2000 Loss: 0.002498825080692768\n",
            "Epoch: 7 Step: 3000 Loss: 1.1545919179916382\n",
            "Epoch: 7 Step: 4000 Loss: 0.00011186392657691613\n",
            "Epoch: 7 Step: 5000 Loss: 0.00675076013430953\n",
            "Epoch: 7 Step: 6000 Loss: 1.832820998970419e-05\n",
            "Epoch: 7 Step: 7000 Loss: 0.00020332283747848123\n",
            "Epoch: 7 Step: 8000 Loss: 0.014154895208775997\n",
            "Epoch: 7 Step: 9000 Loss: 0.00046929012751206756\n",
            "Epoch: 7 Step: 10000 Loss: 0.11359482258558273\n",
            "Epoch: 7 Step: 11000 Loss: 0.07644892483949661\n",
            "Epoch: 7 Step: 12000 Loss: 0.00015587896632496268\n",
            "Epoch: 7 Step: 13000 Loss: 0.0005541727296076715\n",
            "Epoch: 7 Step: 14000 Loss: 0.0002715351292863488\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.05371462226788499\n",
            "Epoch: 8 Step: 0 Loss: 0.00014904177805874497\n",
            "Epoch: 8 Step: 1000 Loss: 0.0005920784315094352\n",
            "Epoch: 8 Step: 2000 Loss: 0.03999592736363411\n",
            "Epoch: 8 Step: 3000 Loss: 0.00020774846780113876\n",
            "Epoch: 8 Step: 4000 Loss: 0.000187737459782511\n",
            "Epoch: 8 Step: 5000 Loss: 8.081361738732085e-05\n",
            "Epoch: 8 Step: 6000 Loss: 5.319413321558386e-05\n",
            "Epoch: 8 Step: 7000 Loss: 0.0001698799169389531\n",
            "Epoch: 8 Step: 8000 Loss: 7.53330095903948e-05\n",
            "Epoch: 8 Step: 9000 Loss: 0.0023893509060144424\n",
            "Epoch: 8 Step: 10000 Loss: 0.07747617363929749\n",
            "Epoch: 8 Step: 11000 Loss: 0.0002205531345680356\n",
            "Epoch: 8 Step: 12000 Loss: 7.64962169341743e-05\n",
            "Epoch: 8 Step: 13000 Loss: 0.0003007383202202618\n",
            "Epoch: 8 Step: 14000 Loss: 0.00012822281860280782\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.05113563359435148\n",
            "Epoch: 9 Step: 0 Loss: 0.002805226482450962\n",
            "Epoch: 9 Step: 1000 Loss: 0.044467076659202576\n",
            "Epoch: 9 Step: 2000 Loss: 0.0005700718029402196\n",
            "Epoch: 9 Step: 3000 Loss: 0.00024269099230878055\n",
            "Epoch: 9 Step: 4000 Loss: 0.00015626332606188953\n",
            "Epoch: 9 Step: 5000 Loss: 0.000156237103510648\n",
            "Epoch: 9 Step: 6000 Loss: 5.912506821914576e-05\n",
            "Epoch: 9 Step: 7000 Loss: 0.0004142892430536449\n",
            "Epoch: 9 Step: 8000 Loss: 0.00012812187196686864\n",
            "Epoch: 9 Step: 9000 Loss: 5.6739430874586105e-05\n",
            "Epoch: 9 Step: 10000 Loss: 2.3096485165297054e-05\n",
            "Epoch: 9 Step: 11000 Loss: 0.0003977391461376101\n",
            "Epoch: 9 Step: 12000 Loss: 0.00017615266551729292\n",
            "Epoch: 9 Step: 13000 Loss: 0.00188357790466398\n",
            "Epoch: 9 Step: 14000 Loss: 0.0038585644215345383\n",
            "Accuracy: 0.9849\n",
            "Test Loss: 0.05227248437899161\n",
            "Epoch: 10 Step: 0 Loss: 0.00013749486242886633\n",
            "Epoch: 10 Step: 1000 Loss: 6.597566971322522e-05\n",
            "Epoch: 10 Step: 2000 Loss: 0.021483831107616425\n",
            "Epoch: 10 Step: 3000 Loss: 3.7310965126380324e-05\n",
            "Epoch: 10 Step: 4000 Loss: 0.00044409395195543766\n",
            "Epoch: 10 Step: 5000 Loss: 0.00038492874591611326\n",
            "Epoch: 10 Step: 6000 Loss: 0.00071675144135952\n",
            "Epoch: 10 Step: 7000 Loss: 2.9324904971872456e-05\n",
            "Epoch: 10 Step: 8000 Loss: 0.0003762496344279498\n",
            "Epoch: 10 Step: 9000 Loss: 0.00030109306680969894\n",
            "Epoch: 10 Step: 10000 Loss: 0.07340879738330841\n",
            "Epoch: 10 Step: 11000 Loss: 0.00017136690439656377\n",
            "Epoch: 10 Step: 12000 Loss: 0.0002802627859637141\n",
            "Epoch: 10 Step: 13000 Loss: 0.00014683534391224384\n",
            "Epoch: 10 Step: 14000 Loss: 0.0005294035654515028\n",
            "Accuracy: 0.9834\n",
            "Test Loss: 0.05727178411953173\n",
            "Epoch: 11 Step: 0 Loss: 0.00022134659229777753\n",
            "Epoch: 11 Step: 1000 Loss: 0.004534982144832611\n",
            "Epoch: 11 Step: 2000 Loss: 1.0162484613829292e-05\n",
            "Epoch: 11 Step: 3000 Loss: 0.004360940307378769\n",
            "Epoch: 11 Step: 4000 Loss: 0.0005046493606641889\n",
            "Epoch: 11 Step: 5000 Loss: 1.4245365491660777e-05\n",
            "Epoch: 11 Step: 6000 Loss: 0.00017931910406332463\n",
            "Epoch: 11 Step: 7000 Loss: 0.00013057858450338244\n",
            "Epoch: 11 Step: 8000 Loss: 0.10884606838226318\n",
            "Epoch: 11 Step: 9000 Loss: 0.006280801258981228\n",
            "Epoch: 11 Step: 10000 Loss: 0.6509559750556946\n",
            "Epoch: 11 Step: 11000 Loss: 0.000828864227514714\n",
            "Epoch: 11 Step: 12000 Loss: 0.0018405583687126637\n",
            "Epoch: 11 Step: 13000 Loss: 0.005500073079019785\n",
            "Epoch: 11 Step: 14000 Loss: 0.00012748103472404182\n",
            "Accuracy: 0.9878\n",
            "Test Loss: 0.044830122753230534\n",
            "Epoch: 12 Step: 0 Loss: 0.00026030041044577956\n",
            "Epoch: 12 Step: 1000 Loss: 0.00528531800955534\n",
            "Epoch: 12 Step: 2000 Loss: 0.0010670834453776479\n",
            "Epoch: 12 Step: 3000 Loss: 0.015655281022191048\n",
            "Epoch: 12 Step: 4000 Loss: 0.0005129047203809023\n",
            "Epoch: 12 Step: 5000 Loss: 0.8014240860939026\n",
            "Epoch: 12 Step: 6000 Loss: 0.002081956947222352\n",
            "Epoch: 12 Step: 7000 Loss: 0.0002479538961779326\n",
            "Epoch: 12 Step: 8000 Loss: 0.001796818571165204\n",
            "Epoch: 12 Step: 9000 Loss: 2.831166420946829e-05\n",
            "Epoch: 12 Step: 10000 Loss: 0.02670135907828808\n",
            "Epoch: 12 Step: 11000 Loss: 2.9234870453365147e-05\n",
            "Epoch: 12 Step: 12000 Loss: 0.00395934097468853\n",
            "Epoch: 12 Step: 13000 Loss: 0.0013910982524976134\n",
            "Epoch: 12 Step: 14000 Loss: 0.016409631818532944\n",
            "Accuracy: 0.985\n",
            "Test Loss: 0.05282119862343297\n",
            "Epoch: 13 Step: 0 Loss: 0.0002668632077984512\n",
            "Epoch: 13 Step: 1000 Loss: 4.231717684888281e-05\n",
            "Epoch: 13 Step: 2000 Loss: 7.986365380929783e-05\n",
            "Epoch: 13 Step: 3000 Loss: 0.0018192494753748178\n",
            "Epoch: 13 Step: 4000 Loss: 0.006334515288472176\n",
            "Epoch: 13 Step: 5000 Loss: 1.0400927749287803e-05\n",
            "Epoch: 13 Step: 6000 Loss: 3.507517612888478e-05\n",
            "Epoch: 13 Step: 7000 Loss: 0.0015923059545457363\n",
            "Epoch: 13 Step: 8000 Loss: 0.0002426284772809595\n",
            "Epoch: 13 Step: 9000 Loss: 0.00010629466851241887\n",
            "Epoch: 13 Step: 10000 Loss: 0.00022623517725151032\n",
            "Epoch: 13 Step: 11000 Loss: 0.00033328915014863014\n",
            "Epoch: 13 Step: 12000 Loss: 9.937489812728018e-05\n",
            "Epoch: 13 Step: 13000 Loss: 1.3083116755296942e-05\n",
            "Epoch: 13 Step: 14000 Loss: 0.0009898993885144591\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.04753813452774011\n",
            "Epoch: 14 Step: 0 Loss: 0.005428381729871035\n",
            "Epoch: 14 Step: 1000 Loss: 0.0009628567495383322\n",
            "Epoch: 14 Step: 2000 Loss: 8.129439083859324e-05\n",
            "Epoch: 14 Step: 3000 Loss: 3.17981030093506e-05\n",
            "Epoch: 14 Step: 4000 Loss: 0.0002498200919944793\n",
            "Epoch: 14 Step: 5000 Loss: 4.934870594297536e-05\n",
            "Epoch: 14 Step: 6000 Loss: 3.188771370332688e-05\n",
            "Epoch: 14 Step: 7000 Loss: 6.317542283795774e-05\n",
            "Epoch: 14 Step: 8000 Loss: 0.042084239423274994\n",
            "Epoch: 14 Step: 9000 Loss: 4.52088970632758e-05\n",
            "Epoch: 14 Step: 10000 Loss: 0.00011945700680371374\n",
            "Epoch: 14 Step: 11000 Loss: 0.0011304488871246576\n",
            "Epoch: 14 Step: 12000 Loss: 0.00030773293110542\n",
            "Epoch: 14 Step: 13000 Loss: 0.11096644401550293\n",
            "Epoch: 14 Step: 14000 Loss: 2.1248799384920858e-05\n",
            "Accuracy: 0.9847\n",
            "Test Loss: 0.05520109772778733\n",
            "Epoch: 15 Step: 0 Loss: 0.0019269660115242004\n",
            "Epoch: 15 Step: 1000 Loss: 0.0013052185531705618\n",
            "Epoch: 15 Step: 2000 Loss: 0.0007312637753784657\n",
            "Epoch: 15 Step: 3000 Loss: 0.0006074968841858208\n",
            "Epoch: 15 Step: 4000 Loss: 0.0002766534162219614\n",
            "Epoch: 15 Step: 5000 Loss: 0.018654095008969307\n",
            "Epoch: 15 Step: 6000 Loss: 0.00010715817916207016\n",
            "Epoch: 15 Step: 7000 Loss: 8.463764061161783e-06\n",
            "Epoch: 15 Step: 8000 Loss: 1.4096320228418335e-05\n",
            "Epoch: 15 Step: 9000 Loss: 2.7328173018759117e-05\n",
            "Epoch: 15 Step: 10000 Loss: 0.00014505005674436688\n",
            "Epoch: 15 Step: 11000 Loss: 1.2755183888657484e-05\n",
            "Epoch: 15 Step: 12000 Loss: 0.0037352193612605333\n",
            "Epoch: 15 Step: 13000 Loss: 0.0003125962393824011\n",
            "Epoch: 15 Step: 14000 Loss: 0.0001206455344799906\n",
            "Accuracy: 0.9846\n",
            "Test Loss: 0.055279722050566076\n",
            "Epoch: 16 Step: 0 Loss: 1.153327502834145e-05\n",
            "Epoch: 16 Step: 1000 Loss: 0.060570936650037766\n",
            "Epoch: 16 Step: 2000 Loss: 8.114680531434715e-05\n",
            "Epoch: 16 Step: 3000 Loss: 3.5464599932311103e-06\n",
            "Epoch: 16 Step: 4000 Loss: 0.00023169018095359206\n",
            "Epoch: 16 Step: 5000 Loss: 0.02092602476477623\n",
            "Epoch: 16 Step: 6000 Loss: 0.0004625475557986647\n",
            "Epoch: 16 Step: 7000 Loss: 0.00019790465012192726\n",
            "Epoch: 16 Step: 8000 Loss: 0.0067488402128219604\n",
            "Epoch: 16 Step: 9000 Loss: 0.00013807890354655683\n",
            "Epoch: 16 Step: 10000 Loss: 3.933788320864551e-05\n",
            "Epoch: 16 Step: 11000 Loss: 3.936685970984399e-05\n",
            "Epoch: 16 Step: 12000 Loss: 0.0003677237837109715\n",
            "Epoch: 16 Step: 13000 Loss: 0.0005954787484370172\n",
            "Epoch: 16 Step: 14000 Loss: 0.08973951637744904\n",
            "Accuracy: 0.9851\n",
            "Test Loss: 0.05981684288226386\n",
            "Epoch: 17 Step: 0 Loss: 3.537435259204358e-05\n",
            "Epoch: 17 Step: 1000 Loss: 4.967642234987579e-05\n",
            "Epoch: 17 Step: 2000 Loss: 3.948606172343716e-05\n",
            "Epoch: 17 Step: 3000 Loss: 0.00038382565253414214\n",
            "Epoch: 17 Step: 4000 Loss: 3.34070464305114e-05\n",
            "Epoch: 17 Step: 5000 Loss: 1.3112870874465443e-05\n",
            "Epoch: 17 Step: 6000 Loss: 2.0861593839072157e-06\n",
            "Epoch: 17 Step: 7000 Loss: 3.6656742850027513e-06\n",
            "Epoch: 17 Step: 8000 Loss: 9.857266559265554e-05\n",
            "Epoch: 17 Step: 9000 Loss: 0.013203748501837254\n",
            "Epoch: 17 Step: 10000 Loss: 0.0134733272716403\n",
            "Epoch: 17 Step: 11000 Loss: 0.0005404215189628303\n",
            "Epoch: 17 Step: 12000 Loss: 5.4653879487887025e-05\n",
            "Epoch: 17 Step: 13000 Loss: 0.005298207979649305\n",
            "Epoch: 17 Step: 14000 Loss: 0.0019689002074301243\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.052007470647988815\n",
            "Epoch: 18 Step: 0 Loss: 5.939046968705952e-05\n",
            "Epoch: 18 Step: 1000 Loss: 3.12315569317434e-05\n",
            "Epoch: 18 Step: 2000 Loss: 0.00010427083179820329\n",
            "Epoch: 18 Step: 3000 Loss: 5.447318108053878e-05\n",
            "Epoch: 18 Step: 4000 Loss: 3.665673148134374e-06\n",
            "Epoch: 18 Step: 5000 Loss: 0.0003930027305614203\n",
            "Epoch: 18 Step: 6000 Loss: 4.717322735814378e-05\n",
            "Epoch: 18 Step: 7000 Loss: 4.684532541432418e-05\n",
            "Epoch: 18 Step: 8000 Loss: 5.900834821659373e-06\n",
            "Epoch: 18 Step: 9000 Loss: 1.43051022405416e-06\n",
            "Epoch: 18 Step: 10000 Loss: 0.00665289256721735\n",
            "Epoch: 18 Step: 11000 Loss: 0.0005825962289236486\n",
            "Epoch: 18 Step: 12000 Loss: 9.357805538456887e-06\n",
            "Epoch: 18 Step: 13000 Loss: 0.0014911871403455734\n",
            "Epoch: 18 Step: 14000 Loss: 0.0002537357504479587\n",
            "Accuracy: 0.9844\n",
            "Test Loss: 0.05916103329405969\n",
            "Epoch: 19 Step: 0 Loss: 2.0384486560942605e-05\n",
            "Epoch: 19 Step: 1000 Loss: 0.0014851262094452977\n",
            "Epoch: 19 Step: 2000 Loss: 0.1551346778869629\n",
            "Epoch: 19 Step: 3000 Loss: 0.0001039613998727873\n",
            "Epoch: 19 Step: 4000 Loss: 0.00028690072940662503\n",
            "Epoch: 19 Step: 5000 Loss: 4.094527685083449e-05\n",
            "Epoch: 19 Step: 6000 Loss: 0.00707996403798461\n",
            "Epoch: 19 Step: 7000 Loss: 0.0051689837127923965\n",
            "Epoch: 19 Step: 8000 Loss: 5.9330210206098855e-05\n",
            "Epoch: 19 Step: 9000 Loss: 5.4682375775882974e-05\n",
            "Epoch: 19 Step: 10000 Loss: 0.0030773573089390993\n",
            "Epoch: 19 Step: 11000 Loss: 0.0004670348425861448\n",
            "Epoch: 19 Step: 12000 Loss: 0.0026410676073282957\n",
            "Epoch: 19 Step: 13000 Loss: 0.0002215198619524017\n",
            "Epoch: 19 Step: 14000 Loss: 0.0003086983342655003\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.05250499911414879\n",
            "Epoch: 20 Step: 0 Loss: 9.693018364487216e-05\n",
            "Epoch: 20 Step: 1000 Loss: 7.35434441594407e-05\n",
            "Epoch: 20 Step: 2000 Loss: 6.23135274508968e-05\n",
            "Epoch: 20 Step: 3000 Loss: 0.00045873975614085793\n",
            "Epoch: 20 Step: 4000 Loss: 0.002656250260770321\n",
            "Epoch: 20 Step: 5000 Loss: 3.4717621019808576e-05\n",
            "Epoch: 20 Step: 6000 Loss: 4.0083185012917966e-05\n",
            "Epoch: 20 Step: 7000 Loss: 6.726069113938138e-05\n",
            "Epoch: 20 Step: 8000 Loss: 2.509250771254301e-05\n",
            "Epoch: 20 Step: 9000 Loss: 8.448238804703578e-05\n",
            "Epoch: 20 Step: 10000 Loss: 0.00018986078794114292\n",
            "Epoch: 20 Step: 11000 Loss: 3.03678370983107e-05\n",
            "Epoch: 20 Step: 12000 Loss: 0.00014636186824645847\n",
            "Epoch: 20 Step: 13000 Loss: 0.00016530349967069924\n",
            "Epoch: 20 Step: 14000 Loss: 0.007155913859605789\n",
            "Accuracy: 0.9875\n",
            "Test Loss: 0.05126768873316857\n",
            "Epoch: 21 Step: 0 Loss: 0.000329183938447386\n",
            "Epoch: 21 Step: 1000 Loss: 0.00010190688772127032\n",
            "Epoch: 21 Step: 2000 Loss: 5.528066321858205e-05\n",
            "Epoch: 21 Step: 3000 Loss: 5.8439203712623566e-05\n",
            "Epoch: 21 Step: 4000 Loss: 1.0311474397894926e-05\n",
            "Epoch: 21 Step: 5000 Loss: 3.457058483036235e-06\n",
            "Epoch: 21 Step: 6000 Loss: 8.851211532601155e-06\n",
            "Epoch: 21 Step: 7000 Loss: 0.00011701826588250697\n",
            "Epoch: 21 Step: 8000 Loss: 5.900832093175268e-06\n",
            "Epoch: 21 Step: 9000 Loss: 0.04309498518705368\n",
            "Epoch: 21 Step: 10000 Loss: 7.629324045410613e-06\n",
            "Epoch: 21 Step: 11000 Loss: 5.143471935298294e-05\n",
            "Epoch: 21 Step: 12000 Loss: 0.0002541788562666625\n",
            "Epoch: 21 Step: 13000 Loss: 1.1116155292256735e-05\n",
            "Epoch: 21 Step: 14000 Loss: 0.6705156564712524\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.05613134273731721\n",
            "Epoch: 22 Step: 0 Loss: 0.002014812780544162\n",
            "Epoch: 22 Step: 1000 Loss: 0.0001540328230476007\n",
            "Epoch: 22 Step: 2000 Loss: 5.018277443014085e-05\n",
            "Epoch: 22 Step: 3000 Loss: 0.00011263287160545588\n",
            "Epoch: 22 Step: 4000 Loss: 1.364920535706915e-05\n",
            "Epoch: 22 Step: 5000 Loss: 9.536680408928078e-06\n",
            "Epoch: 22 Step: 6000 Loss: 0.11348935961723328\n",
            "Epoch: 22 Step: 7000 Loss: 0.00024946671328507364\n",
            "Epoch: 22 Step: 8000 Loss: 0.00014164784806780517\n",
            "Epoch: 22 Step: 9000 Loss: 0.005932930391281843\n",
            "Epoch: 22 Step: 10000 Loss: 0.0008249868988059461\n",
            "Epoch: 22 Step: 11000 Loss: 0.00015680356591474265\n",
            "Epoch: 22 Step: 12000 Loss: 9.877560660243034e-05\n",
            "Epoch: 22 Step: 13000 Loss: 0.0007590531604364514\n",
            "Epoch: 22 Step: 14000 Loss: 0.001492407638579607\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.054612507792026165\n",
            "Epoch: 23 Step: 0 Loss: 8.105277083814144e-05\n",
            "Epoch: 23 Step: 1000 Loss: 1.7881361600302625e-06\n",
            "Epoch: 23 Step: 2000 Loss: 0.000704561360180378\n",
            "Epoch: 23 Step: 3000 Loss: 0.00037420171429403126\n",
            "Epoch: 23 Step: 4000 Loss: 7.629325864400016e-06\n",
            "Epoch: 23 Step: 5000 Loss: 3.069478407269344e-05\n",
            "Epoch: 23 Step: 6000 Loss: 0.019466381520032883\n",
            "Epoch: 23 Step: 7000 Loss: 0.0041011031717062\n",
            "Epoch: 23 Step: 8000 Loss: 2.1010040654800832e-05\n",
            "Epoch: 23 Step: 9000 Loss: 4.0826245822245255e-05\n",
            "Epoch: 23 Step: 10000 Loss: 1.224847073899582e-05\n",
            "Epoch: 23 Step: 11000 Loss: 0.16839462518692017\n",
            "Epoch: 23 Step: 12000 Loss: 0.026101792231202126\n",
            "Epoch: 23 Step: 13000 Loss: 0.004474792629480362\n",
            "Epoch: 23 Step: 14000 Loss: 5.125983534526313e-06\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.05388863294100746\n",
            "Epoch: 24 Step: 0 Loss: 2.765591852949001e-05\n",
            "Epoch: 24 Step: 1000 Loss: 0.00048284398508258164\n",
            "Epoch: 24 Step: 2000 Loss: 0.0009350351174362004\n",
            "Epoch: 24 Step: 3000 Loss: 1.0162520084122662e-05\n",
            "Epoch: 24 Step: 4000 Loss: 7.3611008701846e-06\n",
            "Epoch: 24 Step: 5000 Loss: 0.00423834566026926\n",
            "Epoch: 24 Step: 6000 Loss: 8.821431038086303e-06\n",
            "Epoch: 24 Step: 7000 Loss: 2.428779589536134e-05\n",
            "Epoch: 24 Step: 8000 Loss: 2.929413312813267e-05\n",
            "Epoch: 24 Step: 9000 Loss: 2.9055778213660233e-05\n",
            "Epoch: 24 Step: 10000 Loss: 2.095073796226643e-05\n",
            "Epoch: 24 Step: 11000 Loss: 3.0696296562382486e-06\n",
            "Epoch: 24 Step: 12000 Loss: 2.2947318939259276e-05\n",
            "Epoch: 24 Step: 13000 Loss: 2.9206166800577193e-06\n",
            "Epoch: 24 Step: 14000 Loss: 3.763764470932074e-05\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.058857488093857865\n",
            "Epoch: 25 Step: 0 Loss: 4.872450517723337e-05\n",
            "Epoch: 25 Step: 1000 Loss: 2.4288132408400998e-05\n",
            "Epoch: 25 Step: 2000 Loss: 0.0006816477980464697\n",
            "Epoch: 25 Step: 3000 Loss: 7.104375254129991e-05\n",
            "Epoch: 25 Step: 4000 Loss: 0.0006047204951755702\n",
            "Epoch: 25 Step: 5000 Loss: 3.8801335904281586e-05\n",
            "Epoch: 25 Step: 6000 Loss: 0.0075158616527915\n",
            "Epoch: 25 Step: 7000 Loss: 2.7120051981910365e-06\n",
            "Epoch: 25 Step: 8000 Loss: 2.5212246328010224e-05\n",
            "Epoch: 25 Step: 9000 Loss: 5.66210292163305e-05\n",
            "Epoch: 25 Step: 10000 Loss: 0.006970606278628111\n",
            "Epoch: 25 Step: 11000 Loss: 0.0002238236047560349\n",
            "Epoch: 25 Step: 12000 Loss: 1.4126075257081538e-05\n",
            "Epoch: 25 Step: 13000 Loss: 0.0001398168969899416\n",
            "Epoch: 25 Step: 14000 Loss: 8.582319424021989e-05\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.05809466390949142\n",
            "Epoch: 26 Step: 0 Loss: 9.404723823536187e-05\n",
            "Epoch: 26 Step: 1000 Loss: 0.0004068477137479931\n",
            "Epoch: 26 Step: 2000 Loss: 0.0006316277431324124\n",
            "Epoch: 26 Step: 3000 Loss: 3.48686330653436e-06\n",
            "Epoch: 26 Step: 4000 Loss: 2.9055798222543672e-05\n",
            "Epoch: 26 Step: 5000 Loss: 0.02159770019352436\n",
            "Epoch: 26 Step: 6000 Loss: 2.3661979867029004e-05\n",
            "Epoch: 26 Step: 7000 Loss: 0.0007040852797217667\n",
            "Epoch: 26 Step: 8000 Loss: 0.00010331240628147498\n",
            "Epoch: 26 Step: 9000 Loss: 0.0011458361987024546\n",
            "Epoch: 26 Step: 10000 Loss: 7.283339800778776e-05\n",
            "Epoch: 26 Step: 11000 Loss: 0.00013718384434469044\n",
            "Epoch: 26 Step: 12000 Loss: 5.429521115729585e-05\n",
            "Epoch: 26 Step: 13000 Loss: 0.9784117341041565\n",
            "Epoch: 26 Step: 14000 Loss: 2.1576399376499467e-05\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.05619444650894404\n",
            "Epoch: 27 Step: 0 Loss: 0.00012330601748544723\n",
            "Epoch: 27 Step: 1000 Loss: 0.0025735043454915285\n",
            "Epoch: 27 Step: 2000 Loss: 2.2589554646401666e-05\n",
            "Epoch: 27 Step: 3000 Loss: 1.2427373803802766e-05\n",
            "Epoch: 27 Step: 4000 Loss: 2.270847835461609e-05\n",
            "Epoch: 27 Step: 5000 Loss: 9.000179488793947e-06\n",
            "Epoch: 27 Step: 6000 Loss: 4.5385681005427614e-05\n",
            "Epoch: 27 Step: 7000 Loss: 0.00022058887407183647\n",
            "Epoch: 27 Step: 8000 Loss: 0.00013801196473650634\n",
            "Epoch: 27 Step: 9000 Loss: 0.00020016232156194746\n",
            "Epoch: 27 Step: 10000 Loss: 0.8674571514129639\n",
            "Epoch: 27 Step: 11000 Loss: 0.005754024721682072\n",
            "Epoch: 27 Step: 12000 Loss: 0.05549071356654167\n",
            "Epoch: 27 Step: 13000 Loss: 1.6003505152184516e-05\n",
            "Epoch: 27 Step: 14000 Loss: 0.0019213893683627248\n",
            "Accuracy: 0.9868\n",
            "Test Loss: 0.058093109547800996\n",
            "Epoch: 28 Step: 0 Loss: 4.529925718088634e-06\n",
            "Epoch: 28 Step: 1000 Loss: 4.5981360017322004e-05\n",
            "Epoch: 28 Step: 2000 Loss: 1.1056508810725063e-05\n",
            "Epoch: 28 Step: 3000 Loss: 0.022166047245264053\n",
            "Epoch: 28 Step: 4000 Loss: 1.9371468624740373e-06\n",
            "Epoch: 28 Step: 5000 Loss: 1.916240762511734e-05\n",
            "Epoch: 28 Step: 6000 Loss: 8.781979704508558e-05\n",
            "Epoch: 28 Step: 7000 Loss: 1.7344744264846668e-05\n",
            "Epoch: 28 Step: 8000 Loss: 1.7195385225932114e-05\n",
            "Epoch: 28 Step: 9000 Loss: 0.005631382577121258\n",
            "Epoch: 28 Step: 10000 Loss: 1.0669052244338673e-05\n",
            "Epoch: 28 Step: 11000 Loss: 5.572977443080163e-06\n",
            "Epoch: 28 Step: 12000 Loss: 2.5986537366406992e-05\n",
            "Epoch: 28 Step: 13000 Loss: 0.00040344460285268724\n",
            "Epoch: 28 Step: 14000 Loss: 0.19481709599494934\n",
            "Accuracy: 0.9846\n",
            "Test Loss: 0.06433062265592474\n",
            "Epoch: 29 Step: 0 Loss: 0.007439387962222099\n",
            "Epoch: 29 Step: 1000 Loss: 3.1888362173049245e-06\n",
            "Epoch: 29 Step: 2000 Loss: 2.3334439902100712e-05\n",
            "Epoch: 29 Step: 3000 Loss: 3.316886068205349e-05\n",
            "Epoch: 29 Step: 4000 Loss: 0.0009169850964099169\n",
            "Epoch: 29 Step: 5000 Loss: 3.877191920764744e-05\n",
            "Epoch: 29 Step: 6000 Loss: 0.0004215996013954282\n",
            "Epoch: 29 Step: 7000 Loss: 0.0012649411801248789\n",
            "Epoch: 29 Step: 8000 Loss: 2.1069539798190817e-05\n",
            "Epoch: 29 Step: 9000 Loss: 0.0002064166183117777\n",
            "Epoch: 29 Step: 10000 Loss: 4.5895403673057444e-06\n",
            "Epoch: 29 Step: 11000 Loss: 0.0004638430254999548\n",
            "Epoch: 29 Step: 12000 Loss: 0.07693228125572205\n",
            "Epoch: 29 Step: 13000 Loss: 6.407471119018737e-06\n",
            "Epoch: 29 Step: 14000 Loss: 3.5582306736614555e-05\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.05452629201184801\n",
            "Epoch: 30 Step: 0 Loss: 6.755604408681393e-05\n",
            "Epoch: 30 Step: 1000 Loss: 0.00020966344163753092\n",
            "Epoch: 30 Step: 2000 Loss: 0.00016901751223485917\n",
            "Epoch: 30 Step: 3000 Loss: 1.8745247871265747e-05\n",
            "Epoch: 30 Step: 4000 Loss: 3.7221561797196046e-05\n",
            "Epoch: 30 Step: 5000 Loss: 6.022789602866396e-05\n",
            "Epoch: 30 Step: 6000 Loss: 0.0005887229926884174\n",
            "Epoch: 30 Step: 7000 Loss: 0.011992726475000381\n",
            "Epoch: 30 Step: 8000 Loss: 8.805191464489326e-05\n",
            "Epoch: 30 Step: 9000 Loss: 0.002391108777374029\n",
            "Epoch: 30 Step: 10000 Loss: 4.78879883303307e-05\n",
            "Epoch: 30 Step: 11000 Loss: 0.00013647967716678977\n",
            "Epoch: 30 Step: 12000 Loss: 3.48686330653436e-06\n",
            "Epoch: 30 Step: 13000 Loss: 0.017807990312576294\n",
            "Epoch: 30 Step: 14000 Loss: 0.0009749806486070156\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.054703940365275115\n",
            "Epoch: 31 Step: 0 Loss: 1.1443955372669734e-05\n",
            "Epoch: 31 Step: 1000 Loss: 0.00022461486514657736\n",
            "Epoch: 31 Step: 2000 Loss: 0.0014752086717635393\n",
            "Epoch: 31 Step: 3000 Loss: 3.063568874495104e-05\n",
            "Epoch: 31 Step: 4000 Loss: 0.005600335542112589\n",
            "Epoch: 31 Step: 5000 Loss: 1.8065719604492188\n",
            "Epoch: 31 Step: 6000 Loss: 1.4305096556199715e-06\n",
            "Epoch: 31 Step: 7000 Loss: 1.3082990335533395e-05\n",
            "Epoch: 31 Step: 8000 Loss: 0.00043491419637575746\n",
            "Epoch: 31 Step: 9000 Loss: 0.006055800244212151\n",
            "Epoch: 31 Step: 10000 Loss: 0.0010048941476270556\n",
            "Epoch: 31 Step: 11000 Loss: 0.001336009125225246\n",
            "Epoch: 31 Step: 12000 Loss: 0.00015483872266486287\n",
            "Epoch: 31 Step: 13000 Loss: 4.49398867203854e-05\n",
            "Epoch: 31 Step: 14000 Loss: 0.0005384493269957602\n",
            "Accuracy: 0.9858\n",
            "Test Loss: 0.06126109281765644\n",
            "Epoch: 32 Step: 0 Loss: 0.00015607828390784562\n",
            "Epoch: 32 Step: 1000 Loss: 0.005345500074326992\n",
            "Epoch: 32 Step: 2000 Loss: 0.0001599303213879466\n",
            "Epoch: 32 Step: 3000 Loss: 6.538010347867385e-05\n",
            "Epoch: 32 Step: 4000 Loss: 1.7106212908402085e-05\n",
            "Epoch: 32 Step: 5000 Loss: 1.6540041542612016e-05\n",
            "Epoch: 32 Step: 6000 Loss: 3.5464606753521366e-06\n",
            "Epoch: 32 Step: 7000 Loss: 1.9669147150125355e-05\n",
            "Epoch: 32 Step: 8000 Loss: 1.8208842448075302e-05\n",
            "Epoch: 32 Step: 9000 Loss: 4.4703469370688254e-07\n",
            "Epoch: 32 Step: 10000 Loss: 0.05044599995017052\n",
            "Epoch: 32 Step: 11000 Loss: 3.3317337511107326e-05\n",
            "Epoch: 32 Step: 12000 Loss: 8.255194188677706e-06\n",
            "Epoch: 32 Step: 13000 Loss: 6.07962056164979e-06\n",
            "Epoch: 32 Step: 14000 Loss: 3.3080509638239164e-06\n",
            "Accuracy: 0.984\n",
            "Test Loss: 0.06556085214095889\n",
            "Epoch: 33 Step: 0 Loss: 0.0009145786752924323\n",
            "Epoch: 33 Step: 1000 Loss: 0.00012359151151031256\n",
            "Epoch: 33 Step: 2000 Loss: 0.000500936817843467\n",
            "Epoch: 33 Step: 3000 Loss: 7.748598136458895e-07\n",
            "Epoch: 33 Step: 4000 Loss: 1.1473813174234238e-05\n",
            "Epoch: 33 Step: 5000 Loss: 1.2457157936296426e-05\n",
            "Epoch: 33 Step: 6000 Loss: 0.0029298425652086735\n",
            "Epoch: 33 Step: 7000 Loss: 0.016579126939177513\n",
            "Epoch: 33 Step: 8000 Loss: 0.0008788187406025827\n",
            "Epoch: 33 Step: 9000 Loss: 1.7403974197804928e-05\n",
            "Epoch: 33 Step: 10000 Loss: 1.624178548809141e-05\n",
            "Epoch: 33 Step: 11000 Loss: 5.459476597025059e-05\n",
            "Epoch: 33 Step: 12000 Loss: 0.0004886394599452615\n",
            "Epoch: 33 Step: 13000 Loss: 2.7267953555565327e-05\n",
            "Epoch: 33 Step: 14000 Loss: 0.006685102824121714\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.05700914295780667\n",
            "Epoch: 34 Step: 0 Loss: 3.492715040920302e-05\n",
            "Epoch: 34 Step: 1000 Loss: 0.00018603242642711848\n",
            "Epoch: 34 Step: 2000 Loss: 7.62933814257849e-06\n",
            "Epoch: 34 Step: 3000 Loss: 0.0001336286513833329\n",
            "Epoch: 34 Step: 4000 Loss: 6.854493676655693e-06\n",
            "Epoch: 34 Step: 5000 Loss: 3.167884278809652e-05\n",
            "Epoch: 34 Step: 6000 Loss: 1.2606181371666025e-05\n",
            "Epoch: 34 Step: 7000 Loss: 9.925239282893017e-05\n",
            "Epoch: 34 Step: 8000 Loss: 6.169034350023139e-06\n",
            "Epoch: 34 Step: 9000 Loss: 6.556507514687837e-07\n",
            "Epoch: 34 Step: 10000 Loss: 0.0014043075498193502\n",
            "Epoch: 34 Step: 11000 Loss: 9.59628050622996e-06\n",
            "Epoch: 34 Step: 12000 Loss: 0.0002109657070832327\n",
            "Epoch: 34 Step: 13000 Loss: 1.7285325384364114e-06\n",
            "Epoch: 34 Step: 14000 Loss: 2.0384433810249902e-05\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.06115508798306331\n",
            "Epoch: 35 Step: 0 Loss: 6.7650889832293615e-06\n",
            "Epoch: 35 Step: 1000 Loss: 0.00040528387762606144\n",
            "Epoch: 35 Step: 2000 Loss: 0.0002755792811512947\n",
            "Epoch: 35 Step: 3000 Loss: 7.458844629582018e-05\n",
            "Epoch: 35 Step: 4000 Loss: 0.24809011816978455\n",
            "Epoch: 35 Step: 5000 Loss: 5.960414455330465e-06\n",
            "Epoch: 35 Step: 6000 Loss: 3.8948906876612455e-05\n",
            "Epoch: 35 Step: 7000 Loss: 0.05278295651078224\n",
            "Epoch: 35 Step: 8000 Loss: 0.00025917813763953745\n",
            "Epoch: 35 Step: 9000 Loss: 0.00042778285569511354\n",
            "Epoch: 35 Step: 10000 Loss: 1.1324875686113955e-06\n",
            "Epoch: 35 Step: 11000 Loss: 0.0006482625612989068\n",
            "Epoch: 35 Step: 12000 Loss: 0.0002579013234935701\n",
            "Epoch: 35 Step: 13000 Loss: 5.906124715693295e-05\n",
            "Epoch: 35 Step: 14000 Loss: 1.9370916561456397e-05\n",
            "Accuracy: 0.9879\n",
            "Test Loss: 0.053618629903274144\n",
            "Epoch: 36 Step: 0 Loss: 2.6850650101550855e-05\n",
            "Epoch: 36 Step: 1000 Loss: 5.751822300226195e-06\n",
            "Epoch: 36 Step: 2000 Loss: 9.238708571501775e-07\n",
            "Epoch: 36 Step: 3000 Loss: 3.3167791116284207e-05\n",
            "Epoch: 36 Step: 4000 Loss: 0.0018417718820273876\n",
            "Epoch: 36 Step: 5000 Loss: 0.00017711205873638391\n",
            "Epoch: 36 Step: 6000 Loss: 0.0003217073972336948\n",
            "Epoch: 36 Step: 7000 Loss: 1.0877767635975033e-05\n",
            "Epoch: 36 Step: 8000 Loss: 1.9490180420689285e-05\n",
            "Epoch: 36 Step: 9000 Loss: 0.00034686955041252077\n",
            "Epoch: 36 Step: 10000 Loss: 2.4615706934127957e-05\n",
            "Epoch: 36 Step: 11000 Loss: 2.2649717266176594e-06\n",
            "Epoch: 36 Step: 12000 Loss: 5.956812310614623e-05\n",
            "Epoch: 36 Step: 13000 Loss: 7.301498044398613e-06\n",
            "Epoch: 36 Step: 14000 Loss: 1.0937328625004739e-05\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.06074921638310365\n",
            "Epoch: 37 Step: 0 Loss: 0.0009952340042218566\n",
            "Epoch: 37 Step: 1000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 37 Step: 2000 Loss: 1.2904239156341646e-05\n",
            "Epoch: 37 Step: 3000 Loss: 0.6500625014305115\n",
            "Epoch: 37 Step: 4000 Loss: 6.407470664271386e-06\n",
            "Epoch: 37 Step: 5000 Loss: 7.539941179857124e-06\n",
            "Epoch: 37 Step: 6000 Loss: 0.0002271999401273206\n",
            "Epoch: 37 Step: 7000 Loss: 0.0007010123808868229\n",
            "Epoch: 37 Step: 8000 Loss: 0.0005700427573174238\n",
            "Epoch: 37 Step: 9000 Loss: 0.0006580585613846779\n",
            "Epoch: 37 Step: 10000 Loss: 2.5181700038956478e-05\n",
            "Epoch: 37 Step: 11000 Loss: 0.00035222567385062575\n",
            "Epoch: 37 Step: 12000 Loss: 5.6681939895497635e-05\n",
            "Epoch: 37 Step: 13000 Loss: 9.28182271309197e-05\n",
            "Epoch: 37 Step: 14000 Loss: 1.8804588762577623e-05\n",
            "Accuracy: 0.9873\n",
            "Test Loss: 0.05663788692760372\n",
            "Epoch: 38 Step: 0 Loss: 0.00038186777965165675\n",
            "Epoch: 38 Step: 1000 Loss: 6.85453073856479e-07\n",
            "Epoch: 38 Step: 2000 Loss: 0.0002891318581532687\n",
            "Epoch: 38 Step: 3000 Loss: 8.62929955474101e-05\n",
            "Epoch: 38 Step: 4000 Loss: 4.547494972939603e-05\n",
            "Epoch: 38 Step: 5000 Loss: 4.541526868706569e-05\n",
            "Epoch: 38 Step: 6000 Loss: 2.2678954337607138e-05\n",
            "Epoch: 38 Step: 7000 Loss: 0.1981121003627777\n",
            "Epoch: 38 Step: 8000 Loss: 5.203044565860182e-05\n",
            "Epoch: 38 Step: 9000 Loss: 1.5198915207292885e-05\n",
            "Epoch: 38 Step: 10000 Loss: 5.125950337969698e-06\n",
            "Epoch: 38 Step: 11000 Loss: 4.011078999610618e-05\n",
            "Epoch: 38 Step: 12000 Loss: 4.231918865116313e-06\n",
            "Epoch: 38 Step: 13000 Loss: 0.0221174918115139\n",
            "Epoch: 38 Step: 14000 Loss: 3.194685632479377e-05\n",
            "Accuracy: 0.9873\n",
            "Test Loss: 0.06202224967352514\n",
            "Epoch: 39 Step: 0 Loss: 2.390085683146026e-05\n",
            "Epoch: 39 Step: 1000 Loss: 0.050134964287281036\n",
            "Epoch: 39 Step: 2000 Loss: 1.9669496396090835e-06\n",
            "Epoch: 39 Step: 3000 Loss: 8.206224447349086e-05\n",
            "Epoch: 39 Step: 4000 Loss: 0.002239393536001444\n",
            "Epoch: 39 Step: 5000 Loss: 1.4305099966804846e-06\n",
            "Epoch: 39 Step: 6000 Loss: 1.251695948667475e-06\n",
            "Epoch: 39 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 39 Step: 8000 Loss: 5.394182153395377e-06\n",
            "Epoch: 39 Step: 9000 Loss: 0.0004294703539926559\n",
            "Epoch: 39 Step: 10000 Loss: 0.006421773228794336\n",
            "Epoch: 39 Step: 11000 Loss: 5.167437484487891e-05\n",
            "Epoch: 39 Step: 12000 Loss: 2.0116151063120924e-05\n",
            "Epoch: 39 Step: 13000 Loss: 2.0354240405140445e-05\n",
            "Epoch: 39 Step: 14000 Loss: 2.1605797883239575e-05\n",
            "Accuracy: 0.9854\n",
            "Test Loss: 0.06535135074753196\n",
            "Epoch: 40 Step: 0 Loss: 0.00044075300684198737\n",
            "Epoch: 40 Step: 1000 Loss: 9.827995381783694e-05\n",
            "Epoch: 40 Step: 2000 Loss: 0.0005135891260579228\n",
            "Epoch: 40 Step: 3000 Loss: 0.4981961250305176\n",
            "Epoch: 40 Step: 4000 Loss: 0.0005896033835597336\n",
            "Epoch: 40 Step: 5000 Loss: 9.387584213982336e-06\n",
            "Epoch: 40 Step: 6000 Loss: 0.0001563575933687389\n",
            "Epoch: 40 Step: 7000 Loss: 6.671874871244654e-05\n",
            "Epoch: 40 Step: 8000 Loss: 4.7353234549518675e-05\n",
            "Epoch: 40 Step: 9000 Loss: 6.680832302663475e-05\n",
            "Epoch: 40 Step: 10000 Loss: 7.003495738899801e-06\n",
            "Epoch: 40 Step: 11000 Loss: 0.0009658788912929595\n",
            "Epoch: 40 Step: 12000 Loss: 2.533186943765031e-06\n",
            "Epoch: 40 Step: 13000 Loss: 1.069890731741907e-05\n",
            "Epoch: 40 Step: 14000 Loss: 6.467071216320619e-06\n",
            "Accuracy: 0.9875\n",
            "Test Loss: 0.06265210464942965\n",
            "Epoch: 41 Step: 0 Loss: 3.102317714365199e-05\n",
            "Epoch: 41 Step: 1000 Loss: 8.46372677187901e-06\n",
            "Epoch: 41 Step: 2000 Loss: 0.03516365587711334\n",
            "Epoch: 41 Step: 3000 Loss: 0.00015361497935373336\n",
            "Epoch: 41 Step: 4000 Loss: 4.505945253185928e-05\n",
            "Epoch: 41 Step: 5000 Loss: 0.00012932662502862513\n",
            "Epoch: 41 Step: 6000 Loss: 3.4868603506765794e-06\n",
            "Epoch: 41 Step: 7000 Loss: 2.0384239178383723e-05\n",
            "Epoch: 41 Step: 8000 Loss: 5.939113179920241e-05\n",
            "Epoch: 41 Step: 9000 Loss: 0.0010216537630185485\n",
            "Epoch: 41 Step: 10000 Loss: 2.3661928935325705e-05\n",
            "Epoch: 41 Step: 11000 Loss: 7.420720066875219e-06\n",
            "Epoch: 41 Step: 12000 Loss: 3.9485043089371175e-05\n",
            "Epoch: 41 Step: 13000 Loss: 0.0002202913019573316\n",
            "Epoch: 41 Step: 14000 Loss: 0.005441040266305208\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.0634663100784546\n",
            "Epoch: 42 Step: 0 Loss: 2.1427045794553123e-05\n",
            "Epoch: 42 Step: 1000 Loss: 0.00010054077574750409\n",
            "Epoch: 42 Step: 2000 Loss: 0.0016287314938381314\n",
            "Epoch: 42 Step: 3000 Loss: 0.00018367725715506822\n",
            "Epoch: 42 Step: 4000 Loss: 7.152552825573366e-07\n",
            "Epoch: 42 Step: 5000 Loss: 0.00026964343851432204\n",
            "Epoch: 42 Step: 6000 Loss: 2.2529868147103116e-05\n",
            "Epoch: 42 Step: 7000 Loss: 2.2350777726387605e-05\n",
            "Epoch: 42 Step: 8000 Loss: 0.009800962172448635\n",
            "Epoch: 42 Step: 9000 Loss: 0.011706620454788208\n",
            "Epoch: 42 Step: 10000 Loss: 5.8585224905982614e-05\n",
            "Epoch: 42 Step: 11000 Loss: 0.0003809782210737467\n",
            "Epoch: 42 Step: 12000 Loss: 0.00014048564480617642\n",
            "Epoch: 42 Step: 13000 Loss: 3.1590375328960363e-06\n",
            "Epoch: 42 Step: 14000 Loss: 1.2218940810271306e-06\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.06539207880502601\n",
            "Epoch: 43 Step: 0 Loss: 1.7285327658100869e-06\n",
            "Epoch: 43 Step: 1000 Loss: 1.829833854571916e-05\n",
            "Epoch: 43 Step: 2000 Loss: 0.010523641481995583\n",
            "Epoch: 43 Step: 3000 Loss: 5.4681961046298966e-05\n",
            "Epoch: 43 Step: 4000 Loss: 0.0010333283571526408\n",
            "Epoch: 43 Step: 5000 Loss: 9.149221114057582e-06\n",
            "Epoch: 43 Step: 6000 Loss: 0.0001200448750751093\n",
            "Epoch: 43 Step: 7000 Loss: 8.835397602524608e-05\n",
            "Epoch: 43 Step: 8000 Loss: 0.00034628305002115667\n",
            "Epoch: 43 Step: 9000 Loss: 3.257191929151304e-05\n",
            "Epoch: 43 Step: 10000 Loss: 1.5497176946155378e-06\n",
            "Epoch: 43 Step: 11000 Loss: 3.579059921321459e-05\n",
            "Epoch: 43 Step: 12000 Loss: 0.24554428458213806\n",
            "Epoch: 43 Step: 13000 Loss: 1.8685826944420114e-05\n",
            "Epoch: 43 Step: 14000 Loss: 4.684508894570172e-05\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.06404975844063715\n",
            "Epoch: 44 Step: 0 Loss: 0.0010442814091220498\n",
            "Epoch: 44 Step: 1000 Loss: 9.387564205098897e-06\n",
            "Epoch: 44 Step: 2000 Loss: 1.2397547834552824e-05\n",
            "Epoch: 44 Step: 3000 Loss: 2.5420362362638116e-05\n",
            "Epoch: 44 Step: 4000 Loss: 3.5762775496550603e-07\n",
            "Epoch: 44 Step: 5000 Loss: 8.434010851487983e-06\n",
            "Epoch: 44 Step: 6000 Loss: 0.001480294275097549\n",
            "Epoch: 44 Step: 7000 Loss: 5.119565685163252e-05\n",
            "Epoch: 44 Step: 8000 Loss: 0.005098267458379269\n",
            "Epoch: 44 Step: 9000 Loss: 0.00010270835628034547\n",
            "Epoch: 44 Step: 10000 Loss: 3.993494374299189e-06\n",
            "Epoch: 44 Step: 11000 Loss: 1.2814682122552767e-05\n",
            "Epoch: 44 Step: 12000 Loss: 1.031150168273598e-05\n",
            "Epoch: 44 Step: 13000 Loss: 4.6874691179255024e-05\n",
            "Epoch: 44 Step: 14000 Loss: 0.0016776181291788816\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.06349076688066589\n",
            "Epoch: 45 Step: 0 Loss: 1.4900741916790139e-05\n",
            "Epoch: 45 Step: 1000 Loss: 0.00012650924327317625\n",
            "Epoch: 45 Step: 2000 Loss: 0.00187609251588583\n",
            "Epoch: 45 Step: 3000 Loss: 2.6524019176576985e-06\n",
            "Epoch: 45 Step: 4000 Loss: 4.333054312155582e-05\n",
            "Epoch: 45 Step: 5000 Loss: 5.9902281464019325e-06\n",
            "Epoch: 45 Step: 6000 Loss: 9.536736342852237e-07\n",
            "Epoch: 45 Step: 7000 Loss: 2.264971953991335e-06\n",
            "Epoch: 45 Step: 8000 Loss: 0.0010427475208416581\n",
            "Epoch: 45 Step: 9000 Loss: 6.6458865148888435e-06\n",
            "Epoch: 45 Step: 10000 Loss: 0.00018429267220199108\n",
            "Epoch: 45 Step: 11000 Loss: 4.4703469370688254e-07\n",
            "Epoch: 45 Step: 12000 Loss: 5.900816631765338e-06\n",
            "Epoch: 45 Step: 13000 Loss: 0.00015766063006594777\n",
            "Epoch: 45 Step: 14000 Loss: 8.456521754851565e-05\n",
            "Accuracy: 0.9853\n",
            "Test Loss: 0.06636979720982\n",
            "Epoch: 46 Step: 0 Loss: 0.0002603585016913712\n",
            "Epoch: 46 Step: 1000 Loss: 2.413981746940408e-06\n",
            "Epoch: 46 Step: 2000 Loss: 7.420680503855692e-06\n",
            "Epoch: 46 Step: 3000 Loss: 8.642668376523943e-07\n",
            "Epoch: 46 Step: 4000 Loss: 0.0027023633010685444\n",
            "Epoch: 46 Step: 5000 Loss: 4.583452027873136e-05\n",
            "Epoch: 46 Step: 6000 Loss: 9.357841918244958e-06\n",
            "Epoch: 46 Step: 7000 Loss: 3.516660171953845e-06\n",
            "Epoch: 46 Step: 8000 Loss: 0.9328319430351257\n",
            "Epoch: 46 Step: 9000 Loss: 4.1722901187313255e-06\n",
            "Epoch: 46 Step: 10000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 46 Step: 11000 Loss: 7.816070137778297e-05\n",
            "Epoch: 46 Step: 12000 Loss: 6.467036200774601e-06\n",
            "Epoch: 46 Step: 13000 Loss: 0.000999420415610075\n",
            "Epoch: 46 Step: 14000 Loss: 0.06984709203243256\n",
            "Accuracy: 0.9874\n",
            "Test Loss: 0.05778753478145069\n",
            "Epoch: 47 Step: 0 Loss: 4.023297151434235e-06\n",
            "Epoch: 47 Step: 1000 Loss: 1.3113002523823525e-06\n",
            "Epoch: 47 Step: 2000 Loss: 0.001080843387171626\n",
            "Epoch: 47 Step: 3000 Loss: 7.911453576525673e-05\n",
            "Epoch: 47 Step: 4000 Loss: 1.3411023473963724e-06\n",
            "Epoch: 47 Step: 5000 Loss: 3.361488052178174e-05\n",
            "Epoch: 47 Step: 6000 Loss: 6.4968444348778576e-06\n",
            "Epoch: 47 Step: 7000 Loss: 5.536659591598436e-05\n",
            "Epoch: 47 Step: 8000 Loss: 0.01878414675593376\n",
            "Epoch: 47 Step: 9000 Loss: 1.2933873222209513e-05\n",
            "Epoch: 47 Step: 10000 Loss: 7.333474059123546e-05\n",
            "Epoch: 47 Step: 11000 Loss: 1.4305092008726206e-06\n",
            "Epoch: 47 Step: 12000 Loss: 1.311300025008677e-06\n",
            "Epoch: 47 Step: 13000 Loss: 8.129413618007675e-05\n",
            "Epoch: 47 Step: 14000 Loss: 1.9907496607629582e-05\n",
            "Accuracy: 0.9878\n",
            "Test Loss: 0.05713883625955208\n",
            "Epoch: 48 Step: 0 Loss: 2.056357288893196e-06\n",
            "Epoch: 48 Step: 1000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 48 Step: 2000 Loss: 0.0009537695441395044\n",
            "Epoch: 48 Step: 3000 Loss: 5.5516298743896186e-05\n",
            "Epoch: 48 Step: 4000 Loss: 5.003404658054933e-05\n",
            "Epoch: 48 Step: 5000 Loss: 8.851161510392558e-06\n",
            "Epoch: 48 Step: 6000 Loss: 0.0020496367942541838\n",
            "Epoch: 48 Step: 7000 Loss: 2.1695590476156212e-05\n",
            "Epoch: 48 Step: 8000 Loss: 0.0168558731675148\n",
            "Epoch: 48 Step: 9000 Loss: 8.910799806471914e-06\n",
            "Epoch: 48 Step: 10000 Loss: 3.3378512398485327e-06\n",
            "Epoch: 48 Step: 11000 Loss: 8.046615675993962e-07\n",
            "Epoch: 48 Step: 12000 Loss: 7.152551688704989e-07\n",
            "Epoch: 48 Step: 13000 Loss: 0.00010130753798875958\n",
            "Epoch: 48 Step: 14000 Loss: 1.4007072195454384e-06\n",
            "Accuracy: 0.9878\n",
            "Test Loss: 0.056991875967062774\n",
            "Epoch: 49 Step: 0 Loss: 1.8327771613257937e-05\n",
            "Epoch: 49 Step: 1000 Loss: 3.904087407136103e-06\n",
            "Epoch: 49 Step: 2000 Loss: 1.1622674719546922e-05\n",
            "Epoch: 49 Step: 3000 Loss: 2.384181698289467e-06\n",
            "Epoch: 49 Step: 4000 Loss: 8.404114851146005e-06\n",
            "Epoch: 49 Step: 5000 Loss: 0.0002097239630529657\n",
            "Epoch: 49 Step: 6000 Loss: 4.4703469370688254e-07\n",
            "Epoch: 49 Step: 7000 Loss: 6.534854037454352e-05\n",
            "Epoch: 49 Step: 8000 Loss: 3.3676546991046052e-06\n",
            "Epoch: 49 Step: 9000 Loss: 1.1205451301066205e-05\n",
            "Epoch: 49 Step: 10000 Loss: 2.562989948273753e-06\n",
            "Epoch: 49 Step: 11000 Loss: 0.00013807548384647816\n",
            "Epoch: 49 Step: 12000 Loss: 0.00018233041919302195\n",
            "Epoch: 49 Step: 13000 Loss: 5.24515508004697e-06\n",
            "Epoch: 49 Step: 14000 Loss: 5.751827757194405e-06\n",
            "Accuracy: 0.9841\n",
            "Test Loss: 0.07126746387712414\n",
            "Epoch: 50 Step: 0 Loss: 0.0006938488222658634\n",
            "Epoch: 50 Step: 1000 Loss: 0.0003328491875436157\n",
            "Epoch: 50 Step: 2000 Loss: 0.0001237100368598476\n",
            "Epoch: 50 Step: 3000 Loss: 0.001711009070277214\n",
            "Epoch: 50 Step: 4000 Loss: 0.0022946277167648077\n",
            "Epoch: 50 Step: 5000 Loss: 0.0013284276938065886\n",
            "Epoch: 50 Step: 6000 Loss: 4.4761054596165195e-05\n",
            "Epoch: 50 Step: 7000 Loss: 0.00025185442063957453\n",
            "Epoch: 50 Step: 8000 Loss: 1.2427272849890869e-05\n",
            "Epoch: 50 Step: 9000 Loss: 0.012980006635189056\n",
            "Epoch: 50 Step: 10000 Loss: 0.0006896767881698906\n",
            "Epoch: 50 Step: 11000 Loss: 3.0696301109855995e-06\n",
            "Epoch: 50 Step: 12000 Loss: 0.009636656381189823\n",
            "Epoch: 50 Step: 13000 Loss: 1.6689273252268322e-06\n",
            "Epoch: 50 Step: 14000 Loss: 0.0001773207332007587\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.06129111409734093\n",
            "Epoch: 51 Step: 0 Loss: 0.0008943530265241861\n",
            "Epoch: 51 Step: 1000 Loss: 6.55650637781946e-07\n",
            "Epoch: 51 Step: 2000 Loss: 0.029756790027022362\n",
            "Epoch: 51 Step: 3000 Loss: 0.46589723229408264\n",
            "Epoch: 51 Step: 4000 Loss: 6.1392456700559705e-06\n",
            "Epoch: 51 Step: 5000 Loss: 3.6656692827818915e-06\n",
            "Epoch: 51 Step: 6000 Loss: 7.420234032906592e-05\n",
            "Epoch: 51 Step: 7000 Loss: 4.014244404970668e-05\n",
            "Epoch: 51 Step: 8000 Loss: 4.5444772695191205e-05\n",
            "Epoch: 51 Step: 9000 Loss: 0.0001001883065328002\n",
            "Epoch: 51 Step: 10000 Loss: 0.00020948491874150932\n",
            "Epoch: 51 Step: 11000 Loss: 6.114941061241552e-05\n",
            "Epoch: 51 Step: 12000 Loss: 2.747626967902761e-05\n",
            "Epoch: 51 Step: 13000 Loss: 7.152548846534046e-07\n",
            "Epoch: 51 Step: 14000 Loss: 2.768508238659706e-05\n",
            "Accuracy: 0.9878\n",
            "Test Loss: 0.060339011056619776\n",
            "Epoch: 52 Step: 0 Loss: 8.73198951012455e-06\n",
            "Epoch: 52 Step: 1000 Loss: 5.096154836792266e-06\n",
            "Epoch: 52 Step: 2000 Loss: 0.00010315437975805253\n",
            "Epoch: 52 Step: 3000 Loss: 4.3835887481691316e-05\n",
            "Epoch: 52 Step: 4000 Loss: 6.854530170130602e-07\n",
            "Epoch: 52 Step: 5000 Loss: 0.11771035939455032\n",
            "Epoch: 52 Step: 6000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 52 Step: 7000 Loss: 0.0\n",
            "Epoch: 52 Step: 8000 Loss: 9.284820407629013e-05\n",
            "Epoch: 52 Step: 9000 Loss: 0.00012025403702864423\n",
            "Epoch: 52 Step: 10000 Loss: 3.874300205097825e-07\n",
            "Epoch: 52 Step: 11000 Loss: 9.536732932247105e-07\n",
            "Epoch: 52 Step: 12000 Loss: 1.2226506471633911\n",
            "Epoch: 52 Step: 13000 Loss: 7.271714366652304e-06\n",
            "Epoch: 52 Step: 14000 Loss: 0.3050500154495239\n",
            "Accuracy: 0.9878\n",
            "Test Loss: 0.06044988605641636\n",
            "Epoch: 53 Step: 0 Loss: 1.728529468891793e-06\n",
            "Epoch: 53 Step: 1000 Loss: 3.6358692341309506e-06\n",
            "Epoch: 53 Step: 2000 Loss: 0.0001003243014565669\n",
            "Epoch: 53 Step: 3000 Loss: 7.872559945099056e-05\n",
            "Epoch: 53 Step: 4000 Loss: 8.165745384758338e-06\n",
            "Epoch: 53 Step: 5000 Loss: 7.375412678811699e-05\n",
            "Epoch: 53 Step: 6000 Loss: 0.00019442247867118567\n",
            "Epoch: 53 Step: 7000 Loss: 1.555652306706179e-05\n",
            "Epoch: 53 Step: 8000 Loss: 0.0006623478257097304\n",
            "Epoch: 53 Step: 9000 Loss: 0.00023398856865242124\n",
            "Epoch: 53 Step: 10000 Loss: 0.0004284347814973444\n",
            "Epoch: 53 Step: 11000 Loss: 7.510135674237972e-06\n",
            "Epoch: 53 Step: 12000 Loss: 0.00018188433023169637\n",
            "Epoch: 53 Step: 13000 Loss: 8.099343540379778e-05\n",
            "Epoch: 53 Step: 14000 Loss: 0.10488402098417282\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06588036908753613\n",
            "Epoch: 54 Step: 0 Loss: 2.5360644940519705e-05\n",
            "Epoch: 54 Step: 1000 Loss: 0.001170735340565443\n",
            "Epoch: 54 Step: 2000 Loss: 1.6569541912758723e-05\n",
            "Epoch: 54 Step: 3000 Loss: 0.009247184731066227\n",
            "Epoch: 54 Step: 4000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 54 Step: 5000 Loss: 7.24692436051555e-05\n",
            "Epoch: 54 Step: 6000 Loss: 3.546461812220514e-06\n",
            "Epoch: 54 Step: 7000 Loss: 8.203421020880342e-05\n",
            "Epoch: 54 Step: 8000 Loss: 0.00012043389142490923\n",
            "Epoch: 54 Step: 9000 Loss: 2.384179651926388e-06\n",
            "Epoch: 54 Step: 10000 Loss: 2.086156428049435e-06\n",
            "Epoch: 54 Step: 11000 Loss: 2.38418056142109e-06\n",
            "Epoch: 54 Step: 12000 Loss: 6.496823516499717e-06\n",
            "Epoch: 54 Step: 13000 Loss: 0.0007222703425213695\n",
            "Epoch: 54 Step: 14000 Loss: 0.0014438891084864736\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.05890370644822072\n",
            "Epoch: 55 Step: 0 Loss: 4.4730593799613416e-05\n",
            "Epoch: 55 Step: 1000 Loss: 0.0014555796515196562\n",
            "Epoch: 55 Step: 2000 Loss: 0.00023281187168322504\n",
            "Epoch: 55 Step: 3000 Loss: 0.0005299531039781868\n",
            "Epoch: 55 Step: 4000 Loss: 0.0001002981371129863\n",
            "Epoch: 55 Step: 5000 Loss: 0.0002288896357640624\n",
            "Epoch: 55 Step: 6000 Loss: 0.00043957456364296377\n",
            "Epoch: 55 Step: 7000 Loss: 9.983600648411084e-06\n",
            "Epoch: 55 Step: 8000 Loss: 8.046619086599094e-07\n",
            "Epoch: 55 Step: 9000 Loss: 2.354374373680912e-06\n",
            "Epoch: 55 Step: 10000 Loss: 0.0011186121264472604\n",
            "Epoch: 55 Step: 11000 Loss: 9.536729521641973e-07\n",
            "Epoch: 55 Step: 12000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 55 Step: 13000 Loss: 1.636106208025012e-05\n",
            "Epoch: 55 Step: 14000 Loss: 7.182319222920341e-06\n",
            "Accuracy: 0.9884\n",
            "Test Loss: 0.05533494855317028\n",
            "Epoch: 56 Step: 0 Loss: 0.00013961907825432718\n",
            "Epoch: 56 Step: 1000 Loss: 0.00022827500652056187\n",
            "Epoch: 56 Step: 2000 Loss: 0.0005295625305734575\n",
            "Epoch: 56 Step: 3000 Loss: 0.001652353792451322\n",
            "Epoch: 56 Step: 4000 Loss: 1.3976922673464287e-05\n",
            "Epoch: 56 Step: 5000 Loss: 4.053100383316632e-06\n",
            "Epoch: 56 Step: 6000 Loss: 0.0010239732218906283\n",
            "Epoch: 56 Step: 7000 Loss: 0.06873804330825806\n",
            "Epoch: 56 Step: 8000 Loss: 1.6986839909804985e-05\n",
            "Epoch: 56 Step: 9000 Loss: 4.112696842639707e-06\n",
            "Epoch: 56 Step: 10000 Loss: 3.474879849818535e-05\n",
            "Epoch: 56 Step: 11000 Loss: 2.771608251350699e-06\n",
            "Epoch: 56 Step: 12000 Loss: 0.0004925617249682546\n",
            "Epoch: 56 Step: 13000 Loss: 2.202340510848444e-05\n",
            "Epoch: 56 Step: 14000 Loss: 0.17792843282222748\n",
            "Accuracy: 0.9858\n",
            "Test Loss: 0.06477445582270523\n",
            "Epoch: 57 Step: 0 Loss: 0.000103494938230142\n",
            "Epoch: 57 Step: 1000 Loss: 1.5526748029515147e-05\n",
            "Epoch: 57 Step: 2000 Loss: 9.894213690131437e-06\n",
            "Epoch: 57 Step: 3000 Loss: 1.144386624218896e-05\n",
            "Epoch: 57 Step: 4000 Loss: 4.0141003410099074e-05\n",
            "Epoch: 57 Step: 5000 Loss: 0.0002659062447492033\n",
            "Epoch: 57 Step: 6000 Loss: 1.1443836228863802e-05\n",
            "Epoch: 57 Step: 7000 Loss: 8.117646939354017e-05\n",
            "Epoch: 57 Step: 8000 Loss: 8.733543654670939e-05\n",
            "Epoch: 57 Step: 9000 Loss: 1.0788231520564295e-05\n",
            "Epoch: 57 Step: 10000 Loss: 0.002358949976041913\n",
            "Epoch: 57 Step: 11000 Loss: 4.380926384328632e-06\n",
            "Epoch: 57 Step: 12000 Loss: 3.522500628605485e-05\n",
            "Epoch: 57 Step: 13000 Loss: 6.749812018824741e-05\n",
            "Epoch: 57 Step: 14000 Loss: 0.01812615804374218\n",
            "Accuracy: 0.9877\n",
            "Test Loss: 0.05923771241153063\n",
            "Epoch: 58 Step: 0 Loss: 2.1457635739352554e-06\n",
            "Epoch: 58 Step: 1000 Loss: 0.0002263748028781265\n",
            "Epoch: 58 Step: 2000 Loss: 0.00018354506755713373\n",
            "Epoch: 58 Step: 3000 Loss: 1.2847709655761719\n",
            "Epoch: 58 Step: 4000 Loss: 0.0006721328827552497\n",
            "Epoch: 58 Step: 5000 Loss: 1.1384274330339395e-05\n",
            "Epoch: 58 Step: 6000 Loss: 3.4868603506765794e-06\n",
            "Epoch: 58 Step: 7000 Loss: 1.6093231351987924e-06\n",
            "Epoch: 58 Step: 8000 Loss: 4.4879765482619405e-05\n",
            "Epoch: 58 Step: 9000 Loss: 1.403652822773438e-05\n",
            "Epoch: 58 Step: 10000 Loss: 8.970344424596988e-06\n",
            "Epoch: 58 Step: 11000 Loss: 0.0002493294305168092\n",
            "Epoch: 58 Step: 12000 Loss: 2.62259277405974e-06\n",
            "Epoch: 58 Step: 13000 Loss: 0.00046687028952874243\n",
            "Epoch: 58 Step: 14000 Loss: 3.5998942621517926e-05\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.063422630107975\n",
            "Epoch: 59 Step: 0 Loss: 3.159038897138089e-06\n",
            "Epoch: 59 Step: 1000 Loss: 0.00014636576815973967\n",
            "Epoch: 59 Step: 2000 Loss: 1.9579372747102752e-05\n",
            "Epoch: 59 Step: 3000 Loss: 7.49127211747691e-05\n",
            "Epoch: 59 Step: 4000 Loss: 5.36441632448259e-07\n",
            "Epoch: 59 Step: 5000 Loss: 8.642535249236971e-06\n",
            "Epoch: 59 Step: 6000 Loss: 0.00022157296189107\n",
            "Epoch: 59 Step: 7000 Loss: 2.476454210409429e-05\n",
            "Epoch: 59 Step: 8000 Loss: 5.60279113415163e-06\n",
            "Epoch: 59 Step: 9000 Loss: 0.00012419113772921264\n",
            "Epoch: 59 Step: 10000 Loss: 0.00014789396664127707\n",
            "Epoch: 59 Step: 11000 Loss: 1.558620533614885e-05\n",
            "Epoch: 59 Step: 12000 Loss: 8.344643447344424e-07\n",
            "Epoch: 59 Step: 13000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 59 Step: 14000 Loss: 0.017715269699692726\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.06419401579274031\n",
            "Epoch: 60 Step: 0 Loss: 3.2782400012365542e-06\n",
            "Epoch: 60 Step: 1000 Loss: 1.3619406672660261e-05\n",
            "Epoch: 60 Step: 2000 Loss: 1.2785021681338549e-05\n",
            "Epoch: 60 Step: 3000 Loss: 3.2782436392153613e-06\n",
            "Epoch: 60 Step: 4000 Loss: 6.952394323889166e-05\n",
            "Epoch: 60 Step: 5000 Loss: 0.0029588930774480104\n",
            "Epoch: 60 Step: 6000 Loss: 2.1159573861950776e-06\n",
            "Epoch: 60 Step: 7000 Loss: 1.9967521893704543e-06\n",
            "Epoch: 60 Step: 8000 Loss: 1.1086225640610792e-05\n",
            "Epoch: 60 Step: 9000 Loss: 0.0003288614680059254\n",
            "Epoch: 60 Step: 10000 Loss: 1.260618864762364e-05\n",
            "Epoch: 60 Step: 11000 Loss: 6.496870810224209e-06\n",
            "Epoch: 60 Step: 12000 Loss: 1.2308099030633457e-05\n",
            "Epoch: 60 Step: 13000 Loss: 9.328069609182421e-06\n",
            "Epoch: 60 Step: 14000 Loss: 0.002175887580960989\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.05897502533585664\n",
            "Epoch: 61 Step: 0 Loss: 1.5824598449398763e-05\n",
            "Epoch: 61 Step: 1000 Loss: 8.046615107559774e-07\n",
            "Epoch: 61 Step: 2000 Loss: 3.7667494325432926e-05\n",
            "Epoch: 61 Step: 3000 Loss: 4.917342266708147e-06\n",
            "Epoch: 61 Step: 4000 Loss: 4.341848398325965e-05\n",
            "Epoch: 61 Step: 5000 Loss: 1.1861202438012697e-05\n",
            "Epoch: 61 Step: 6000 Loss: 8.344640036739293e-07\n",
            "Epoch: 61 Step: 7000 Loss: 0.48218098282814026\n",
            "Epoch: 61 Step: 8000 Loss: 0.010074930265545845\n",
            "Epoch: 61 Step: 9000 Loss: 0.0022062312345951796\n",
            "Epoch: 61 Step: 10000 Loss: 5.066392532171449e-07\n",
            "Epoch: 61 Step: 11000 Loss: 0.0005327168037183583\n",
            "Epoch: 61 Step: 12000 Loss: 0.06318605691194534\n",
            "Epoch: 61 Step: 13000 Loss: 6.377643785526743e-06\n",
            "Epoch: 61 Step: 14000 Loss: 0.00011328444088576362\n",
            "Accuracy: 0.9855\n",
            "Test Loss: 0.07242039199200029\n",
            "Epoch: 62 Step: 0 Loss: 3.173758159391582e-05\n",
            "Epoch: 62 Step: 1000 Loss: 7.369046215899289e-05\n",
            "Epoch: 62 Step: 2000 Loss: 1.2844479897466954e-05\n",
            "Epoch: 62 Step: 3000 Loss: 7.3611081461422145e-06\n",
            "Epoch: 62 Step: 4000 Loss: 5.990233603370143e-06\n",
            "Epoch: 62 Step: 5000 Loss: 1.0192288755206391e-05\n",
            "Epoch: 62 Step: 6000 Loss: 2.795303225866519e-05\n",
            "Epoch: 62 Step: 7000 Loss: 1.3142480383976363e-05\n",
            "Epoch: 62 Step: 8000 Loss: 1.7881392011531716e-07\n",
            "Epoch: 62 Step: 9000 Loss: 0.0003807599423453212\n",
            "Epoch: 62 Step: 10000 Loss: 3.5045472031924874e-05\n",
            "Epoch: 62 Step: 11000 Loss: 0.0005411125021055341\n",
            "Epoch: 62 Step: 12000 Loss: 0.4532375931739807\n",
            "Epoch: 62 Step: 13000 Loss: 3.906824713340029e-05\n",
            "Epoch: 62 Step: 14000 Loss: 1.5497161029998097e-06\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.06412376085441089\n",
            "Epoch: 63 Step: 0 Loss: 4.5597284952236805e-06\n",
            "Epoch: 63 Step: 1000 Loss: 0.0015469215577468276\n",
            "Epoch: 63 Step: 2000 Loss: 1.379812692903215e-05\n",
            "Epoch: 63 Step: 3000 Loss: 4.4703137973556295e-06\n",
            "Epoch: 63 Step: 4000 Loss: 3.969357567257248e-05\n",
            "Epoch: 63 Step: 5000 Loss: 1.5169147445703857e-05\n",
            "Epoch: 63 Step: 6000 Loss: 2.8312101676419843e-06\n",
            "Epoch: 63 Step: 7000 Loss: 5.960457656328799e-07\n",
            "Epoch: 63 Step: 8000 Loss: 1.1771707249863539e-05\n",
            "Epoch: 63 Step: 9000 Loss: 3.742929038708098e-05\n",
            "Epoch: 63 Step: 10000 Loss: 0.0005388117861002684\n",
            "Epoch: 63 Step: 11000 Loss: 6.854456842120271e-06\n",
            "Epoch: 63 Step: 12000 Loss: 0.0013238295214250684\n",
            "Epoch: 63 Step: 13000 Loss: 4.12430235883221e-05\n",
            "Epoch: 63 Step: 14000 Loss: 6.7054897954221815e-06\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.05873303997194319\n",
            "Epoch: 64 Step: 0 Loss: 0.00025703702704049647\n",
            "Epoch: 64 Step: 1000 Loss: 4.470344947549165e-07\n",
            "Epoch: 64 Step: 2000 Loss: 4.827930297324201e-06\n",
            "Epoch: 64 Step: 3000 Loss: 4.7683681714261184e-07\n",
            "Epoch: 64 Step: 4000 Loss: 1.0728815595939523e-06\n",
            "Epoch: 64 Step: 5000 Loss: 3.695469558806508e-06\n",
            "Epoch: 64 Step: 6000 Loss: 4.4703472212859197e-07\n",
            "Epoch: 64 Step: 7000 Loss: 5.751792286901036e-06\n",
            "Epoch: 64 Step: 8000 Loss: 2.145762891814229e-06\n",
            "Epoch: 64 Step: 9000 Loss: 6.556507514687837e-07\n",
            "Epoch: 64 Step: 10000 Loss: 7.4802824201469775e-06\n",
            "Epoch: 64 Step: 11000 Loss: 0.00010748699423857033\n",
            "Epoch: 64 Step: 12000 Loss: 3.7369456549640745e-05\n",
            "Epoch: 64 Step: 13000 Loss: 3.3676542443572544e-06\n",
            "Epoch: 64 Step: 14000 Loss: 8.076334779616445e-06\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.06361367798601793\n",
            "Epoch: 65 Step: 0 Loss: 8.233160042436793e-05\n",
            "Epoch: 65 Step: 1000 Loss: 5.06639196373726e-07\n",
            "Epoch: 65 Step: 2000 Loss: 0.0003304402926005423\n",
            "Epoch: 65 Step: 3000 Loss: 0.02861860580742359\n",
            "Epoch: 65 Step: 4000 Loss: 1.043079237206257e-06\n",
            "Epoch: 65 Step: 5000 Loss: 5.543189217860345e-06\n",
            "Epoch: 65 Step: 6000 Loss: 1.2516956076069619e-06\n",
            "Epoch: 65 Step: 7000 Loss: 7.629283572896384e-06\n",
            "Epoch: 65 Step: 8000 Loss: 7.390869996015681e-06\n",
            "Epoch: 65 Step: 9000 Loss: 9.536735774418048e-07\n",
            "Epoch: 65 Step: 10000 Loss: 0.00400324584916234\n",
            "Epoch: 65 Step: 11000 Loss: 3.4657703508855775e-05\n",
            "Epoch: 65 Step: 12000 Loss: 6.854524485788716e-07\n",
            "Epoch: 65 Step: 13000 Loss: 7.748599273327272e-07\n",
            "Epoch: 65 Step: 14000 Loss: 1.937143679242581e-06\n",
            "Accuracy: 0.9853\n",
            "Test Loss: 0.07170887299786127\n",
            "Epoch: 66 Step: 0 Loss: 1.9669491848617326e-06\n",
            "Epoch: 66 Step: 1000 Loss: 3.4031963878078386e-05\n",
            "Epoch: 66 Step: 2000 Loss: 3.3378491934854537e-06\n",
            "Epoch: 66 Step: 3000 Loss: 0.0002270807744935155\n",
            "Epoch: 66 Step: 4000 Loss: 3.8743007735320134e-07\n",
            "Epoch: 66 Step: 5000 Loss: 5.125958068674663e-06\n",
            "Epoch: 66 Step: 6000 Loss: 8.940696005765858e-08\n",
            "Epoch: 66 Step: 7000 Loss: 9.029941793414764e-06\n",
            "Epoch: 66 Step: 8000 Loss: 5.066392532171449e-07\n",
            "Epoch: 66 Step: 9000 Loss: 3.7550705656030914e-06\n",
            "Epoch: 66 Step: 10000 Loss: 0.0007944188546389341\n",
            "Epoch: 66 Step: 11000 Loss: 2.5002960683195852e-05\n",
            "Epoch: 66 Step: 12000 Loss: 1.630147744435817e-05\n",
            "Epoch: 66 Step: 13000 Loss: 2.387053427810315e-05\n",
            "Epoch: 66 Step: 14000 Loss: 3.990226105088368e-05\n",
            "Accuracy: 0.9849\n",
            "Test Loss: 0.07041971639191849\n",
            "Epoch: 67 Step: 0 Loss: 0.0001926301629282534\n",
            "Epoch: 67 Step: 1000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 67 Step: 2000 Loss: 0.002746917773038149\n",
            "Epoch: 67 Step: 3000 Loss: 1.5199160543488688e-06\n",
            "Epoch: 67 Step: 4000 Loss: 6.311473407549784e-05\n",
            "Epoch: 67 Step: 5000 Loss: 4.768368739860307e-07\n",
            "Epoch: 67 Step: 6000 Loss: 0.00013026190572418272\n",
            "Epoch: 67 Step: 7000 Loss: 2.354378921154421e-06\n",
            "Epoch: 67 Step: 8000 Loss: 0.016093052923679352\n",
            "Epoch: 67 Step: 9000 Loss: 0.02948644943535328\n",
            "Epoch: 67 Step: 10000 Loss: 0.000146078149555251\n",
            "Epoch: 67 Step: 11000 Loss: 1.1950501175306272e-05\n",
            "Epoch: 67 Step: 12000 Loss: 3.278254041561013e-07\n",
            "Epoch: 67 Step: 13000 Loss: 1.2516954939201241e-06\n",
            "Epoch: 67 Step: 14000 Loss: 8.642664397484623e-07\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.06100550053333416\n",
            "Epoch: 68 Step: 0 Loss: 6.616071914322674e-06\n",
            "Epoch: 68 Step: 1000 Loss: 1.4901159772762185e-07\n",
            "Epoch: 68 Step: 2000 Loss: 0.00021686985564883798\n",
            "Epoch: 68 Step: 3000 Loss: 0.00015997995797079057\n",
            "Epoch: 68 Step: 4000 Loss: 1.7165551980724558e-05\n",
            "Epoch: 68 Step: 5000 Loss: 1.5795199033163954e-06\n",
            "Epoch: 68 Step: 6000 Loss: 0.00023673159012105316\n",
            "Epoch: 68 Step: 7000 Loss: 5.200018495088443e-05\n",
            "Epoch: 68 Step: 8000 Loss: 5.960463766996327e-08\n",
            "Epoch: 68 Step: 9000 Loss: 3.1292247513192706e-06\n",
            "Epoch: 68 Step: 10000 Loss: 2.416867391730193e-05\n",
            "Epoch: 68 Step: 11000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 68 Step: 12000 Loss: 0.0002400926750851795\n",
            "Epoch: 68 Step: 13000 Loss: 2.0920662791468203e-05\n",
            "Epoch: 68 Step: 14000 Loss: 2.6822084464583895e-07\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.07368643799820339\n",
            "Epoch: 69 Step: 0 Loss: 9.238710845238529e-07\n",
            "Epoch: 69 Step: 1000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 69 Step: 2000 Loss: 6.1392061070364434e-06\n",
            "Epoch: 69 Step: 3000 Loss: 1.540732591820415e-05\n",
            "Epoch: 69 Step: 4000 Loss: 0.03796613588929176\n",
            "Epoch: 69 Step: 5000 Loss: 7.569681656605098e-06\n",
            "Epoch: 69 Step: 6000 Loss: 7.748595294287952e-07\n",
            "Epoch: 69 Step: 7000 Loss: 6.347821454255609e-06\n",
            "Epoch: 69 Step: 8000 Loss: 1.2516958349806373e-06\n",
            "Epoch: 69 Step: 9000 Loss: 1.5318028090405278e-05\n",
            "Epoch: 69 Step: 10000 Loss: 1.2814978163078194e-06\n",
            "Epoch: 69 Step: 11000 Loss: 0.0006841079448349774\n",
            "Epoch: 69 Step: 12000 Loss: 0.18192563951015472\n",
            "Epoch: 69 Step: 13000 Loss: 2.682208162241295e-07\n",
            "Epoch: 69 Step: 14000 Loss: 9.536732932247105e-07\n",
            "Accuracy: 0.9853\n",
            "Test Loss: 0.06897931354526302\n",
            "Epoch: 70 Step: 0 Loss: 1.1086218364653178e-05\n",
            "Epoch: 70 Step: 1000 Loss: 7.3312648964929394e-06\n",
            "Epoch: 70 Step: 2000 Loss: 0.00019406533101573586\n",
            "Epoch: 70 Step: 3000 Loss: 2.008633964578621e-05\n",
            "Epoch: 70 Step: 4000 Loss: 2.2171983800944872e-05\n",
            "Epoch: 70 Step: 5000 Loss: 1.0043276233773213e-05\n",
            "Epoch: 70 Step: 6000 Loss: 4.827933480555657e-06\n",
            "Epoch: 70 Step: 7000 Loss: 1.555633934913203e-05\n",
            "Epoch: 70 Step: 8000 Loss: 2.801404207275482e-06\n",
            "Epoch: 70 Step: 9000 Loss: 1.4901159772762185e-07\n",
            "Epoch: 70 Step: 10000 Loss: 1.6093224530777661e-06\n",
            "Epoch: 70 Step: 11000 Loss: 2.3542859707958996e-05\n",
            "Epoch: 70 Step: 12000 Loss: 0.02885165438055992\n",
            "Epoch: 70 Step: 13000 Loss: 7.15254714123148e-07\n",
            "Epoch: 70 Step: 14000 Loss: 2.9802313861182483e-07\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.07319045009309491\n",
            "Epoch: 71 Step: 0 Loss: 3.933882908313535e-06\n",
            "Epoch: 71 Step: 1000 Loss: 7.152549414968234e-07\n",
            "Epoch: 71 Step: 2000 Loss: 0.0002599428698886186\n",
            "Epoch: 71 Step: 3000 Loss: 2.4437840693281032e-06\n",
            "Epoch: 71 Step: 4000 Loss: 0.00031517003662884235\n",
            "Epoch: 71 Step: 5000 Loss: 0.2673510015010834\n",
            "Epoch: 71 Step: 6000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 71 Step: 7000 Loss: 2.0443674657144584e-05\n",
            "Epoch: 71 Step: 8000 Loss: 0.0004175789945293218\n",
            "Epoch: 71 Step: 9000 Loss: 6.854526191091281e-07\n",
            "Epoch: 71 Step: 10000 Loss: 1.0072983059217222e-05\n",
            "Epoch: 71 Step: 11000 Loss: 0.0002790028229355812\n",
            "Epoch: 71 Step: 12000 Loss: 0.00032378247124142945\n",
            "Epoch: 71 Step: 13000 Loss: 0.00014450489834416658\n",
            "Epoch: 71 Step: 14000 Loss: 3.069627382501494e-06\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.06802109827857174\n",
            "Epoch: 72 Step: 0 Loss: 0.6695177555084229\n",
            "Epoch: 72 Step: 1000 Loss: 2.0265524653950706e-06\n",
            "Epoch: 72 Step: 2000 Loss: 4.7385301513713785e-06\n",
            "Epoch: 72 Step: 3000 Loss: 4.231911589158699e-06\n",
            "Epoch: 72 Step: 4000 Loss: 8.910742508305702e-06\n",
            "Epoch: 72 Step: 5000 Loss: 0.00011224055924685672\n",
            "Epoch: 72 Step: 6000 Loss: 9.536727247905219e-07\n",
            "Epoch: 72 Step: 7000 Loss: 3.725273927557282e-06\n",
            "Epoch: 72 Step: 8000 Loss: 3.3048818295355886e-05\n",
            "Epoch: 72 Step: 9000 Loss: 3.844471848424291e-06\n",
            "Epoch: 72 Step: 10000 Loss: 3.206542896805331e-05\n",
            "Epoch: 72 Step: 11000 Loss: 4.440517386683496e-06\n",
            "Epoch: 72 Step: 12000 Loss: 0.0006181109347380698\n",
            "Epoch: 72 Step: 13000 Loss: 1.4632733837061096e-05\n",
            "Epoch: 72 Step: 14000 Loss: 1.1533316865097731e-05\n",
            "Accuracy: 0.9853\n",
            "Test Loss: 0.07038272141671\n",
            "Epoch: 73 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 73 Step: 1000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 73 Step: 2000 Loss: 1.400706196363899e-06\n",
            "Epoch: 73 Step: 3000 Loss: 1.6987279423119617e-06\n",
            "Epoch: 73 Step: 4000 Loss: 1.5288227587006986e-05\n",
            "Epoch: 73 Step: 5000 Loss: 4.7683684556432127e-07\n",
            "Epoch: 73 Step: 6000 Loss: 0.00010074179590446874\n",
            "Epoch: 73 Step: 7000 Loss: 5.372792293201201e-05\n",
            "Epoch: 73 Step: 8000 Loss: 4.768369876728684e-07\n",
            "Epoch: 73 Step: 9000 Loss: 0.8445447683334351\n",
            "Epoch: 73 Step: 10000 Loss: 2.6671656087273732e-05\n",
            "Epoch: 73 Step: 11000 Loss: 0.00012189168046461418\n",
            "Epoch: 73 Step: 12000 Loss: 5.36441234544327e-07\n",
            "Epoch: 73 Step: 13000 Loss: 2.086161714487389e-07\n",
            "Epoch: 73 Step: 14000 Loss: 8.553153747925535e-06\n",
            "Accuracy: 0.9849\n",
            "Test Loss: 0.07107746232698525\n",
            "Epoch: 74 Step: 0 Loss: 0.0002179751463700086\n",
            "Epoch: 74 Step: 1000 Loss: 0.0006672501913271844\n",
            "Epoch: 74 Step: 2000 Loss: 3.6060571346752113e-06\n",
            "Epoch: 74 Step: 3000 Loss: 1.2367661838652566e-05\n",
            "Epoch: 74 Step: 4000 Loss: 2.4139765173458727e-06\n",
            "Epoch: 74 Step: 5000 Loss: 3.2782546099952015e-07\n",
            "Epoch: 74 Step: 6000 Loss: 0.0028493404388427734\n",
            "Epoch: 74 Step: 7000 Loss: 5.662437274622789e-07\n",
            "Epoch: 74 Step: 8000 Loss: 1.0728605957410764e-05\n",
            "Epoch: 74 Step: 9000 Loss: 3.805520464084111e-05\n",
            "Epoch: 74 Step: 10000 Loss: 3.874272806569934e-06\n",
            "Epoch: 74 Step: 11000 Loss: 4.6491395551129244e-06\n",
            "Epoch: 74 Step: 12000 Loss: 5.572988811763935e-06\n",
            "Epoch: 74 Step: 13000 Loss: 0.0\n",
            "Epoch: 74 Step: 14000 Loss: 2.828081778716296e-05\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.069678413623221\n",
            "Epoch: 75 Step: 0 Loss: 0.0009389344486407936\n",
            "Epoch: 75 Step: 1000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 75 Step: 2000 Loss: 5.960458793197176e-07\n",
            "Epoch: 75 Step: 3000 Loss: 4.127376087126322e-05\n",
            "Epoch: 75 Step: 4000 Loss: 7.450569228240056e-07\n",
            "Epoch: 75 Step: 5000 Loss: 0.0033643508795648813\n",
            "Epoch: 75 Step: 6000 Loss: 0.18592259287834167\n",
            "Epoch: 75 Step: 7000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 75 Step: 8000 Loss: 0.00338676106184721\n",
            "Epoch: 75 Step: 9000 Loss: 1.1205425835214555e-05\n",
            "Epoch: 75 Step: 10000 Loss: 1.7881361600302625e-06\n",
            "Epoch: 75 Step: 11000 Loss: 5.283395148580894e-05\n",
            "Epoch: 75 Step: 12000 Loss: 0.4860388934612274\n",
            "Epoch: 75 Step: 13000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 75 Step: 14000 Loss: 0.029886726289987564\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.06783949942490339\n",
            "Epoch: 76 Step: 0 Loss: 7.837924385967199e-06\n",
            "Epoch: 76 Step: 1000 Loss: 0.001243622275069356\n",
            "Epoch: 76 Step: 2000 Loss: 6.854525622657093e-07\n",
            "Epoch: 76 Step: 3000 Loss: 4.7683681714261184e-07\n",
            "Epoch: 76 Step: 4000 Loss: 8.200472075259313e-05\n",
            "Epoch: 76 Step: 5000 Loss: 4.7683374759799335e-06\n",
            "Epoch: 76 Step: 6000 Loss: 9.77047675405629e-05\n",
            "Epoch: 76 Step: 7000 Loss: 2.926419983850792e-05\n",
            "Epoch: 76 Step: 8000 Loss: 0.00039377622306346893\n",
            "Epoch: 76 Step: 9000 Loss: 2.652394414326409e-06\n",
            "Epoch: 76 Step: 10000 Loss: 0.0\n",
            "Epoch: 76 Step: 11000 Loss: 0.006876316852867603\n",
            "Epoch: 76 Step: 12000 Loss: 5.960463766996327e-08\n",
            "Epoch: 76 Step: 13000 Loss: 7.450574344147753e-07\n",
            "Epoch: 76 Step: 14000 Loss: 0.00385150290094316\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.06647664406301051\n",
            "Epoch: 77 Step: 0 Loss: 1.390358805656433\n",
            "Epoch: 77 Step: 1000 Loss: 7.688885489187669e-06\n",
            "Epoch: 77 Step: 2000 Loss: 7.539900707342895e-06\n",
            "Epoch: 77 Step: 3000 Loss: 4.410707333590835e-06\n",
            "Epoch: 77 Step: 4000 Loss: 1.0221992852166295e-05\n",
            "Epoch: 77 Step: 5000 Loss: 3.159026618959615e-06\n",
            "Epoch: 77 Step: 6000 Loss: 0.001043790252879262\n",
            "Epoch: 77 Step: 7000 Loss: 2.741715252341237e-05\n",
            "Epoch: 77 Step: 8000 Loss: 3.948495941585861e-05\n",
            "Epoch: 77 Step: 9000 Loss: 2.622592319312389e-06\n",
            "Epoch: 77 Step: 10000 Loss: 2.694047179829795e-05\n",
            "Epoch: 77 Step: 11000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 77 Step: 12000 Loss: 0.0001427621755283326\n",
            "Epoch: 77 Step: 13000 Loss: 2.533185352149303e-06\n",
            "Epoch: 77 Step: 14000 Loss: 1.3411022337095346e-06\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.06742829029738853\n",
            "Epoch: 78 Step: 0 Loss: 1.7881390590446244e-07\n",
            "Epoch: 78 Step: 1000 Loss: 1.1175620784342755e-05\n",
            "Epoch: 78 Step: 2000 Loss: 3.248433586122701e-06\n",
            "Epoch: 78 Step: 3000 Loss: 0.00024589765234850347\n",
            "Epoch: 78 Step: 4000 Loss: 0.00012534737470559776\n",
            "Epoch: 78 Step: 5000 Loss: 1.3709030781683396e-06\n",
            "Epoch: 78 Step: 6000 Loss: 4.678940968005918e-06\n",
            "Epoch: 78 Step: 7000 Loss: 5.6624367061886e-07\n",
            "Epoch: 78 Step: 8000 Loss: 4.3448260839795694e-05\n",
            "Epoch: 78 Step: 9000 Loss: 1.8506609194446355e-05\n",
            "Epoch: 78 Step: 10000 Loss: 0.005764429923146963\n",
            "Epoch: 78 Step: 11000 Loss: 8.463718586426694e-06\n",
            "Epoch: 78 Step: 12000 Loss: 5.036554284743033e-06\n",
            "Epoch: 78 Step: 13000 Loss: 0.00010932129225693643\n",
            "Epoch: 78 Step: 14000 Loss: 4.917354544886621e-06\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.07598559550130383\n",
            "Epoch: 79 Step: 0 Loss: 1.9192024410585873e-05\n",
            "Epoch: 79 Step: 1000 Loss: 1.4662523426522966e-05\n",
            "Epoch: 79 Step: 2000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 79 Step: 3000 Loss: 7.152549414968234e-07\n",
            "Epoch: 79 Step: 4000 Loss: 9.834748198045418e-07\n",
            "Epoch: 79 Step: 5000 Loss: 2.264967406517826e-06\n",
            "Epoch: 79 Step: 6000 Loss: 4.321299456933048e-06\n",
            "Epoch: 79 Step: 7000 Loss: 0.0015996062429621816\n",
            "Epoch: 79 Step: 8000 Loss: 3.3676506063784473e-06\n",
            "Epoch: 79 Step: 9000 Loss: 0.0053731282241642475\n",
            "Epoch: 79 Step: 10000 Loss: 0.1746254712343216\n",
            "Epoch: 79 Step: 11000 Loss: 2.5629876745369984e-06\n",
            "Epoch: 79 Step: 12000 Loss: 1.4453776202572044e-05\n",
            "Epoch: 79 Step: 13000 Loss: 0.00010423555067973211\n",
            "Epoch: 79 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.06405394531398208\n",
            "Epoch: 80 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 80 Step: 1000 Loss: 3.218565689167008e-05\n",
            "Epoch: 80 Step: 2000 Loss: 0.00012001498544123024\n",
            "Epoch: 80 Step: 3000 Loss: 9.029943612404168e-06\n",
            "Epoch: 80 Step: 4000 Loss: 9.834750471782172e-07\n",
            "Epoch: 80 Step: 5000 Loss: 4.920097126159817e-05\n",
            "Epoch: 80 Step: 6000 Loss: 5.632602551486343e-06\n",
            "Epoch: 80 Step: 7000 Loss: 0.00014397871564142406\n",
            "Epoch: 80 Step: 8000 Loss: 9.13871917873621e-05\n",
            "Epoch: 80 Step: 9000 Loss: 2.682207878024201e-07\n",
            "Epoch: 80 Step: 10000 Loss: 7.131123857107013e-05\n",
            "Epoch: 80 Step: 11000 Loss: 2.3274571503861807e-05\n",
            "Epoch: 80 Step: 12000 Loss: 3.576278118089249e-07\n",
            "Epoch: 80 Step: 13000 Loss: 2.0265499642846407e-06\n",
            "Epoch: 80 Step: 14000 Loss: 0.0021579263266175985\n",
            "Accuracy: 0.9863\n",
            "Test Loss: 0.06651843299020978\n",
            "Epoch: 81 Step: 0 Loss: 1.4901158351676713e-07\n",
            "Epoch: 81 Step: 1000 Loss: 4.947156412526965e-06\n",
            "Epoch: 81 Step: 2000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 81 Step: 3000 Loss: 8.34464117360767e-07\n",
            "Epoch: 81 Step: 4000 Loss: 6.010735160089098e-05\n",
            "Epoch: 81 Step: 5000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 81 Step: 6000 Loss: 2.1457647108036326e-06\n",
            "Epoch: 81 Step: 7000 Loss: 1.2427313777152449e-05\n",
            "Epoch: 81 Step: 8000 Loss: 0.00010893506259890273\n",
            "Epoch: 81 Step: 9000 Loss: 0.0\n",
            "Epoch: 81 Step: 10000 Loss: 4.7888082917779684e-05\n",
            "Epoch: 81 Step: 11000 Loss: 4.708736923930701e-06\n",
            "Epoch: 81 Step: 12000 Loss: 5.0961771194124594e-06\n",
            "Epoch: 81 Step: 13000 Loss: 1.8477419416740304e-06\n",
            "Epoch: 81 Step: 14000 Loss: 1.4901156930591242e-07\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.0705573462638545\n",
            "Epoch: 82 Step: 0 Loss: 3.05756984744221e-05\n",
            "Epoch: 82 Step: 1000 Loss: 4.5415101340040565e-05\n",
            "Epoch: 82 Step: 2000 Loss: 0.0001552520989207551\n",
            "Epoch: 82 Step: 3000 Loss: 2.0115803636144847e-05\n",
            "Epoch: 82 Step: 4000 Loss: 0.002066968474537134\n",
            "Epoch: 82 Step: 5000 Loss: 1.7285286730839289e-06\n",
            "Epoch: 82 Step: 6000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 82 Step: 7000 Loss: 7.675934466533363e-05\n",
            "Epoch: 82 Step: 8000 Loss: 5.364412913877459e-07\n",
            "Epoch: 82 Step: 9000 Loss: 7.5994885264663026e-06\n",
            "Epoch: 82 Step: 10000 Loss: 3.859104253933765e-05\n",
            "Epoch: 82 Step: 11000 Loss: 0.00012621520727407187\n",
            "Epoch: 82 Step: 12000 Loss: 1.3709047834709054e-06\n",
            "Epoch: 82 Step: 13000 Loss: 3.39745156452409e-06\n",
            "Epoch: 82 Step: 14000 Loss: 8.433930815954227e-06\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06915172838169702\n",
            "Epoch: 83 Step: 0 Loss: 2.9294114938238636e-05\n",
            "Epoch: 83 Step: 1000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 83 Step: 2000 Loss: 1.4901129361533094e-06\n",
            "Epoch: 83 Step: 3000 Loss: 3.5762769812208717e-07\n",
            "Epoch: 83 Step: 4000 Loss: 1.0728823554018163e-06\n",
            "Epoch: 83 Step: 5000 Loss: 1.3411013242148329e-06\n",
            "Epoch: 83 Step: 6000 Loss: 3.0994247026683297e-06\n",
            "Epoch: 83 Step: 7000 Loss: 0.002114144852384925\n",
            "Epoch: 83 Step: 8000 Loss: 4.255418025422841e-05\n",
            "Epoch: 83 Step: 9000 Loss: 4.142488705838332e-06\n",
            "Epoch: 83 Step: 10000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 83 Step: 11000 Loss: 0.1549101024866104\n",
            "Epoch: 83 Step: 12000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 83 Step: 13000 Loss: 3.278254325778107e-07\n",
            "Epoch: 83 Step: 14000 Loss: 3.0188037271727808e-05\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06576775533849147\n",
            "Epoch: 84 Step: 0 Loss: 5.662436137754412e-07\n",
            "Epoch: 84 Step: 1000 Loss: 1.996751507249428e-06\n",
            "Epoch: 84 Step: 2000 Loss: 3.576276128569589e-07\n",
            "Epoch: 84 Step: 3000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 84 Step: 4000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 84 Step: 5000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 84 Step: 6000 Loss: 3.906890924554318e-05\n",
            "Epoch: 84 Step: 7000 Loss: 0.2694973051548004\n",
            "Epoch: 84 Step: 8000 Loss: 4.4703443791149766e-07\n",
            "Epoch: 84 Step: 9000 Loss: 0.0028586559928953648\n",
            "Epoch: 84 Step: 10000 Loss: 3.5762769812208717e-07\n",
            "Epoch: 84 Step: 11000 Loss: 5.424001301435055e-06\n",
            "Epoch: 84 Step: 12000 Loss: 7.450573775713565e-07\n",
            "Epoch: 84 Step: 13000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 84 Step: 14000 Loss: 0.3776489198207855\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06529196255860435\n",
            "Epoch: 85 Step: 0 Loss: 0.0018820815021172166\n",
            "Epoch: 85 Step: 1000 Loss: 0.007844826206564903\n",
            "Epoch: 85 Step: 2000 Loss: 6.991160626057535e-05\n",
            "Epoch: 85 Step: 3000 Loss: 4.7683701609457785e-07\n",
            "Epoch: 85 Step: 4000 Loss: 0.005176068749278784\n",
            "Epoch: 85 Step: 5000 Loss: 0.0\n",
            "Epoch: 85 Step: 6000 Loss: 5.990230420138687e-06\n",
            "Epoch: 85 Step: 7000 Loss: 4.8007255827542394e-05\n",
            "Epoch: 85 Step: 8000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 85 Step: 9000 Loss: 3.292941255494952e-05\n",
            "Epoch: 85 Step: 10000 Loss: 4.708723281510174e-06\n",
            "Epoch: 85 Step: 11000 Loss: 0.015346083790063858\n",
            "Epoch: 85 Step: 12000 Loss: 6.288256372499745e-06\n",
            "Epoch: 85 Step: 13000 Loss: 1.937143679242581e-06\n",
            "Epoch: 85 Step: 14000 Loss: 3.1888284865999594e-06\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.06676546295216391\n",
            "Epoch: 86 Step: 0 Loss: 0.00011757248284993693\n",
            "Epoch: 86 Step: 1000 Loss: 2.384184938364342e-07\n",
            "Epoch: 86 Step: 2000 Loss: 0.0002907021844293922\n",
            "Epoch: 86 Step: 3000 Loss: 3.576276412786683e-07\n",
            "Epoch: 86 Step: 4000 Loss: 0.002796310232952237\n",
            "Epoch: 86 Step: 5000 Loss: 2.771600520645734e-06\n",
            "Epoch: 86 Step: 6000 Loss: 1.9192044419469312e-05\n",
            "Epoch: 86 Step: 7000 Loss: 0.00012493255781009793\n",
            "Epoch: 86 Step: 8000 Loss: 2.80140579889121e-06\n",
            "Epoch: 86 Step: 9000 Loss: 7.152548846534046e-07\n",
            "Epoch: 86 Step: 10000 Loss: 0.0004585360875353217\n",
            "Epoch: 86 Step: 11000 Loss: 8.940683642322256e-07\n",
            "Epoch: 86 Step: 12000 Loss: 7.867722160881385e-06\n",
            "Epoch: 86 Step: 13000 Loss: 0.000833504949696362\n",
            "Epoch: 86 Step: 14000 Loss: 9.536735774418048e-07\n",
            "Accuracy: 0.9851\n",
            "Test Loss: 0.07623248332376271\n",
            "Epoch: 87 Step: 0 Loss: 3.0398268791032024e-06\n",
            "Epoch: 87 Step: 1000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 87 Step: 2000 Loss: 1.4901159772762185e-07\n",
            "Epoch: 87 Step: 3000 Loss: 1.6987296476145275e-06\n",
            "Epoch: 87 Step: 4000 Loss: 3.904083314409945e-06\n",
            "Epoch: 87 Step: 5000 Loss: 0.0002661719627212733\n",
            "Epoch: 87 Step: 6000 Loss: 1.2308056284382474e-05\n",
            "Epoch: 87 Step: 7000 Loss: 0.0035216743126511574\n",
            "Epoch: 87 Step: 8000 Loss: 1.7881387748275301e-07\n",
            "Epoch: 87 Step: 9000 Loss: 0.0\n",
            "Epoch: 87 Step: 10000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 87 Step: 11000 Loss: 1.370904101349879e-06\n",
            "Epoch: 87 Step: 12000 Loss: 0.0008698455058038235\n",
            "Epoch: 87 Step: 13000 Loss: 0.1128423735499382\n",
            "Epoch: 87 Step: 14000 Loss: 3.477754216874018e-05\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.06931226858747427\n",
            "Epoch: 88 Step: 0 Loss: 0.1260119080543518\n",
            "Epoch: 88 Step: 1000 Loss: 0.00020367563411127776\n",
            "Epoch: 88 Step: 2000 Loss: 3.27825318890973e-07\n",
            "Epoch: 88 Step: 3000 Loss: 5.066390826868883e-07\n",
            "Epoch: 88 Step: 4000 Loss: 1.874497320386581e-05\n",
            "Epoch: 88 Step: 5000 Loss: 8.642666102787189e-07\n",
            "Epoch: 88 Step: 6000 Loss: 0.0005879277596250176\n",
            "Epoch: 88 Step: 7000 Loss: 3.278254325778107e-07\n",
            "Epoch: 88 Step: 8000 Loss: 6.25848201707413e-07\n",
            "Epoch: 88 Step: 9000 Loss: 3.4479260648367926e-05\n",
            "Epoch: 88 Step: 10000 Loss: 0.005779131781309843\n",
            "Epoch: 88 Step: 11000 Loss: 0.002721494762226939\n",
            "Epoch: 88 Step: 12000 Loss: 0.0007101178052835166\n",
            "Epoch: 88 Step: 13000 Loss: 1.102684564102674e-06\n",
            "Epoch: 88 Step: 14000 Loss: 2.959195444418583e-05\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.0743190873134382\n",
            "Epoch: 89 Step: 0 Loss: 2.9802313861182483e-07\n",
            "Epoch: 89 Step: 1000 Loss: 0.0009050618391484022\n",
            "Epoch: 89 Step: 2000 Loss: 3.7787744076922536e-05\n",
            "Epoch: 89 Step: 3000 Loss: 2.2321024516713805e-05\n",
            "Epoch: 89 Step: 4000 Loss: 0.00047996293869800866\n",
            "Epoch: 89 Step: 5000 Loss: 5.960463766996327e-08\n",
            "Epoch: 89 Step: 6000 Loss: 5.364414619180025e-07\n",
            "Epoch: 89 Step: 7000 Loss: 0.03378729522228241\n",
            "Epoch: 89 Step: 8000 Loss: 0.0005359351634979248\n",
            "Epoch: 89 Step: 9000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 89 Step: 10000 Loss: 8.642620741738938e-06\n",
            "Epoch: 89 Step: 11000 Loss: 0.00025974330492317677\n",
            "Epoch: 89 Step: 12000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 89 Step: 13000 Loss: 6.347841917886399e-06\n",
            "Epoch: 89 Step: 14000 Loss: 0.0001455436577089131\n",
            "Accuracy: 0.9862\n",
            "Test Loss: 0.06482559543401198\n",
            "Epoch: 90 Step: 0 Loss: 6.469460640801117e-05\n",
            "Epoch: 90 Step: 1000 Loss: 0.0\n",
            "Epoch: 90 Step: 2000 Loss: 0.00011387858830858022\n",
            "Epoch: 90 Step: 3000 Loss: 4.303175228415057e-05\n",
            "Epoch: 90 Step: 4000 Loss: 6.318050509435125e-06\n",
            "Epoch: 90 Step: 5000 Loss: 1.5705576515756547e-05\n",
            "Epoch: 90 Step: 6000 Loss: 1.63912534389965e-06\n",
            "Epoch: 90 Step: 7000 Loss: 2.413872971374076e-05\n",
            "Epoch: 90 Step: 8000 Loss: 1.907345790641557e-06\n",
            "Epoch: 90 Step: 9000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 90 Step: 10000 Loss: 4.678948698710883e-06\n",
            "Epoch: 90 Step: 11000 Loss: 1.1324856359351543e-06\n",
            "Epoch: 90 Step: 12000 Loss: 8.04662079190166e-07\n",
            "Epoch: 90 Step: 13000 Loss: 2.9800949050695635e-05\n",
            "Epoch: 90 Step: 14000 Loss: 5.6624367061886e-07\n",
            "Accuracy: 0.9851\n",
            "Test Loss: 0.06547413091018912\n",
            "Epoch: 91 Step: 0 Loss: 1.3321319784154184e-05\n",
            "Epoch: 91 Step: 1000 Loss: 0.011132456362247467\n",
            "Epoch: 91 Step: 2000 Loss: 0.08745042234659195\n",
            "Epoch: 91 Step: 3000 Loss: 7.450569228240056e-07\n",
            "Epoch: 91 Step: 4000 Loss: 0.00011194232502020895\n",
            "Epoch: 91 Step: 5000 Loss: 3.2782370453787735e-06\n",
            "Epoch: 91 Step: 6000 Loss: 4.5001133912592195e-06\n",
            "Epoch: 91 Step: 7000 Loss: 2.1486703190021217e-05\n",
            "Epoch: 91 Step: 8000 Loss: 1.1026835409211344e-06\n",
            "Epoch: 91 Step: 9000 Loss: 1.430507381883217e-06\n",
            "Epoch: 91 Step: 10000 Loss: 1.1622896636254154e-06\n",
            "Epoch: 91 Step: 11000 Loss: 2.682207878024201e-07\n",
            "Epoch: 91 Step: 12000 Loss: 9.168505494017154e-05\n",
            "Epoch: 91 Step: 13000 Loss: 0.016463110223412514\n",
            "Epoch: 91 Step: 14000 Loss: 7.953491876833141e-05\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.06571083256844948\n",
            "Epoch: 92 Step: 0 Loss: 1.966949412235408e-06\n",
            "Epoch: 92 Step: 1000 Loss: 1.3529989701055456e-05\n",
            "Epoch: 92 Step: 2000 Loss: 1.7583317912794882e-06\n",
            "Epoch: 92 Step: 3000 Loss: 5.0451064453227445e-05\n",
            "Epoch: 92 Step: 4000 Loss: 1.937146635100362e-06\n",
            "Epoch: 92 Step: 5000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 92 Step: 6000 Loss: 1.6807989595690742e-05\n",
            "Epoch: 92 Step: 7000 Loss: 1.609321316209389e-06\n",
            "Epoch: 92 Step: 8000 Loss: 0.2153952270746231\n",
            "Epoch: 92 Step: 9000 Loss: 0.04921789839863777\n",
            "Epoch: 92 Step: 10000 Loss: 0.027424011379480362\n",
            "Epoch: 92 Step: 11000 Loss: 4.2617130020516925e-06\n",
            "Epoch: 92 Step: 12000 Loss: 0.0002210501115769148\n",
            "Epoch: 92 Step: 13000 Loss: 0.0002858868392650038\n",
            "Epoch: 92 Step: 14000 Loss: 0.0001349618542008102\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.06456490470395645\n",
            "Epoch: 93 Step: 0 Loss: 2.7565887648961507e-05\n",
            "Epoch: 93 Step: 1000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 93 Step: 2000 Loss: 9.834757292992435e-07\n",
            "Epoch: 93 Step: 3000 Loss: 2.1755658963229507e-06\n",
            "Epoch: 93 Step: 4000 Loss: 1.895364584925119e-05\n",
            "Epoch: 93 Step: 5000 Loss: 1.1056418770749588e-05\n",
            "Epoch: 93 Step: 6000 Loss: 7.331272172450554e-06\n",
            "Epoch: 93 Step: 7000 Loss: 2.3810938728274778e-05\n",
            "Epoch: 93 Step: 8000 Loss: 3.5762769812208717e-07\n",
            "Epoch: 93 Step: 9000 Loss: 4.500111117522465e-06\n",
            "Epoch: 93 Step: 10000 Loss: 4.202093350613723e-06\n",
            "Epoch: 93 Step: 11000 Loss: 1.7881360463434248e-06\n",
            "Epoch: 93 Step: 12000 Loss: 1.049027923727408e-05\n",
            "Epoch: 93 Step: 13000 Loss: 7.5100901995028835e-06\n",
            "Epoch: 93 Step: 14000 Loss: 3.874299636663636e-07\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.06952348310107763\n",
            "Epoch: 94 Step: 0 Loss: 4.559714398055803e-06\n",
            "Epoch: 94 Step: 1000 Loss: 4.678921868617181e-06\n",
            "Epoch: 94 Step: 2000 Loss: 1.1920913038920844e-06\n",
            "Epoch: 94 Step: 3000 Loss: 6.481164746219292e-05\n",
            "Epoch: 94 Step: 4000 Loss: 6.1094383454474155e-06\n",
            "Epoch: 94 Step: 5000 Loss: 4.649136826628819e-06\n",
            "Epoch: 94 Step: 6000 Loss: 5.066346602689009e-06\n",
            "Epoch: 94 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 94 Step: 8000 Loss: 3.0696230624016607e-06\n",
            "Epoch: 94 Step: 9000 Loss: 1.9371448161109583e-06\n",
            "Epoch: 94 Step: 10000 Loss: 2.0324470824562013e-05\n",
            "Epoch: 94 Step: 11000 Loss: 1.0967015441565309e-05\n",
            "Epoch: 94 Step: 12000 Loss: 7.890423148637637e-05\n",
            "Epoch: 94 Step: 13000 Loss: 1.579520471750584e-06\n",
            "Epoch: 94 Step: 14000 Loss: 6.6160432652395684e-06\n",
            "Accuracy: 0.9853\n",
            "Test Loss: 0.07264974985053609\n",
            "Epoch: 95 Step: 0 Loss: 3.993493919551838e-06\n",
            "Epoch: 95 Step: 1000 Loss: 0.00025757242110557854\n",
            "Epoch: 95 Step: 2000 Loss: 0.8347219228744507\n",
            "Epoch: 95 Step: 3000 Loss: 1.0043191650765948e-05\n",
            "Epoch: 95 Step: 4000 Loss: 3.0188004529918544e-05\n",
            "Epoch: 95 Step: 5000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 95 Step: 6000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 95 Step: 7000 Loss: 2.741805019468302e-06\n",
            "Epoch: 95 Step: 8000 Loss: 7.837891644157935e-06\n",
            "Epoch: 95 Step: 9000 Loss: 0.0014240123564377427\n",
            "Epoch: 95 Step: 10000 Loss: 2.0563561520248186e-06\n",
            "Epoch: 95 Step: 11000 Loss: 1.23379768410814e-05\n",
            "Epoch: 95 Step: 12000 Loss: 9.536724974168465e-07\n",
            "Epoch: 95 Step: 13000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 95 Step: 14000 Loss: 3.287040090071969e-05\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.06752044959404306\n",
            "Epoch: 96 Step: 0 Loss: 9.794106154004112e-05\n",
            "Epoch: 96 Step: 1000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 96 Step: 2000 Loss: 0.0006260480731725693\n",
            "Epoch: 96 Step: 3000 Loss: 1.1026835409211344e-06\n",
            "Epoch: 96 Step: 4000 Loss: 1.7523201677249745e-05\n",
            "Epoch: 96 Step: 5000 Loss: 2.4049422790994868e-05\n",
            "Epoch: 96 Step: 6000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 96 Step: 7000 Loss: 0.03252032771706581\n",
            "Epoch: 96 Step: 8000 Loss: 0.0006305441493168473\n",
            "Epoch: 96 Step: 9000 Loss: 1.338104993919842e-05\n",
            "Epoch: 96 Step: 10000 Loss: 2.4735811621212633e-06\n",
            "Epoch: 96 Step: 11000 Loss: 1.0430791235194192e-06\n",
            "Epoch: 96 Step: 12000 Loss: 0.00010643371206242591\n",
            "Epoch: 96 Step: 13000 Loss: 0.0008912885095924139\n",
            "Epoch: 96 Step: 14000 Loss: 1.4901156930591242e-07\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.06405654904302978\n",
            "Epoch: 97 Step: 0 Loss: 7.748598136458895e-07\n",
            "Epoch: 97 Step: 1000 Loss: 8.940696005765858e-08\n",
            "Epoch: 97 Step: 2000 Loss: 2.318527731404174e-05\n",
            "Epoch: 97 Step: 3000 Loss: 7.68889640312409e-06\n",
            "Epoch: 97 Step: 4000 Loss: 0.0006191514548845589\n",
            "Epoch: 97 Step: 5000 Loss: 2.1159567040740512e-06\n",
            "Epoch: 97 Step: 6000 Loss: 0.0003754358331207186\n",
            "Epoch: 97 Step: 7000 Loss: 6.0199995459697675e-06\n",
            "Epoch: 97 Step: 8000 Loss: 4.711541987489909e-05\n",
            "Epoch: 97 Step: 9000 Loss: 1.7881387748275301e-07\n",
            "Epoch: 97 Step: 10000 Loss: 6.347814178297995e-06\n",
            "Epoch: 97 Step: 11000 Loss: 0.0004570189048536122\n",
            "Epoch: 97 Step: 12000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 97 Step: 13000 Loss: 4.818669913220219e-05\n",
            "Epoch: 97 Step: 14000 Loss: 4.798141617357032e-06\n",
            "Accuracy: 0.9877\n",
            "Test Loss: 0.06252478021265055\n",
            "Epoch: 98 Step: 0 Loss: 1.251695948667475e-06\n",
            "Epoch: 98 Step: 1000 Loss: 3.3317024644929916e-05\n",
            "Epoch: 98 Step: 2000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 98 Step: 3000 Loss: 2.714915171964094e-05\n",
            "Epoch: 98 Step: 4000 Loss: 0.00011882419494213536\n",
            "Epoch: 98 Step: 5000 Loss: 3.5166626730642747e-06\n",
            "Epoch: 98 Step: 6000 Loss: 6.341212429106236e-05\n",
            "Epoch: 98 Step: 7000 Loss: 0.007336433045566082\n",
            "Epoch: 98 Step: 8000 Loss: 2.3065947971190326e-05\n",
            "Epoch: 98 Step: 9000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 98 Step: 10000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 98 Step: 11000 Loss: 2.0861619987044833e-07\n",
            "Epoch: 98 Step: 12000 Loss: 7.450575481016131e-07\n",
            "Epoch: 98 Step: 13000 Loss: 5.364412913877459e-07\n",
            "Epoch: 98 Step: 14000 Loss: 2.157613926101476e-05\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.06893307886510625\n",
            "Epoch: 99 Step: 0 Loss: 2.3841852225814364e-07\n",
            "Epoch: 99 Step: 1000 Loss: 1.7881387748275301e-07\n",
            "Epoch: 99 Step: 2000 Loss: 1.0132587703992613e-05\n",
            "Epoch: 99 Step: 3000 Loss: 7.15254714123148e-07\n",
            "Epoch: 99 Step: 4000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 99 Step: 5000 Loss: 0.003071356797590852\n",
            "Epoch: 99 Step: 6000 Loss: 0.0006175164598971605\n",
            "Epoch: 99 Step: 7000 Loss: 4.1723220078893064e-07\n",
            "Epoch: 99 Step: 8000 Loss: 4.511857696343213e-05\n",
            "Epoch: 99 Step: 9000 Loss: 8.940696005765858e-08\n",
            "Epoch: 99 Step: 10000 Loss: 0.2501404881477356\n",
            "Epoch: 99 Step: 11000 Loss: 0.002800217131152749\n",
            "Epoch: 99 Step: 12000 Loss: 0.006085364613682032\n",
            "Epoch: 99 Step: 13000 Loss: 3.874005051329732e-05\n",
            "Epoch: 99 Step: 14000 Loss: 3.188650225638412e-05\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.06570109483473581\n",
            "Epoch: 100 Step: 0 Loss: 5.960463766996327e-08\n",
            "Epoch: 100 Step: 1000 Loss: 0.0004582680412568152\n",
            "Epoch: 100 Step: 2000 Loss: 5.196989877731539e-05\n",
            "Epoch: 100 Step: 3000 Loss: 0.0003322882403153926\n",
            "Epoch: 100 Step: 4000 Loss: 3.516655169732985e-06\n",
            "Epoch: 100 Step: 5000 Loss: 0.004873656667768955\n",
            "Epoch: 100 Step: 6000 Loss: 0.0030248193070292473\n",
            "Epoch: 100 Step: 7000 Loss: 6.0405520343920216e-05\n",
            "Epoch: 100 Step: 8000 Loss: 5.900808446313022e-06\n",
            "Epoch: 100 Step: 9000 Loss: 8.910842552722897e-06\n",
            "Epoch: 100 Step: 10000 Loss: 0.000528925156686455\n",
            "Epoch: 100 Step: 11000 Loss: 5.6618006055941805e-05\n",
            "Epoch: 100 Step: 12000 Loss: 0.0008231247775256634\n",
            "Epoch: 100 Step: 13000 Loss: 2.3155331291491166e-05\n",
            "Epoch: 100 Step: 14000 Loss: 8.046623065638414e-07\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06434966824974343\n",
            "Epoch: 101 Step: 0 Loss: 2.0861621408130304e-07\n",
            "Epoch: 101 Step: 1000 Loss: 2.5569299396011047e-05\n",
            "Epoch: 101 Step: 2000 Loss: 8.739573968341574e-05\n",
            "Epoch: 101 Step: 3000 Loss: 6.556505240951083e-07\n",
            "Epoch: 101 Step: 4000 Loss: 1.4960327462176792e-05\n",
            "Epoch: 101 Step: 5000 Loss: 0.0001210873233503662\n",
            "Epoch: 101 Step: 6000 Loss: 0.0012477145064622164\n",
            "Epoch: 101 Step: 7000 Loss: 8.73198951012455e-06\n",
            "Epoch: 101 Step: 8000 Loss: 2.8610120352823287e-06\n",
            "Epoch: 101 Step: 9000 Loss: 5.7517954701324925e-06\n",
            "Epoch: 101 Step: 10000 Loss: 1.5020076716609765e-05\n",
            "Epoch: 101 Step: 11000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 101 Step: 12000 Loss: 0.0\n",
            "Epoch: 101 Step: 13000 Loss: 0.0005555640673264861\n",
            "Epoch: 101 Step: 14000 Loss: 6.765039415768115e-06\n",
            "Accuracy: 0.9874\n",
            "Test Loss: 0.06510404198842557\n",
            "Epoch: 102 Step: 0 Loss: 2.5629929041315336e-06\n",
            "Epoch: 102 Step: 1000 Loss: 0.000956646807026118\n",
            "Epoch: 102 Step: 2000 Loss: 2.592789542177343e-06\n",
            "Epoch: 102 Step: 3000 Loss: 1.2516956076069619e-06\n",
            "Epoch: 102 Step: 4000 Loss: 1.5288154827430844e-05\n",
            "Epoch: 102 Step: 5000 Loss: 5.066392532171449e-07\n",
            "Epoch: 102 Step: 6000 Loss: 2.330434654140845e-05\n",
            "Epoch: 102 Step: 7000 Loss: 1.0102808118972462e-05\n",
            "Epoch: 102 Step: 8000 Loss: 4.470346368634637e-07\n",
            "Epoch: 102 Step: 9000 Loss: 8.523391443304718e-06\n",
            "Epoch: 102 Step: 10000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 102 Step: 11000 Loss: 3.069629428864573e-06\n",
            "Epoch: 102 Step: 12000 Loss: 0.00014473938790615648\n",
            "Epoch: 102 Step: 13000 Loss: 6.645868324994808e-06\n",
            "Epoch: 102 Step: 14000 Loss: 6.55650637781946e-07\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.06557481091902181\n",
            "Epoch: 103 Step: 0 Loss: 6.937498255865648e-05\n",
            "Epoch: 103 Step: 1000 Loss: 6.258480880205752e-07\n",
            "Epoch: 103 Step: 2000 Loss: 5.364412913877459e-07\n",
            "Epoch: 103 Step: 3000 Loss: 5.367073026718572e-05\n",
            "Epoch: 103 Step: 4000 Loss: 2.3274549675988965e-05\n",
            "Epoch: 103 Step: 5000 Loss: 4.7683684556432127e-07\n",
            "Epoch: 103 Step: 6000 Loss: 0.0\n",
            "Epoch: 103 Step: 7000 Loss: 9.536729521641973e-07\n",
            "Epoch: 103 Step: 8000 Loss: 3.874300489314919e-07\n",
            "Epoch: 103 Step: 9000 Loss: 4.172322576323495e-07\n",
            "Epoch: 103 Step: 10000 Loss: 1.153154969215393\n",
            "Epoch: 103 Step: 11000 Loss: 5.349185084924102e-05\n",
            "Epoch: 103 Step: 12000 Loss: 4.559731223707786e-06\n",
            "Epoch: 103 Step: 13000 Loss: 4.6636316255899146e-05\n",
            "Epoch: 103 Step: 14000 Loss: 2.446652069920674e-05\n",
            "Accuracy: 0.9879\n",
            "Test Loss: 0.05986542272652325\n",
            "Epoch: 104 Step: 0 Loss: 1.7881073290482163e-05\n",
            "Epoch: 104 Step: 1000 Loss: 2.6822075938071066e-07\n",
            "Epoch: 104 Step: 2000 Loss: 2.6225909550703363e-06\n",
            "Epoch: 104 Step: 3000 Loss: 2.205367763963295e-06\n",
            "Epoch: 104 Step: 4000 Loss: 8.940684779190633e-07\n",
            "Epoch: 104 Step: 5000 Loss: 1.5884135791566223e-05\n",
            "Epoch: 104 Step: 6000 Loss: 0.0006376862293109298\n",
            "Epoch: 104 Step: 7000 Loss: 2.4139803826983552e-06\n",
            "Epoch: 104 Step: 8000 Loss: 6.765038961020764e-06\n",
            "Epoch: 104 Step: 9000 Loss: 4.4703452317662595e-07\n",
            "Epoch: 104 Step: 10000 Loss: 2.4735822989896405e-06\n",
            "Epoch: 104 Step: 11000 Loss: 0.0006701907841488719\n",
            "Epoch: 104 Step: 12000 Loss: 2.9802313861182483e-07\n",
            "Epoch: 104 Step: 13000 Loss: 1.725507536320947e-05\n",
            "Epoch: 104 Step: 14000 Loss: 1.3709058066524449e-06\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.06167517304426517\n",
            "Epoch: 105 Step: 0 Loss: 3.576264134608209e-06\n",
            "Epoch: 105 Step: 1000 Loss: 1.6391223880418693e-06\n",
            "Epoch: 105 Step: 2000 Loss: 8.22538095235359e-06\n",
            "Epoch: 105 Step: 3000 Loss: 7.297652337001637e-05\n",
            "Epoch: 105 Step: 4000 Loss: 1.0430801467009587e-06\n",
            "Epoch: 105 Step: 5000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 105 Step: 6000 Loss: 5.033133493270725e-05\n",
            "Epoch: 105 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 105 Step: 8000 Loss: 0.0008072921773418784\n",
            "Epoch: 105 Step: 9000 Loss: 0.00010753399692475796\n",
            "Epoch: 105 Step: 10000 Loss: 6.07963602305972e-06\n",
            "Epoch: 105 Step: 11000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 105 Step: 12000 Loss: 1.6807949577923864e-05\n",
            "Epoch: 105 Step: 13000 Loss: 0.0002555740938987583\n",
            "Epoch: 105 Step: 14000 Loss: 0.00012892148515675217\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.07234096751437478\n",
            "Epoch: 106 Step: 0 Loss: 6.854526191091281e-07\n",
            "Epoch: 106 Step: 1000 Loss: 0.0029706531204283237\n",
            "Epoch: 106 Step: 2000 Loss: 3.248441316827666e-06\n",
            "Epoch: 106 Step: 3000 Loss: 2.7029309421777725e-05\n",
            "Epoch: 106 Step: 4000 Loss: 0.0005990836070850492\n",
            "Epoch: 106 Step: 5000 Loss: 1.4006699530000333e-05\n",
            "Epoch: 106 Step: 6000 Loss: 3.8742987840123533e-07\n",
            "Epoch: 106 Step: 7000 Loss: 0.9557374119758606\n",
            "Epoch: 106 Step: 8000 Loss: 1.2665744179685134e-05\n",
            "Epoch: 106 Step: 9000 Loss: 1.1086336598964408e-05\n",
            "Epoch: 106 Step: 10000 Loss: 2.682207878024201e-07\n",
            "Epoch: 106 Step: 11000 Loss: 2.4735807073739124e-06\n",
            "Epoch: 106 Step: 12000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 106 Step: 13000 Loss: 8.642663260616246e-07\n",
            "Epoch: 106 Step: 14000 Loss: 0.0014912958722561598\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06445195426268772\n",
            "Epoch: 107 Step: 0 Loss: 4.136226561968215e-05\n",
            "Epoch: 107 Step: 1000 Loss: 0.11317846924066544\n",
            "Epoch: 107 Step: 2000 Loss: 0.00015546026406809688\n",
            "Epoch: 107 Step: 3000 Loss: 0.00021722863311879337\n",
            "Epoch: 107 Step: 4000 Loss: 3.576276128569589e-07\n",
            "Epoch: 107 Step: 5000 Loss: 1.7701979231787845e-05\n",
            "Epoch: 107 Step: 6000 Loss: 5.364414619180025e-07\n",
            "Epoch: 107 Step: 7000 Loss: 1.7881347957882099e-06\n",
            "Epoch: 107 Step: 8000 Loss: 2.1755636225861963e-06\n",
            "Epoch: 107 Step: 9000 Loss: 0.00027046847390010953\n",
            "Epoch: 107 Step: 10000 Loss: 4.931805597152561e-05\n",
            "Epoch: 107 Step: 11000 Loss: 1.5735191482235678e-05\n",
            "Epoch: 107 Step: 12000 Loss: 3.606055543059483e-06\n",
            "Epoch: 107 Step: 13000 Loss: 9.298160875914618e-06\n",
            "Epoch: 107 Step: 14000 Loss: 1.3709031918551773e-06\n",
            "Accuracy: 0.9873\n",
            "Test Loss: 0.06248319328497813\n",
            "Epoch: 108 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 108 Step: 1000 Loss: 0.0004177477676421404\n",
            "Epoch: 108 Step: 2000 Loss: 5.960463766996327e-08\n",
            "Epoch: 108 Step: 3000 Loss: 1.7314856449957006e-05\n",
            "Epoch: 108 Step: 4000 Loss: 5.662437843056978e-07\n",
            "Epoch: 108 Step: 5000 Loss: 0.00020528430468402803\n",
            "Epoch: 108 Step: 6000 Loss: 2.5688326786621474e-05\n",
            "Epoch: 108 Step: 7000 Loss: 6.0731617850251496e-05\n",
            "Epoch: 108 Step: 8000 Loss: 0.00018211423594038934\n",
            "Epoch: 108 Step: 9000 Loss: 0.0\n",
            "Epoch: 108 Step: 10000 Loss: 8.197287388611585e-05\n",
            "Epoch: 108 Step: 11000 Loss: 0.0\n",
            "Epoch: 108 Step: 12000 Loss: 5.84961635468062e-05\n",
            "Epoch: 108 Step: 13000 Loss: 0.0014572296058759093\n",
            "Epoch: 108 Step: 14000 Loss: 1.4901158351676713e-07\n",
            "Accuracy: 0.9869\n",
            "Test Loss: 0.06165943844811292\n",
            "Epoch: 109 Step: 0 Loss: 3.2422824006062e-05\n",
            "Epoch: 109 Step: 1000 Loss: 1.1265066859778017e-05\n",
            "Epoch: 109 Step: 2000 Loss: 0.0009475580300204456\n",
            "Epoch: 109 Step: 3000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 109 Step: 4000 Loss: 6.556502967214328e-07\n",
            "Epoch: 109 Step: 5000 Loss: 0.00012050315854139626\n",
            "Epoch: 109 Step: 6000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 109 Step: 7000 Loss: 3.2782546099952015e-07\n",
            "Epoch: 109 Step: 8000 Loss: 4.768367034557741e-07\n",
            "Epoch: 109 Step: 9000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 109 Step: 10000 Loss: 0.0008855989435687661\n",
            "Epoch: 109 Step: 11000 Loss: 1.6987271465040976e-06\n",
            "Epoch: 109 Step: 12000 Loss: 5.066391395303071e-07\n",
            "Epoch: 109 Step: 13000 Loss: 1.3411020063358592e-06\n",
            "Epoch: 109 Step: 14000 Loss: 5.662437843056978e-07\n",
            "Accuracy: 0.988\n",
            "Test Loss: 0.0635100687965439\n",
            "Epoch: 110 Step: 0 Loss: 0.04106313735246658\n",
            "Epoch: 110 Step: 1000 Loss: 0.0018802040722221136\n",
            "Epoch: 110 Step: 2000 Loss: 5.573015641857637e-06\n",
            "Epoch: 110 Step: 3000 Loss: 3.5762598145083757e-06\n",
            "Epoch: 110 Step: 4000 Loss: 0.0002763235825113952\n",
            "Epoch: 110 Step: 5000 Loss: 0.00017675472190603614\n",
            "Epoch: 110 Step: 6000 Loss: 1.5288278518710285e-05\n",
            "Epoch: 110 Step: 7000 Loss: 3.665659278340172e-06\n",
            "Epoch: 110 Step: 8000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 110 Step: 9000 Loss: 0.001270547043532133\n",
            "Epoch: 110 Step: 10000 Loss: 0.0001352244580630213\n",
            "Epoch: 110 Step: 11000 Loss: 2.0861619987044833e-07\n",
            "Epoch: 110 Step: 12000 Loss: 1.8775426724459976e-06\n",
            "Epoch: 110 Step: 13000 Loss: 7.510073373850901e-06\n",
            "Epoch: 110 Step: 14000 Loss: 0.00434418860822916\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.062598280929119\n",
            "Epoch: 111 Step: 0 Loss: 3.35552504111547e-05\n",
            "Epoch: 111 Step: 1000 Loss: 2.2947699562791968e-06\n",
            "Epoch: 111 Step: 2000 Loss: 0.006099242717027664\n",
            "Epoch: 111 Step: 3000 Loss: 0.00032942777033895254\n",
            "Epoch: 111 Step: 4000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 111 Step: 5000 Loss: 4.172293301962782e-06\n",
            "Epoch: 111 Step: 6000 Loss: 2.9802313861182483e-07\n",
            "Epoch: 111 Step: 7000 Loss: 1.3529945135815069e-05\n",
            "Epoch: 111 Step: 8000 Loss: 7.122656825231388e-06\n",
            "Epoch: 111 Step: 9000 Loss: 3.1292349831346655e-06\n",
            "Epoch: 111 Step: 10000 Loss: 3.2484390430909116e-06\n",
            "Epoch: 111 Step: 11000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 111 Step: 12000 Loss: 4.053089469380211e-06\n",
            "Epoch: 111 Step: 13000 Loss: 1.0669094990589656e-05\n",
            "Epoch: 111 Step: 14000 Loss: 3.516661308822222e-06\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.06787291900007403\n",
            "Epoch: 112 Step: 0 Loss: 0.00011822962551377714\n",
            "Epoch: 112 Step: 1000 Loss: 1.0132773695659125e-06\n",
            "Epoch: 112 Step: 2000 Loss: 0.00018342642579227686\n",
            "Epoch: 112 Step: 3000 Loss: 1.907343744278478e-06\n",
            "Epoch: 112 Step: 4000 Loss: 0.002065334701910615\n",
            "Epoch: 112 Step: 5000 Loss: 6.258481448639941e-07\n",
            "Epoch: 112 Step: 6000 Loss: 8.940696005765858e-08\n",
            "Epoch: 112 Step: 7000 Loss: 9.1000554675702e-05\n",
            "Epoch: 112 Step: 8000 Loss: 6.19882393948501e-06\n",
            "Epoch: 112 Step: 9000 Loss: 0.0004579697852022946\n",
            "Epoch: 112 Step: 10000 Loss: 0.14206790924072266\n",
            "Epoch: 112 Step: 11000 Loss: 0.0004593679332174361\n",
            "Epoch: 112 Step: 12000 Loss: 8.090031769825146e-05\n",
            "Epoch: 112 Step: 13000 Loss: 5.9007929849030916e-06\n",
            "Epoch: 112 Step: 14000 Loss: 3.486858076939825e-06\n",
            "Accuracy: 0.9874\n",
            "Test Loss: 0.06742740338112972\n",
            "Epoch: 113 Step: 0 Loss: 1.1920927533992653e-07\n",
            "Epoch: 113 Step: 1000 Loss: 4.890085619990714e-05\n",
            "Epoch: 113 Step: 2000 Loss: 1.5795187664480181e-06\n",
            "Epoch: 113 Step: 3000 Loss: 3.1886957003735006e-05\n",
            "Epoch: 113 Step: 4000 Loss: 1.0430803740746342e-06\n",
            "Epoch: 113 Step: 5000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 113 Step: 6000 Loss: 8.641201566206291e-05\n",
            "Epoch: 113 Step: 7000 Loss: 0.0\n",
            "Epoch: 113 Step: 8000 Loss: 4.470344947549165e-07\n",
            "Epoch: 113 Step: 9000 Loss: 2.4735807073739124e-06\n",
            "Epoch: 113 Step: 10000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 113 Step: 11000 Loss: 2.8908109470648924e-06\n",
            "Epoch: 113 Step: 12000 Loss: 3.9276390452869236e-05\n",
            "Epoch: 113 Step: 13000 Loss: 9.536732932247105e-07\n",
            "Epoch: 113 Step: 14000 Loss: 0.010766609571874142\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.07286931604931181\n",
            "Epoch: 114 Step: 0 Loss: 3.874300205097825e-07\n",
            "Epoch: 114 Step: 1000 Loss: 8.940696005765858e-08\n",
            "Epoch: 114 Step: 2000 Loss: 8.295589941553771e-05\n",
            "Epoch: 114 Step: 3000 Loss: 3.8742987840123533e-07\n",
            "Epoch: 114 Step: 4000 Loss: 6.258479743337375e-07\n",
            "Epoch: 114 Step: 5000 Loss: 1.7881389169360773e-07\n",
            "Epoch: 114 Step: 6000 Loss: 7.271690719790058e-06\n",
            "Epoch: 114 Step: 7000 Loss: 0.0004829361569136381\n",
            "Epoch: 114 Step: 8000 Loss: 3.3882944990182295e-05\n",
            "Epoch: 114 Step: 9000 Loss: 8.940695295223122e-08\n",
            "Epoch: 114 Step: 10000 Loss: 0.0\n",
            "Epoch: 114 Step: 11000 Loss: 1.3917303476773668e-05\n",
            "Epoch: 114 Step: 12000 Loss: 4.023297606181586e-06\n",
            "Epoch: 114 Step: 13000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 114 Step: 14000 Loss: 5.811409664602252e-06\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.05771910773510616\n",
            "Epoch: 115 Step: 0 Loss: 5.870997028978309e-06\n",
            "Epoch: 115 Step: 1000 Loss: 9.834750471782172e-07\n",
            "Epoch: 115 Step: 2000 Loss: 2.682208162241295e-07\n",
            "Epoch: 115 Step: 3000 Loss: 5.960463766996327e-08\n",
            "Epoch: 115 Step: 4000 Loss: 0.005273802671581507\n",
            "Epoch: 115 Step: 5000 Loss: 1.6391243207181105e-06\n",
            "Epoch: 115 Step: 6000 Loss: 0.0\n",
            "Epoch: 115 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 115 Step: 8000 Loss: 0.0\n",
            "Epoch: 115 Step: 9000 Loss: 1.2486871128203347e-05\n",
            "Epoch: 115 Step: 10000 Loss: 4.470344947549165e-07\n",
            "Epoch: 115 Step: 11000 Loss: 2.235167812614236e-06\n",
            "Epoch: 115 Step: 12000 Loss: 7.450574912581942e-07\n",
            "Epoch: 115 Step: 13000 Loss: 7.795072451699525e-05\n",
            "Epoch: 115 Step: 14000 Loss: 0.007382663898169994\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.06963547187148798\n",
            "Epoch: 116 Step: 0 Loss: 2.0861622829215776e-07\n",
            "Epoch: 116 Step: 1000 Loss: 0.0075243315659463406\n",
            "Epoch: 116 Step: 2000 Loss: 0.10768455266952515\n",
            "Epoch: 116 Step: 3000 Loss: 2.235164174635429e-06\n",
            "Epoch: 116 Step: 4000 Loss: 4.91735409013927e-06\n",
            "Epoch: 116 Step: 5000 Loss: 0.0011214162223041058\n",
            "Epoch: 116 Step: 6000 Loss: 2.67612595052924e-05\n",
            "Epoch: 116 Step: 7000 Loss: 2.8312051654211245e-06\n",
            "Epoch: 116 Step: 8000 Loss: 8.940696005765858e-08\n",
            "Epoch: 116 Step: 9000 Loss: 7.450570933542622e-07\n",
            "Epoch: 116 Step: 10000 Loss: 1.3649108950630762e-05\n",
            "Epoch: 116 Step: 11000 Loss: 0.0008405858534388244\n",
            "Epoch: 116 Step: 12000 Loss: 5.960457656328799e-07\n",
            "Epoch: 116 Step: 13000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 116 Step: 14000 Loss: 3.874299352446542e-07\n",
            "Accuracy: 0.9862\n",
            "Test Loss: 0.06677561559359818\n",
            "Epoch: 117 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 117 Step: 1000 Loss: 0.00013386608043219894\n",
            "Epoch: 117 Step: 2000 Loss: 6.169018888613209e-06\n",
            "Epoch: 117 Step: 3000 Loss: 3.954455314669758e-05\n",
            "Epoch: 117 Step: 4000 Loss: 5.960463766996327e-08\n",
            "Epoch: 117 Step: 5000 Loss: 0.00033012620406225324\n",
            "Epoch: 117 Step: 6000 Loss: 2.7446432795841247e-05\n",
            "Epoch: 117 Step: 7000 Loss: 0.0009134706342592835\n",
            "Epoch: 117 Step: 8000 Loss: 4.172321723672212e-07\n",
            "Epoch: 117 Step: 9000 Loss: 1.668927779974183e-06\n",
            "Epoch: 117 Step: 10000 Loss: 8.88025970198214e-05\n",
            "Epoch: 117 Step: 11000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 117 Step: 12000 Loss: 1.013277596939588e-06\n",
            "Epoch: 117 Step: 13000 Loss: 7.986899618117604e-06\n",
            "Epoch: 117 Step: 14000 Loss: 6.258480880205752e-07\n",
            "Accuracy: 0.9858\n",
            "Test Loss: 0.0743224769818889\n",
            "Epoch: 118 Step: 0 Loss: 0.008304896764457226\n",
            "Epoch: 118 Step: 1000 Loss: 5.721980869566323e-06\n",
            "Epoch: 118 Step: 2000 Loss: 9.983684321923647e-06\n",
            "Epoch: 118 Step: 3000 Loss: 6.854527327959659e-07\n",
            "Epoch: 118 Step: 4000 Loss: 9.484287875238806e-05\n",
            "Epoch: 118 Step: 5000 Loss: 2.0861621408130304e-07\n",
            "Epoch: 118 Step: 6000 Loss: 7.420685960823903e-06\n",
            "Epoch: 118 Step: 7000 Loss: 1.4185502550390083e-05\n",
            "Epoch: 118 Step: 8000 Loss: 1.4662316061730962e-05\n",
            "Epoch: 118 Step: 9000 Loss: 1.4901159772762185e-07\n",
            "Epoch: 118 Step: 10000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 118 Step: 11000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 118 Step: 12000 Loss: 1.6391223880418693e-06\n",
            "Epoch: 118 Step: 13000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 118 Step: 14000 Loss: 0.00403517484664917\n",
            "Accuracy: 0.9872\n",
            "Test Loss: 0.06417408073207766\n",
            "Epoch: 119 Step: 0 Loss: 2.1755606667284155e-06\n",
            "Epoch: 119 Step: 1000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 119 Step: 2000 Loss: 8.940696005765858e-08\n",
            "Epoch: 119 Step: 3000 Loss: 0.0775872990489006\n",
            "Epoch: 119 Step: 4000 Loss: 4.768367034557741e-07\n",
            "Epoch: 119 Step: 5000 Loss: 1.0132598617929034e-05\n",
            "Epoch: 119 Step: 6000 Loss: 1.8774770069285296e-05\n",
            "Epoch: 119 Step: 7000 Loss: 7.450570933542622e-07\n",
            "Epoch: 119 Step: 8000 Loss: 5.960463766996327e-08\n",
            "Epoch: 119 Step: 9000 Loss: 0.002301054773852229\n",
            "Epoch: 119 Step: 10000 Loss: 8.910737051337492e-06\n",
            "Epoch: 119 Step: 11000 Loss: 8.940695295223122e-08\n",
            "Epoch: 119 Step: 12000 Loss: 0.1215975433588028\n",
            "Epoch: 119 Step: 13000 Loss: 3.546451580405119e-06\n",
            "Epoch: 119 Step: 14000 Loss: 2.503383711882634e-06\n",
            "Accuracy: 0.9863\n",
            "Test Loss: 0.06334980088655456\n",
            "Epoch: 120 Step: 0 Loss: 5.960463766996327e-08\n",
            "Epoch: 120 Step: 1000 Loss: 3.546451580405119e-06\n",
            "Epoch: 120 Step: 2000 Loss: 5.960461066933931e-07\n",
            "Epoch: 120 Step: 3000 Loss: 5.662435569320223e-07\n",
            "Epoch: 120 Step: 4000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 120 Step: 5000 Loss: 1.6093206340883626e-06\n",
            "Epoch: 120 Step: 6000 Loss: 0.0002670649264473468\n",
            "Epoch: 120 Step: 7000 Loss: 0.00012756767682731152\n",
            "Epoch: 120 Step: 8000 Loss: 1.7285286730839289e-06\n",
            "Epoch: 120 Step: 9000 Loss: 1.430507836630568e-06\n",
            "Epoch: 120 Step: 10000 Loss: 4.172321723672212e-07\n",
            "Epoch: 120 Step: 11000 Loss: 2.598635728645604e-05\n",
            "Epoch: 120 Step: 12000 Loss: 7.450576049450319e-07\n",
            "Epoch: 120 Step: 13000 Loss: 0.05962695553898811\n",
            "Epoch: 120 Step: 14000 Loss: 7.59907197789289e-05\n",
            "Accuracy: 0.9868\n",
            "Test Loss: 0.07143189814613997\n",
            "Epoch: 121 Step: 0 Loss: 8.642662123747868e-07\n",
            "Epoch: 121 Step: 1000 Loss: 4.470344947549165e-07\n",
            "Epoch: 121 Step: 2000 Loss: 9.834757292992435e-07\n",
            "Epoch: 121 Step: 3000 Loss: 5.781585059594363e-06\n",
            "Epoch: 121 Step: 4000 Loss: 0.0019106201361864805\n",
            "Epoch: 121 Step: 5000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 121 Step: 6000 Loss: 3.278254041561013e-07\n",
            "Epoch: 121 Step: 7000 Loss: 3.576276412786683e-07\n",
            "Epoch: 121 Step: 8000 Loss: 5.662437843056978e-07\n",
            "Epoch: 121 Step: 9000 Loss: 2.205362079621409e-06\n",
            "Epoch: 121 Step: 10000 Loss: 0.019064627587795258\n",
            "Epoch: 121 Step: 11000 Loss: 1.1920914175789221e-06\n",
            "Epoch: 121 Step: 12000 Loss: 5.6624367061886e-07\n",
            "Epoch: 121 Step: 13000 Loss: 7.748595294287952e-07\n",
            "Epoch: 121 Step: 14000 Loss: 2.771600975393085e-06\n",
            "Accuracy: 0.9874\n",
            "Test Loss: 0.0687465461822558\n",
            "Epoch: 122 Step: 0 Loss: 1.0311413461749908e-05\n",
            "Epoch: 122 Step: 1000 Loss: 0.003194930963218212\n",
            "Epoch: 122 Step: 2000 Loss: 0.000631369708571583\n",
            "Epoch: 122 Step: 3000 Loss: 3.0696203339175554e-06\n",
            "Epoch: 122 Step: 4000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 122 Step: 5000 Loss: 0.0005436300998553634\n",
            "Epoch: 122 Step: 6000 Loss: 0.00018599249597173184\n",
            "Epoch: 122 Step: 7000 Loss: 0.0004639492544811219\n",
            "Epoch: 122 Step: 8000 Loss: 2.4735811621212633e-06\n",
            "Epoch: 122 Step: 9000 Loss: 1.090741170628462e-05\n",
            "Epoch: 122 Step: 10000 Loss: 7.152500984375365e-06\n",
            "Epoch: 122 Step: 11000 Loss: 0.00028665331774391234\n",
            "Epoch: 122 Step: 12000 Loss: 1.308290029555792e-05\n",
            "Epoch: 122 Step: 13000 Loss: 1.7583309954716242e-06\n",
            "Epoch: 122 Step: 14000 Loss: 8.197284478228539e-05\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.0643350484996768\n",
            "Epoch: 123 Step: 0 Loss: 0.004004272632300854\n",
            "Epoch: 123 Step: 1000 Loss: 1.3202256013755687e-05\n",
            "Epoch: 123 Step: 2000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 123 Step: 3000 Loss: 0.0008208085200749338\n",
            "Epoch: 123 Step: 4000 Loss: 5.5133709793153685e-06\n",
            "Epoch: 123 Step: 5000 Loss: 3.120116161881015e-05\n",
            "Epoch: 123 Step: 6000 Loss: 1.3053186194156297e-05\n",
            "Epoch: 123 Step: 7000 Loss: 7.748598136458895e-07\n",
            "Epoch: 123 Step: 8000 Loss: 0.0002425923739792779\n",
            "Epoch: 123 Step: 9000 Loss: 6.914043297001626e-06\n",
            "Epoch: 123 Step: 10000 Loss: 4.7087223720154725e-06\n",
            "Epoch: 123 Step: 11000 Loss: 5.36441234544327e-07\n",
            "Epoch: 123 Step: 12000 Loss: 1.659937515796628e-05\n",
            "Epoch: 123 Step: 13000 Loss: 8.224089833674952e-05\n",
            "Epoch: 123 Step: 14000 Loss: 2.6821949177247006e-06\n",
            "Accuracy: 0.9882\n",
            "Test Loss: 0.061551470698227355\n",
            "Epoch: 124 Step: 0 Loss: 0.0\n",
            "Epoch: 124 Step: 1000 Loss: 0.00019703505677171052\n",
            "Epoch: 124 Step: 2000 Loss: 1.9251563571742736e-05\n",
            "Epoch: 124 Step: 3000 Loss: 0.0\n",
            "Epoch: 124 Step: 4000 Loss: 0.004790253005921841\n",
            "Epoch: 124 Step: 5000 Loss: 9.477023922954686e-06\n",
            "Epoch: 124 Step: 6000 Loss: 6.675631084362976e-06\n",
            "Epoch: 124 Step: 7000 Loss: 4.470311523618875e-06\n",
            "Epoch: 124 Step: 8000 Loss: 1.1920900533368695e-06\n",
            "Epoch: 124 Step: 9000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 124 Step: 10000 Loss: 1.311298774453462e-06\n",
            "Epoch: 124 Step: 11000 Loss: 3.063529948121868e-05\n",
            "Epoch: 124 Step: 12000 Loss: 0.00671650655567646\n",
            "Epoch: 124 Step: 13000 Loss: 5.960463766996327e-08\n",
            "Epoch: 124 Step: 14000 Loss: 7.688883670198265e-06\n",
            "Accuracy: 0.9888\n",
            "Test Loss: 0.05191444061010659\n",
            "Epoch: 125 Step: 0 Loss: 1.1920926112907182e-07\n",
            "Epoch: 125 Step: 1000 Loss: 0.0005898072267882526\n",
            "Epoch: 125 Step: 2000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 125 Step: 3000 Loss: 0.00015155850269366056\n",
            "Epoch: 125 Step: 4000 Loss: 5.364414619180025e-07\n",
            "Epoch: 125 Step: 5000 Loss: 1.8029759303317405e-05\n",
            "Epoch: 125 Step: 6000 Loss: 9.457940905122086e-05\n",
            "Epoch: 125 Step: 7000 Loss: 3.552316047716886e-05\n",
            "Epoch: 125 Step: 8000 Loss: 2.0682071408373304e-05\n",
            "Epoch: 125 Step: 9000 Loss: 9.417444744030945e-06\n",
            "Epoch: 125 Step: 10000 Loss: 1.4901131635269849e-06\n",
            "Epoch: 125 Step: 11000 Loss: 9.120826871367171e-05\n",
            "Epoch: 125 Step: 12000 Loss: 3.8743007735320134e-07\n",
            "Epoch: 125 Step: 13000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 125 Step: 14000 Loss: 0.0011213944526389241\n",
            "Accuracy: 0.9877\n",
            "Test Loss: 0.06838109727577589\n",
            "Epoch: 126 Step: 0 Loss: 0.001721048611216247\n",
            "Epoch: 126 Step: 1000 Loss: 0.12228982150554657\n",
            "Epoch: 126 Step: 2000 Loss: 0.0009176869061775506\n",
            "Epoch: 126 Step: 3000 Loss: 0.0020341507624834776\n",
            "Epoch: 126 Step: 4000 Loss: 0.0004420583136379719\n",
            "Epoch: 126 Step: 5000 Loss: 0.0\n",
            "Epoch: 126 Step: 6000 Loss: 5.992619480821304e-05\n",
            "Epoch: 126 Step: 7000 Loss: 1.877475733635947e-05\n",
            "Epoch: 126 Step: 8000 Loss: 1.0132772558790748e-06\n",
            "Epoch: 126 Step: 9000 Loss: 2.8908120839332696e-06\n",
            "Epoch: 126 Step: 10000 Loss: 5.066390258434694e-07\n",
            "Epoch: 126 Step: 11000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 126 Step: 12000 Loss: 8.940696005765858e-08\n",
            "Epoch: 126 Step: 13000 Loss: 5.6350625527556986e-05\n",
            "Epoch: 126 Step: 14000 Loss: 3.278254325778107e-07\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.0674863711010249\n",
            "Epoch: 127 Step: 0 Loss: 0.0\n",
            "Epoch: 127 Step: 1000 Loss: 1.8894072127295658e-05\n",
            "Epoch: 127 Step: 2000 Loss: 0.00018119228479918092\n",
            "Epoch: 127 Step: 3000 Loss: 0.0006144244689494371\n",
            "Epoch: 127 Step: 4000 Loss: 1.4305087461252697e-06\n",
            "Epoch: 127 Step: 5000 Loss: 0.00048777737538330257\n",
            "Epoch: 127 Step: 6000 Loss: 4.7683681714261184e-07\n",
            "Epoch: 127 Step: 7000 Loss: 1.8477381900083856e-06\n",
            "Epoch: 127 Step: 8000 Loss: 4.142489160585683e-06\n",
            "Epoch: 127 Step: 9000 Loss: 1.4007071058586007e-06\n",
            "Epoch: 127 Step: 10000 Loss: 0.0\n",
            "Epoch: 127 Step: 11000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 127 Step: 12000 Loss: 5.662438979925355e-07\n",
            "Epoch: 127 Step: 13000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 127 Step: 14000 Loss: 6.8246758928580675e-06\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.0710573856911843\n",
            "Epoch: 128 Step: 0 Loss: 2.544989729358349e-05\n",
            "Epoch: 128 Step: 1000 Loss: 1.7881329767988063e-06\n",
            "Epoch: 128 Step: 2000 Loss: 5.811394203192322e-06\n",
            "Epoch: 128 Step: 3000 Loss: 0.01756943203508854\n",
            "Epoch: 128 Step: 4000 Loss: 9.928202052833512e-05\n",
            "Epoch: 128 Step: 5000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 128 Step: 6000 Loss: 1.4603107274524518e-06\n",
            "Epoch: 128 Step: 7000 Loss: 1.192090621771058e-06\n",
            "Epoch: 128 Step: 8000 Loss: 4.7683693082944956e-07\n",
            "Epoch: 128 Step: 9000 Loss: 0.0\n",
            "Epoch: 128 Step: 10000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 128 Step: 11000 Loss: 2.3543730094388593e-06\n",
            "Epoch: 128 Step: 12000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 128 Step: 13000 Loss: 3.576276128569589e-07\n",
            "Epoch: 128 Step: 14000 Loss: 0.0011417692294344306\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.07720011147826106\n",
            "Epoch: 129 Step: 0 Loss: 0.0014725385699421167\n",
            "Epoch: 129 Step: 1000 Loss: 0.0001519978977739811\n",
            "Epoch: 129 Step: 2000 Loss: 7.450572638845188e-07\n",
            "Epoch: 129 Step: 3000 Loss: 7.510103387176059e-06\n",
            "Epoch: 129 Step: 4000 Loss: 5.781585059594363e-06\n",
            "Epoch: 129 Step: 5000 Loss: 0.00035829495755024254\n",
            "Epoch: 129 Step: 6000 Loss: 9.447157935937867e-06\n",
            "Epoch: 129 Step: 7000 Loss: 0.0005750940181314945\n",
            "Epoch: 129 Step: 8000 Loss: 2.175564077333547e-06\n",
            "Epoch: 129 Step: 9000 Loss: 1.579518425387505e-06\n",
            "Epoch: 129 Step: 10000 Loss: 0.00045300187775865197\n",
            "Epoch: 129 Step: 11000 Loss: 6.6160387177660596e-06\n",
            "Epoch: 129 Step: 12000 Loss: 5.319195406627841e-05\n",
            "Epoch: 129 Step: 13000 Loss: 1.9371452708583092e-06\n",
            "Epoch: 129 Step: 14000 Loss: 1.7881390590446244e-07\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.0778081267542043\n",
            "Epoch: 130 Step: 0 Loss: 8.34455931908451e-06\n",
            "Epoch: 130 Step: 1000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 130 Step: 2000 Loss: 2.270834011142142e-05\n",
            "Epoch: 130 Step: 3000 Loss: 3.069621698159608e-06\n",
            "Epoch: 130 Step: 4000 Loss: 5.721983143303078e-06\n",
            "Epoch: 130 Step: 5000 Loss: 3.188828259226284e-06\n",
            "Epoch: 130 Step: 6000 Loss: 6.701705569867045e-05\n",
            "Epoch: 130 Step: 7000 Loss: 0.020871367305517197\n",
            "Epoch: 130 Step: 8000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 130 Step: 9000 Loss: 0.0550113320350647\n",
            "Epoch: 130 Step: 10000 Loss: 9.834757292992435e-07\n",
            "Epoch: 130 Step: 11000 Loss: 1.8775422176986467e-06\n",
            "Epoch: 130 Step: 12000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 130 Step: 13000 Loss: 0.0\n",
            "Epoch: 130 Step: 14000 Loss: 1.3470354133460205e-05\n",
            "Accuracy: 0.9855\n",
            "Test Loss: 0.07987284686412936\n",
            "Epoch: 131 Step: 0 Loss: 5.6623844102432486e-06\n",
            "Epoch: 131 Step: 1000 Loss: 2.5629901756474283e-06\n",
            "Epoch: 131 Step: 2000 Loss: 1.5407355022034608e-05\n",
            "Epoch: 131 Step: 3000 Loss: 5.960463766996327e-08\n",
            "Epoch: 131 Step: 4000 Loss: 0.0\n",
            "Epoch: 131 Step: 5000 Loss: 0.0020111454650759697\n",
            "Epoch: 131 Step: 6000 Loss: 1.192090621771058e-06\n",
            "Epoch: 131 Step: 7000 Loss: 2.658228731888812e-05\n",
            "Epoch: 131 Step: 8000 Loss: 0.0007386609213426709\n",
            "Epoch: 131 Step: 9000 Loss: 6.960854079807177e-05\n",
            "Epoch: 131 Step: 10000 Loss: 0.00016615238564554602\n",
            "Epoch: 131 Step: 11000 Loss: 2.2947701836528722e-06\n",
            "Epoch: 131 Step: 12000 Loss: 1.1026837682948099e-06\n",
            "Epoch: 131 Step: 13000 Loss: 4.768367034557741e-07\n",
            "Epoch: 131 Step: 14000 Loss: 5.960463766996327e-08\n",
            "Accuracy: 0.9858\n",
            "Test Loss: 0.07514834163877657\n",
            "Epoch: 132 Step: 0 Loss: 2.9802316703353426e-07\n",
            "Epoch: 132 Step: 1000 Loss: 0.0\n",
            "Epoch: 132 Step: 2000 Loss: 5.036542916059261e-06\n",
            "Epoch: 132 Step: 3000 Loss: 3.27825318890973e-07\n",
            "Epoch: 132 Step: 4000 Loss: 2.1188561731833033e-05\n",
            "Epoch: 132 Step: 5000 Loss: 5.960463766996327e-08\n",
            "Epoch: 132 Step: 6000 Loss: 7.152548846534046e-07\n",
            "Epoch: 132 Step: 7000 Loss: 1.0132770285053994e-06\n",
            "Epoch: 132 Step: 8000 Loss: 0.0009038080461323261\n",
            "Epoch: 132 Step: 9000 Loss: 6.437235242628958e-06\n",
            "Epoch: 132 Step: 10000 Loss: 0.02847030945122242\n",
            "Epoch: 132 Step: 11000 Loss: 5.364415187614213e-07\n",
            "Epoch: 132 Step: 12000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 132 Step: 13000 Loss: 2.7418011541158194e-06\n",
            "Epoch: 132 Step: 14000 Loss: 4.678923232859233e-06\n",
            "Accuracy: 0.9837\n",
            "Test Loss: 0.0863751782457652\n",
            "Epoch: 133 Step: 0 Loss: 0.0\n",
            "Epoch: 133 Step: 1000 Loss: 8.344644584212801e-07\n",
            "Epoch: 133 Step: 2000 Loss: 0.06607615202665329\n",
            "Epoch: 133 Step: 3000 Loss: 6.616043719986919e-06\n",
            "Epoch: 133 Step: 4000 Loss: 3.1590273010806413e-06\n",
            "Epoch: 133 Step: 5000 Loss: 2.270861659781076e-05\n",
            "Epoch: 133 Step: 6000 Loss: 7.74859188368282e-07\n",
            "Epoch: 133 Step: 7000 Loss: 1.1026843367289985e-06\n",
            "Epoch: 133 Step: 8000 Loss: 2.562986537668621e-06\n",
            "Epoch: 133 Step: 9000 Loss: 1.7285299236391438e-06\n",
            "Epoch: 133 Step: 10000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 133 Step: 11000 Loss: 0.0014132063370198011\n",
            "Epoch: 133 Step: 12000 Loss: 5.3757608839077875e-05\n",
            "Epoch: 133 Step: 13000 Loss: 1.305309524468612e-05\n",
            "Epoch: 133 Step: 14000 Loss: 1.0728819006544654e-06\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.07305693218793413\n",
            "Epoch: 134 Step: 0 Loss: 2.086161714487389e-07\n",
            "Epoch: 134 Step: 1000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 134 Step: 2000 Loss: 1.3321296137291938e-05\n",
            "Epoch: 134 Step: 3000 Loss: 4.1723228605405893e-07\n",
            "Epoch: 134 Step: 4000 Loss: 0.001352358260191977\n",
            "Epoch: 134 Step: 5000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 134 Step: 6000 Loss: 5.364413482311647e-07\n",
            "Epoch: 134 Step: 7000 Loss: 7.15254714123148e-07\n",
            "Epoch: 134 Step: 8000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 134 Step: 9000 Loss: 1.7881389169360773e-07\n",
            "Epoch: 134 Step: 10000 Loss: 0.0\n",
            "Epoch: 134 Step: 11000 Loss: 3.129227025056025e-06\n",
            "Epoch: 134 Step: 12000 Loss: 3.2782440939627122e-06\n",
            "Epoch: 134 Step: 13000 Loss: 1.1622882993833628e-06\n",
            "Epoch: 134 Step: 14000 Loss: 1.8477379626347101e-06\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.07175321493242144\n",
            "Epoch: 135 Step: 0 Loss: 4.172321723672212e-07\n",
            "Epoch: 135 Step: 1000 Loss: 3.993487098341575e-06\n",
            "Epoch: 135 Step: 2000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 135 Step: 3000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 135 Step: 4000 Loss: 2.9502582037821412e-05\n",
            "Epoch: 135 Step: 5000 Loss: 2.682207878024201e-07\n",
            "Epoch: 135 Step: 6000 Loss: 2.8014048893965082e-06\n",
            "Epoch: 135 Step: 7000 Loss: 1.1920928244535389e-07\n",
            "Epoch: 135 Step: 8000 Loss: 8.344643447344424e-07\n",
            "Epoch: 135 Step: 9000 Loss: 5.36441234544327e-07\n",
            "Epoch: 135 Step: 10000 Loss: 1.817935526560177e-06\n",
            "Epoch: 135 Step: 11000 Loss: 8.940696005765858e-08\n",
            "Epoch: 135 Step: 12000 Loss: 1.937143679242581e-06\n",
            "Epoch: 135 Step: 13000 Loss: 0.0010713068768382072\n",
            "Epoch: 135 Step: 14000 Loss: 1.7881389169360773e-07\n",
            "Accuracy: 0.9846\n",
            "Test Loss: 0.08235430957355881\n",
            "Epoch: 136 Step: 0 Loss: 0.00037772729410789907\n",
            "Epoch: 136 Step: 1000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 136 Step: 2000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 136 Step: 3000 Loss: 2.980223371196189e-06\n",
            "Epoch: 136 Step: 4000 Loss: 6.305491115199402e-05\n",
            "Epoch: 136 Step: 5000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 136 Step: 6000 Loss: 7.95595406088978e-05\n",
            "Epoch: 136 Step: 7000 Loss: 0.00025463695055805147\n",
            "Epoch: 136 Step: 8000 Loss: 7.539924354205141e-06\n",
            "Epoch: 136 Step: 9000 Loss: 1.1235242709517479e-05\n",
            "Epoch: 136 Step: 10000 Loss: 2.9144986910978332e-05\n",
            "Epoch: 136 Step: 11000 Loss: 0.0002736496098805219\n",
            "Epoch: 136 Step: 12000 Loss: 6.169047082948964e-06\n",
            "Epoch: 136 Step: 13000 Loss: 2.4555907657486387e-05\n",
            "Epoch: 136 Step: 14000 Loss: 4.937757330480963e-05\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.08072591038051767\n",
            "Epoch: 137 Step: 0 Loss: 1.6480154954479076e-05\n",
            "Epoch: 137 Step: 1000 Loss: 9.523012704448774e-05\n",
            "Epoch: 137 Step: 2000 Loss: 1.13248734123772e-06\n",
            "Epoch: 137 Step: 3000 Loss: 1.0400872270110995e-05\n",
            "Epoch: 137 Step: 4000 Loss: 0.0034006356727331877\n",
            "Epoch: 137 Step: 5000 Loss: 0.0003557945601642132\n",
            "Epoch: 137 Step: 6000 Loss: 7.074096356518567e-05\n",
            "Epoch: 137 Step: 7000 Loss: 0.0\n",
            "Epoch: 137 Step: 8000 Loss: 1.0162385478906799e-05\n",
            "Epoch: 137 Step: 9000 Loss: 0.00040237276698462665\n",
            "Epoch: 137 Step: 10000 Loss: 6.020001364959171e-06\n",
            "Epoch: 137 Step: 11000 Loss: 7.748592452117009e-07\n",
            "Epoch: 137 Step: 12000 Loss: 1.0728820143413031e-06\n",
            "Epoch: 137 Step: 13000 Loss: 1.0192217814619653e-05\n",
            "Epoch: 137 Step: 14000 Loss: 2.8312128961260896e-06\n",
            "Accuracy: 0.9862\n",
            "Test Loss: 0.07840345706131734\n",
            "Epoch: 138 Step: 0 Loss: 0.0006418614648282528\n",
            "Epoch: 138 Step: 1000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 138 Step: 2000 Loss: 9.24000414670445e-05\n",
            "Epoch: 138 Step: 3000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 138 Step: 4000 Loss: 8.775320020504296e-05\n",
            "Epoch: 138 Step: 5000 Loss: 8.940696005765858e-08\n",
            "Epoch: 138 Step: 6000 Loss: 2.6524003260419704e-06\n",
            "Epoch: 138 Step: 7000 Loss: 0.0\n",
            "Epoch: 138 Step: 8000 Loss: 8.046618518164905e-07\n",
            "Epoch: 138 Step: 9000 Loss: 9.059824151336215e-06\n",
            "Epoch: 138 Step: 10000 Loss: 7.659079528821167e-06\n",
            "Epoch: 138 Step: 11000 Loss: 3.576276128569589e-07\n",
            "Epoch: 138 Step: 12000 Loss: 0.00034811694058589637\n",
            "Epoch: 138 Step: 13000 Loss: 6.251790910027921e-05\n",
            "Epoch: 138 Step: 14000 Loss: 4.768369876728684e-07\n",
            "Accuracy: 0.9855\n",
            "Test Loss: 0.07447936959856827\n",
            "Epoch: 139 Step: 0 Loss: 2.6822084464583895e-07\n",
            "Epoch: 139 Step: 1000 Loss: 1.2218931715324288e-06\n",
            "Epoch: 139 Step: 2000 Loss: 2.8312133508734405e-06\n",
            "Epoch: 139 Step: 3000 Loss: 8.940695295223122e-08\n",
            "Epoch: 139 Step: 4000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 139 Step: 5000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 139 Step: 6000 Loss: 1.0460398698342033e-05\n",
            "Epoch: 139 Step: 7000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 139 Step: 8000 Loss: 5.364412913877459e-07\n",
            "Epoch: 139 Step: 9000 Loss: 8.940695295223122e-08\n",
            "Epoch: 139 Step: 10000 Loss: 0.00012281503586564213\n",
            "Epoch: 139 Step: 11000 Loss: 8.995724783744663e-05\n",
            "Epoch: 139 Step: 12000 Loss: 1.2904120922030415e-05\n",
            "Epoch: 139 Step: 13000 Loss: 0.00374203035607934\n",
            "Epoch: 139 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.07839323366468823\n",
            "Epoch: 140 Step: 0 Loss: 4.4703443791149766e-07\n",
            "Epoch: 140 Step: 1000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 140 Step: 2000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 140 Step: 3000 Loss: 8.344516572833527e-06\n",
            "Epoch: 140 Step: 4000 Loss: 5.42398856850923e-06\n",
            "Epoch: 140 Step: 5000 Loss: 3.295933493063785e-05\n",
            "Epoch: 140 Step: 6000 Loss: 1.9162253011018038e-05\n",
            "Epoch: 140 Step: 7000 Loss: 1.877539261840866e-06\n",
            "Epoch: 140 Step: 8000 Loss: 7.748596999590518e-07\n",
            "Epoch: 140 Step: 9000 Loss: 9.83475501925568e-07\n",
            "Epoch: 140 Step: 10000 Loss: 5.960463766996327e-08\n",
            "Epoch: 140 Step: 11000 Loss: 0.29308265447616577\n",
            "Epoch: 140 Step: 12000 Loss: 8.940696005765858e-08\n",
            "Epoch: 140 Step: 13000 Loss: 2.7418047920946265e-06\n",
            "Epoch: 140 Step: 14000 Loss: 2.6821949177247006e-06\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.07221902195521411\n",
            "Epoch: 141 Step: 0 Loss: 0.007395741529762745\n",
            "Epoch: 141 Step: 1000 Loss: 4.049987546750344e-05\n",
            "Epoch: 141 Step: 2000 Loss: 0.04400869458913803\n",
            "Epoch: 141 Step: 3000 Loss: 7.5994767030351795e-06\n",
            "Epoch: 141 Step: 4000 Loss: 4.738524239655817e-06\n",
            "Epoch: 141 Step: 5000 Loss: 7.837900739104953e-06\n",
            "Epoch: 141 Step: 6000 Loss: 8.940537554735783e-06\n",
            "Epoch: 141 Step: 7000 Loss: 1.1622884130702005e-06\n",
            "Epoch: 141 Step: 8000 Loss: 0.02961660735309124\n",
            "Epoch: 141 Step: 9000 Loss: 1.129487827711273e-05\n",
            "Epoch: 141 Step: 10000 Loss: 1.4900729183864314e-05\n",
            "Epoch: 141 Step: 11000 Loss: 8.940696005765858e-08\n",
            "Epoch: 141 Step: 12000 Loss: 3.7252771107887384e-06\n",
            "Epoch: 141 Step: 13000 Loss: 3.6954606912331656e-06\n",
            "Epoch: 141 Step: 14000 Loss: 2.461606709402986e-05\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.06958436234147132\n",
            "Epoch: 142 Step: 0 Loss: 1.2457097909646109e-05\n",
            "Epoch: 142 Step: 1000 Loss: 5.066391395303071e-07\n",
            "Epoch: 142 Step: 2000 Loss: 7.461423228960484e-05\n",
            "Epoch: 142 Step: 3000 Loss: 0.0006551198894158006\n",
            "Epoch: 142 Step: 4000 Loss: 6.258483153942507e-07\n",
            "Epoch: 142 Step: 5000 Loss: 4.925840403302573e-05\n",
            "Epoch: 142 Step: 6000 Loss: 3.117130836471915e-05\n",
            "Epoch: 142 Step: 7000 Loss: 0.04236121475696564\n",
            "Epoch: 142 Step: 8000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 142 Step: 9000 Loss: 6.139225206425181e-06\n",
            "Epoch: 142 Step: 10000 Loss: 0.00041717878775671124\n",
            "Epoch: 142 Step: 11000 Loss: 3.0398193757719127e-06\n",
            "Epoch: 142 Step: 12000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 142 Step: 13000 Loss: 6.288245003815973e-06\n",
            "Epoch: 142 Step: 14000 Loss: 3.039818466277211e-06\n",
            "Accuracy: 0.9852\n",
            "Test Loss: 0.08079310307900932\n",
            "Epoch: 143 Step: 0 Loss: 8.67236485646572e-06\n",
            "Epoch: 143 Step: 1000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 143 Step: 2000 Loss: 7.688359619351104e-05\n",
            "Epoch: 143 Step: 3000 Loss: 2.23516462938278e-06\n",
            "Epoch: 143 Step: 4000 Loss: 1.0311447113053873e-05\n",
            "Epoch: 143 Step: 5000 Loss: 2.7714653697330505e-05\n",
            "Epoch: 143 Step: 6000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 143 Step: 7000 Loss: 1.937147771968739e-06\n",
            "Epoch: 143 Step: 8000 Loss: 5.781621439382434e-06\n",
            "Epoch: 143 Step: 9000 Loss: 3.069628291996196e-06\n",
            "Epoch: 143 Step: 10000 Loss: 1.1026835409211344e-06\n",
            "Epoch: 143 Step: 11000 Loss: 0.02319915220141411\n",
            "Epoch: 143 Step: 12000 Loss: 1.4274978639150504e-05\n",
            "Epoch: 143 Step: 13000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 143 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9868\n",
            "Test Loss: 0.07586012672694599\n",
            "Epoch: 144 Step: 0 Loss: 2.056356834145845e-06\n",
            "Epoch: 144 Step: 1000 Loss: 4.7232235374394804e-05\n",
            "Epoch: 144 Step: 2000 Loss: 2.086161714487389e-07\n",
            "Epoch: 144 Step: 3000 Loss: 2.980217232106952e-06\n",
            "Epoch: 144 Step: 4000 Loss: 0.00030216199229471385\n",
            "Epoch: 144 Step: 5000 Loss: 0.0019310778006911278\n",
            "Epoch: 144 Step: 6000 Loss: 8.752096618991345e-05\n",
            "Epoch: 144 Step: 7000 Loss: 3.5166656289220555e-06\n",
            "Epoch: 144 Step: 8000 Loss: 4.172323428974778e-07\n",
            "Epoch: 144 Step: 9000 Loss: 2.589719952084124e-05\n",
            "Epoch: 144 Step: 10000 Loss: 9.53673520598386e-07\n",
            "Epoch: 144 Step: 11000 Loss: 1.0430799193272833e-06\n",
            "Epoch: 144 Step: 12000 Loss: 2.7833888452732936e-05\n",
            "Epoch: 144 Step: 13000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 144 Step: 14000 Loss: 5.527723624254577e-05\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.07836885248597474\n",
            "Epoch: 145 Step: 0 Loss: 5.170169970369898e-05\n",
            "Epoch: 145 Step: 1000 Loss: 1.2427259207470343e-05\n",
            "Epoch: 145 Step: 2000 Loss: 3.188829396094661e-06\n",
            "Epoch: 145 Step: 3000 Loss: 8.346488903043792e-05\n",
            "Epoch: 145 Step: 4000 Loss: 2.294775185873732e-06\n",
            "Epoch: 145 Step: 5000 Loss: 0.057596247643232346\n",
            "Epoch: 145 Step: 6000 Loss: 4.470345800200448e-07\n",
            "Epoch: 145 Step: 7000 Loss: 2.0175530153210275e-05\n",
            "Epoch: 145 Step: 8000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 145 Step: 9000 Loss: 0.00015456831897608936\n",
            "Epoch: 145 Step: 10000 Loss: 8.076315680227708e-06\n",
            "Epoch: 145 Step: 11000 Loss: 1.6927218894124962e-05\n",
            "Epoch: 145 Step: 12000 Loss: 2.0861619987044833e-07\n",
            "Epoch: 145 Step: 13000 Loss: 8.344643447344424e-07\n",
            "Epoch: 145 Step: 14000 Loss: 2.294768592037144e-06\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.07338489080741842\n",
            "Epoch: 146 Step: 0 Loss: 7.729527715127915e-05\n",
            "Epoch: 146 Step: 1000 Loss: 4.172293301962782e-06\n",
            "Epoch: 146 Step: 2000 Loss: 2.8907288651680574e-05\n",
            "Epoch: 146 Step: 3000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 146 Step: 4000 Loss: 7.182285116869025e-06\n",
            "Epoch: 146 Step: 5000 Loss: 2.84298621409107e-05\n",
            "Epoch: 146 Step: 6000 Loss: 6.318020496109966e-06\n",
            "Epoch: 146 Step: 7000 Loss: 0.00039257665048353374\n",
            "Epoch: 146 Step: 8000 Loss: 1.7195374311995693e-05\n",
            "Epoch: 146 Step: 9000 Loss: 2.2559841454494745e-05\n",
            "Epoch: 146 Step: 10000 Loss: 6.70545841785497e-06\n",
            "Epoch: 146 Step: 11000 Loss: 7.735492545180023e-05\n",
            "Epoch: 146 Step: 12000 Loss: 1.4603094768972369e-06\n",
            "Epoch: 146 Step: 13000 Loss: 1.5586150766466744e-05\n",
            "Epoch: 146 Step: 14000 Loss: 6.5564436226850376e-06\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.07303493460700333\n",
            "Epoch: 147 Step: 0 Loss: 3.844487309834221e-06\n",
            "Epoch: 147 Step: 1000 Loss: 2.559896165621467e-05\n",
            "Epoch: 147 Step: 2000 Loss: 2.7119251171825454e-05\n",
            "Epoch: 147 Step: 3000 Loss: 1.4185705367708579e-05\n",
            "Epoch: 147 Step: 4000 Loss: 5.960463766996327e-08\n",
            "Epoch: 147 Step: 5000 Loss: 0.009111416526138783\n",
            "Epoch: 147 Step: 6000 Loss: 6.496833975688787e-06\n",
            "Epoch: 147 Step: 7000 Loss: 0.00034964221413247287\n",
            "Epoch: 147 Step: 8000 Loss: 3.2095107599161565e-05\n",
            "Epoch: 147 Step: 9000 Loss: 5.3400075557874516e-05\n",
            "Epoch: 147 Step: 10000 Loss: 5.811392838950269e-06\n",
            "Epoch: 147 Step: 11000 Loss: 5.960463766996327e-08\n",
            "Epoch: 147 Step: 12000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 147 Step: 13000 Loss: 2.252998274343554e-05\n",
            "Epoch: 147 Step: 14000 Loss: 6.347816452034749e-06\n",
            "Accuracy: 0.9869\n",
            "Test Loss: 0.06923664222950239\n",
            "Epoch: 148 Step: 0 Loss: 1.7672155081527308e-05\n",
            "Epoch: 148 Step: 1000 Loss: 3.278254041561013e-07\n",
            "Epoch: 148 Step: 2000 Loss: 3.260173980379477e-05\n",
            "Epoch: 148 Step: 3000 Loss: 3.457057573541533e-06\n",
            "Epoch: 148 Step: 4000 Loss: 9.50679986999603e-06\n",
            "Epoch: 148 Step: 5000 Loss: 0.00014703496708534658\n",
            "Epoch: 148 Step: 6000 Loss: 9.119426977122203e-06\n",
            "Epoch: 148 Step: 7000 Loss: 9.327955012849998e-06\n",
            "Epoch: 148 Step: 8000 Loss: 9.767335723154247e-05\n",
            "Epoch: 148 Step: 9000 Loss: 5.0067501433659345e-06\n",
            "Epoch: 148 Step: 10000 Loss: 1.5199148037936538e-06\n",
            "Epoch: 148 Step: 11000 Loss: 5.066392532171449e-07\n",
            "Epoch: 148 Step: 12000 Loss: 4.27333106927108e-05\n",
            "Epoch: 148 Step: 13000 Loss: 0.00017273497360292822\n",
            "Epoch: 148 Step: 14000 Loss: 0.00012921931920573115\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.07158204285446797\n",
            "Epoch: 149 Step: 0 Loss: 1.5199140079857898e-06\n",
            "Epoch: 149 Step: 1000 Loss: 0.0\n",
            "Epoch: 149 Step: 2000 Loss: 8.642660418445303e-07\n",
            "Epoch: 149 Step: 3000 Loss: 9.834757292992435e-07\n",
            "Epoch: 149 Step: 4000 Loss: 5.274981049296912e-06\n",
            "Epoch: 149 Step: 5000 Loss: 1.2814982710551703e-06\n",
            "Epoch: 149 Step: 6000 Loss: 1.4901141867085244e-06\n",
            "Epoch: 149 Step: 7000 Loss: 1.2546463949547615e-05\n",
            "Epoch: 149 Step: 8000 Loss: 0.00011742412607418373\n",
            "Epoch: 149 Step: 9000 Loss: 1.3917305295763072e-05\n",
            "Epoch: 149 Step: 10000 Loss: 4.172323428974778e-07\n",
            "Epoch: 149 Step: 11000 Loss: 0.0005879365489818156\n",
            "Epoch: 149 Step: 12000 Loss: 2.9802316703353426e-07\n",
            "Epoch: 149 Step: 13000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 149 Step: 14000 Loss: 6.556507514687837e-07\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.06575709794977788\n",
            "Epoch: 150 Step: 0 Loss: 0.00020119392138440162\n",
            "Epoch: 150 Step: 1000 Loss: 2.4139824290614342e-06\n",
            "Epoch: 150 Step: 2000 Loss: 3.3406628062948585e-05\n",
            "Epoch: 150 Step: 3000 Loss: 1.4513325368170626e-05\n",
            "Epoch: 150 Step: 4000 Loss: 6.258482585508318e-07\n",
            "Epoch: 150 Step: 5000 Loss: 7.748599273327272e-07\n",
            "Epoch: 150 Step: 6000 Loss: 5.841192432853859e-06\n",
            "Epoch: 150 Step: 7000 Loss: 1.1920928244535389e-07\n",
            "Epoch: 150 Step: 8000 Loss: 0.0\n",
            "Epoch: 150 Step: 9000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 150 Step: 10000 Loss: 2.3095853975974023e-05\n",
            "Epoch: 150 Step: 11000 Loss: 4.142509624216473e-06\n",
            "Epoch: 150 Step: 12000 Loss: 9.536726111036842e-07\n",
            "Epoch: 150 Step: 13000 Loss: 1.1920905080842203e-06\n",
            "Epoch: 150 Step: 14000 Loss: 3.966375516029075e-05\n",
            "Accuracy: 0.9859\n",
            "Test Loss: 0.069508022746146\n",
            "Epoch: 151 Step: 0 Loss: 2.3841853646899835e-07\n",
            "Epoch: 151 Step: 1000 Loss: 9.536724974168465e-07\n",
            "Epoch: 151 Step: 2000 Loss: 1.5407364116981626e-05\n",
            "Epoch: 151 Step: 3000 Loss: 0.0006784502184018493\n",
            "Epoch: 151 Step: 4000 Loss: 5.960459361631365e-07\n",
            "Epoch: 151 Step: 5000 Loss: 1.2516961760411505e-06\n",
            "Epoch: 151 Step: 6000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 151 Step: 7000 Loss: 1.4274926797952503e-05\n",
            "Epoch: 151 Step: 8000 Loss: 2.3185169993666932e-05\n",
            "Epoch: 151 Step: 9000 Loss: 6.332193879643455e-05\n",
            "Epoch: 151 Step: 10000 Loss: 6.556505809385271e-07\n",
            "Epoch: 151 Step: 11000 Loss: 2.205362079621409e-06\n",
            "Epoch: 151 Step: 12000 Loss: 0.00019117190095130354\n",
            "Epoch: 151 Step: 13000 Loss: 0.000798949331510812\n",
            "Epoch: 151 Step: 14000 Loss: 2.017576844082214e-05\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.07598022178809243\n",
            "Epoch: 152 Step: 0 Loss: 3.367651061125798e-06\n",
            "Epoch: 152 Step: 1000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 152 Step: 2000 Loss: 0.0007223178399726748\n",
            "Epoch: 152 Step: 3000 Loss: 8.940696005765858e-08\n",
            "Epoch: 152 Step: 4000 Loss: 5.364413482311647e-07\n",
            "Epoch: 152 Step: 5000 Loss: 6.1392061070364434e-06\n",
            "Epoch: 152 Step: 6000 Loss: 0.0008424139814451337\n",
            "Epoch: 152 Step: 7000 Loss: 8.940696005765858e-08\n",
            "Epoch: 152 Step: 8000 Loss: 0.0017123186262324452\n",
            "Epoch: 152 Step: 9000 Loss: 3.662470044218935e-05\n",
            "Epoch: 152 Step: 10000 Loss: 2.2649667243967997e-06\n",
            "Epoch: 152 Step: 11000 Loss: 6.39781792415306e-05\n",
            "Epoch: 152 Step: 12000 Loss: 0.0\n",
            "Epoch: 152 Step: 13000 Loss: 8.940696005765858e-08\n",
            "Epoch: 152 Step: 14000 Loss: 6.258484290810884e-07\n",
            "Accuracy: 0.9858\n",
            "Test Loss: 0.07678260284840502\n",
            "Epoch: 153 Step: 0 Loss: 0.0001491459843236953\n",
            "Epoch: 153 Step: 1000 Loss: 1.6391259123338386e-06\n",
            "Epoch: 153 Step: 2000 Loss: 1.3112988881402998e-06\n",
            "Epoch: 153 Step: 3000 Loss: 2.384184938364342e-07\n",
            "Epoch: 153 Step: 4000 Loss: 1.192090280710545e-06\n",
            "Epoch: 153 Step: 5000 Loss: 3.8742987840123533e-07\n",
            "Epoch: 153 Step: 6000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 153 Step: 7000 Loss: 3.874300205097825e-07\n",
            "Epoch: 153 Step: 8000 Loss: 3.4272447919647675e-06\n",
            "Epoch: 153 Step: 9000 Loss: 2.9802231438225135e-06\n",
            "Epoch: 153 Step: 10000 Loss: 1.8893961168942042e-05\n",
            "Epoch: 153 Step: 11000 Loss: 6.854529601696413e-07\n",
            "Epoch: 153 Step: 12000 Loss: 7.450573775713565e-07\n",
            "Epoch: 153 Step: 13000 Loss: 9.536728384773596e-07\n",
            "Epoch: 153 Step: 14000 Loss: 7.748598136458895e-07\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.07034625371573457\n",
            "Epoch: 154 Step: 0 Loss: 5.8824858570005745e-05\n",
            "Epoch: 154 Step: 1000 Loss: 0.003326304256916046\n",
            "Epoch: 154 Step: 2000 Loss: 8.046617381296528e-07\n",
            "Epoch: 154 Step: 3000 Loss: 1.1592942428251263e-05\n",
            "Epoch: 154 Step: 4000 Loss: 5.125962161400821e-06\n",
            "Epoch: 154 Step: 5000 Loss: 7.450570365108433e-07\n",
            "Epoch: 154 Step: 6000 Loss: 0.0\n",
            "Epoch: 154 Step: 7000 Loss: 2.9681878004339524e-05\n",
            "Epoch: 154 Step: 8000 Loss: 2.6016095944214612e-05\n",
            "Epoch: 154 Step: 9000 Loss: 7.065222598612309e-05\n",
            "Epoch: 154 Step: 10000 Loss: 3.2782546099952015e-07\n",
            "Epoch: 154 Step: 11000 Loss: 6.4968248807417694e-06\n",
            "Epoch: 154 Step: 12000 Loss: 0.004373608622699976\n",
            "Epoch: 154 Step: 13000 Loss: 0.004468023311346769\n",
            "Epoch: 154 Step: 14000 Loss: 3.4868505736085353e-06\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.068349583208054\n",
            "Epoch: 155 Step: 0 Loss: 2.682207878024201e-07\n",
            "Epoch: 155 Step: 1000 Loss: 1.847737621574197e-06\n",
            "Epoch: 155 Step: 2000 Loss: 0.00014032954641152173\n",
            "Epoch: 155 Step: 3000 Loss: 4.338980943430215e-05\n",
            "Epoch: 155 Step: 4000 Loss: 6.556505240951083e-07\n",
            "Epoch: 155 Step: 5000 Loss: 1.3857888916390948e-05\n",
            "Epoch: 155 Step: 6000 Loss: 7.36605070414953e-05\n",
            "Epoch: 155 Step: 7000 Loss: 3.8742987840123533e-07\n",
            "Epoch: 155 Step: 8000 Loss: 4.172322576323495e-07\n",
            "Epoch: 155 Step: 9000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 155 Step: 10000 Loss: 5.170400254428387e-05\n",
            "Epoch: 155 Step: 11000 Loss: 3.039822559003369e-06\n",
            "Epoch: 155 Step: 12000 Loss: 4.4703443791149766e-07\n",
            "Epoch: 155 Step: 13000 Loss: 2.1457583443407202e-06\n",
            "Epoch: 155 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9868\n",
            "Test Loss: 0.06916598464390511\n",
            "Epoch: 156 Step: 0 Loss: 1.1384253411961254e-05\n",
            "Epoch: 156 Step: 1000 Loss: 0.00015048630302771926\n",
            "Epoch: 156 Step: 2000 Loss: 8.940695295223122e-08\n",
            "Epoch: 156 Step: 3000 Loss: 8.143648301484063e-05\n",
            "Epoch: 156 Step: 4000 Loss: 2.285869598388672\n",
            "Epoch: 156 Step: 5000 Loss: 2.1575991922873072e-05\n",
            "Epoch: 156 Step: 6000 Loss: 2.682207878024201e-07\n",
            "Epoch: 156 Step: 7000 Loss: 6.258479743337375e-07\n",
            "Epoch: 156 Step: 8000 Loss: 2.3543752831756137e-06\n",
            "Epoch: 156 Step: 9000 Loss: 4.139199882047251e-05\n",
            "Epoch: 156 Step: 10000 Loss: 4.410718247527257e-06\n",
            "Epoch: 156 Step: 11000 Loss: 0.0007522402447648346\n",
            "Epoch: 156 Step: 12000 Loss: 1.4007055142428726e-06\n",
            "Epoch: 156 Step: 13000 Loss: 0.061837952584028244\n",
            "Epoch: 156 Step: 14000 Loss: 1.639122501728707e-06\n",
            "Accuracy: 0.9868\n",
            "Test Loss: 0.06676849167519755\n",
            "Epoch: 157 Step: 0 Loss: 5.0365538299956825e-06\n",
            "Epoch: 157 Step: 1000 Loss: 0.0003054382686968893\n",
            "Epoch: 157 Step: 2000 Loss: 8.612762030679733e-06\n",
            "Epoch: 157 Step: 3000 Loss: 3.191697032889351e-05\n",
            "Epoch: 157 Step: 4000 Loss: 1.996749006138998e-06\n",
            "Epoch: 157 Step: 5000 Loss: 4.4703452317662595e-07\n",
            "Epoch: 157 Step: 6000 Loss: 1.6987303297355538e-06\n",
            "Epoch: 157 Step: 7000 Loss: 0.011404872871935368\n",
            "Epoch: 157 Step: 8000 Loss: 4.1723220078893064e-07\n",
            "Epoch: 157 Step: 9000 Loss: 0.012432119809091091\n",
            "Epoch: 157 Step: 10000 Loss: 0.00010115843906532973\n",
            "Epoch: 157 Step: 11000 Loss: 2.503388259356143e-06\n",
            "Epoch: 157 Step: 12000 Loss: 2.4139783363352763e-06\n",
            "Epoch: 157 Step: 13000 Loss: 6.228647180250846e-06\n",
            "Epoch: 157 Step: 14000 Loss: 2.741800471994793e-06\n",
            "Accuracy: 0.9869\n",
            "Test Loss: 0.07436212184900652\n",
            "Epoch: 158 Step: 0 Loss: 0.005629321560263634\n",
            "Epoch: 158 Step: 1000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 158 Step: 2000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 158 Step: 3000 Loss: 3.576276412786683e-07\n",
            "Epoch: 158 Step: 4000 Loss: 0.001850890344940126\n",
            "Epoch: 158 Step: 5000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 158 Step: 6000 Loss: 0.009037233889102936\n",
            "Epoch: 158 Step: 7000 Loss: 4.172323428974778e-07\n",
            "Epoch: 158 Step: 8000 Loss: 1.2814987258025212e-06\n",
            "Epoch: 158 Step: 9000 Loss: 8.161564619513229e-05\n",
            "Epoch: 158 Step: 10000 Loss: 2.3841855067985307e-07\n",
            "Epoch: 158 Step: 11000 Loss: 8.940696005765858e-08\n",
            "Epoch: 158 Step: 12000 Loss: 5.3042505896883085e-05\n",
            "Epoch: 158 Step: 13000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 158 Step: 14000 Loss: 0.00052201485959813\n",
            "Accuracy: 0.9877\n",
            "Test Loss: 0.07814387792969331\n",
            "Epoch: 159 Step: 0 Loss: 2.5897028535837308e-05\n",
            "Epoch: 159 Step: 1000 Loss: 5.960463766996327e-08\n",
            "Epoch: 159 Step: 2000 Loss: 0.0\n",
            "Epoch: 159 Step: 3000 Loss: 1.9043198335566558e-05\n",
            "Epoch: 159 Step: 4000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 159 Step: 5000 Loss: 0.0004535079060588032\n",
            "Epoch: 159 Step: 6000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 159 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 159 Step: 8000 Loss: 0.0003056669665966183\n",
            "Epoch: 159 Step: 9000 Loss: 0.006811690051108599\n",
            "Epoch: 159 Step: 10000 Loss: 7.178349915193394e-05\n",
            "Epoch: 159 Step: 11000 Loss: 3.430021752137691e-05\n",
            "Epoch: 159 Step: 12000 Loss: 1.418550618836889e-05\n",
            "Epoch: 159 Step: 13000 Loss: 4.082885425304994e-06\n",
            "Epoch: 159 Step: 14000 Loss: 3.933878815587377e-06\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.07298032082015796\n",
            "Epoch: 160 Step: 0 Loss: 8.940696005765858e-08\n",
            "Epoch: 160 Step: 1000 Loss: 6.5266440287814476e-06\n",
            "Epoch: 160 Step: 2000 Loss: 7.74859188368282e-07\n",
            "Epoch: 160 Step: 3000 Loss: 5.662437843056978e-07\n",
            "Epoch: 160 Step: 4000 Loss: 0.012220807373523712\n",
            "Epoch: 160 Step: 5000 Loss: 5.36441234544327e-07\n",
            "Epoch: 160 Step: 6000 Loss: 1.582456025062129e-05\n",
            "Epoch: 160 Step: 7000 Loss: 2.980231101901154e-07\n",
            "Epoch: 160 Step: 8000 Loss: 1.7583317912794882e-06\n",
            "Epoch: 160 Step: 9000 Loss: 5.960458793197176e-07\n",
            "Epoch: 160 Step: 10000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 160 Step: 11000 Loss: 0.48161086440086365\n",
            "Epoch: 160 Step: 12000 Loss: 1.6093201793410117e-06\n",
            "Epoch: 160 Step: 13000 Loss: 0.0\n",
            "Epoch: 160 Step: 14000 Loss: 6.556504104082705e-07\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.07108194636872943\n",
            "Epoch: 161 Step: 0 Loss: 6.526658580696676e-06\n",
            "Epoch: 161 Step: 1000 Loss: 9.56636358750984e-06\n",
            "Epoch: 161 Step: 2000 Loss: 0.005885210353881121\n",
            "Epoch: 161 Step: 3000 Loss: 0.006280936300754547\n",
            "Epoch: 161 Step: 4000 Loss: 0.0005913827335461974\n",
            "Epoch: 161 Step: 5000 Loss: 3.0100227377261035e-06\n",
            "Epoch: 161 Step: 6000 Loss: 9.14915381144965e-06\n",
            "Epoch: 161 Step: 7000 Loss: 4.470344947549165e-07\n",
            "Epoch: 161 Step: 8000 Loss: 1.5199148037936538e-06\n",
            "Epoch: 161 Step: 9000 Loss: 4.4703443791149766e-07\n",
            "Epoch: 161 Step: 10000 Loss: 1.0430796919536078e-06\n",
            "Epoch: 161 Step: 11000 Loss: 2.1755622583441436e-06\n",
            "Epoch: 161 Step: 12000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 161 Step: 13000 Loss: 9.834749334913795e-07\n",
            "Epoch: 161 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9881\n",
            "Test Loss: 0.06847363668130622\n",
            "Epoch: 162 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 162 Step: 1000 Loss: 8.940683642322256e-07\n",
            "Epoch: 162 Step: 2000 Loss: 8.940695295223122e-08\n",
            "Epoch: 162 Step: 3000 Loss: 6.505248893518001e-05\n",
            "Epoch: 162 Step: 4000 Loss: 0.00016472325660288334\n",
            "Epoch: 162 Step: 5000 Loss: 4.4014166633132845e-05\n",
            "Epoch: 162 Step: 6000 Loss: 7.211136835394427e-05\n",
            "Epoch: 162 Step: 7000 Loss: 4.3418636778369546e-05\n",
            "Epoch: 162 Step: 8000 Loss: 2.384177605563309e-06\n",
            "Epoch: 162 Step: 9000 Loss: 3.099435616604751e-06\n",
            "Epoch: 162 Step: 10000 Loss: 2.798290370265022e-05\n",
            "Epoch: 162 Step: 11000 Loss: 0.1389685422182083\n",
            "Epoch: 162 Step: 12000 Loss: 5.960463766996327e-08\n",
            "Epoch: 162 Step: 13000 Loss: 0.00029728395747952163\n",
            "Epoch: 162 Step: 14000 Loss: 3.9040819501678925e-06\n",
            "Accuracy: 0.9887\n",
            "Test Loss: 0.0663782063350262\n",
            "Epoch: 163 Step: 0 Loss: 9.834757292992435e-07\n",
            "Epoch: 163 Step: 1000 Loss: 0.00016942850197665393\n",
            "Epoch: 163 Step: 2000 Loss: 1.16228943625174e-06\n",
            "Epoch: 163 Step: 3000 Loss: 0.00016189340385608375\n",
            "Epoch: 163 Step: 4000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 163 Step: 5000 Loss: 2.5927956812665798e-06\n",
            "Epoch: 163 Step: 6000 Loss: 0.006067444570362568\n",
            "Epoch: 163 Step: 7000 Loss: 1.5497172398681869e-06\n",
            "Epoch: 163 Step: 8000 Loss: 4.082901796209626e-06\n",
            "Epoch: 163 Step: 9000 Loss: 1.558615804242436e-05\n",
            "Epoch: 163 Step: 10000 Loss: 0.0\n",
            "Epoch: 163 Step: 11000 Loss: 5.960463766996327e-08\n",
            "Epoch: 163 Step: 12000 Loss: 8.046617381296528e-07\n",
            "Epoch: 163 Step: 13000 Loss: 2.1754856788902543e-05\n",
            "Epoch: 163 Step: 14000 Loss: 0.00036756694316864014\n",
            "Accuracy: 0.9866\n",
            "Test Loss: 0.07062697213405231\n",
            "Epoch: 164 Step: 0 Loss: 0.0014562438009306788\n",
            "Epoch: 164 Step: 1000 Loss: 3.847206971840933e-05\n",
            "Epoch: 164 Step: 2000 Loss: 8.046613970691396e-07\n",
            "Epoch: 164 Step: 3000 Loss: 3.4868485272454564e-06\n",
            "Epoch: 164 Step: 4000 Loss: 2.196359127992764e-05\n",
            "Epoch: 164 Step: 5000 Loss: 3.874300205097825e-07\n",
            "Epoch: 164 Step: 6000 Loss: 0.0006057614809833467\n",
            "Epoch: 164 Step: 7000 Loss: 4.1094175685429946e-05\n",
            "Epoch: 164 Step: 8000 Loss: 6.884266440465581e-06\n",
            "Epoch: 164 Step: 9000 Loss: 3.224423198844306e-05\n",
            "Epoch: 164 Step: 10000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 164 Step: 11000 Loss: 9.238708003067586e-07\n",
            "Epoch: 164 Step: 12000 Loss: 8.880942914402112e-06\n",
            "Epoch: 164 Step: 13000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 164 Step: 14000 Loss: 1.16228943625174e-06\n",
            "Accuracy: 0.9874\n",
            "Test Loss: 0.07654695722529976\n",
            "Epoch: 165 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 165 Step: 1000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 165 Step: 2000 Loss: 0.0\n",
            "Epoch: 165 Step: 3000 Loss: 1.7850958101917058e-05\n",
            "Epoch: 165 Step: 4000 Loss: 2.3841855067985307e-07\n",
            "Epoch: 165 Step: 5000 Loss: 2.5509571059956215e-05\n",
            "Epoch: 165 Step: 6000 Loss: 4.910981806460768e-05\n",
            "Epoch: 165 Step: 7000 Loss: 3.278254325778107e-07\n",
            "Epoch: 165 Step: 8000 Loss: 2.682208162241295e-07\n",
            "Epoch: 165 Step: 9000 Loss: 4.470346652851731e-07\n",
            "Epoch: 165 Step: 10000 Loss: 1.0013380233431235e-05\n",
            "Epoch: 165 Step: 11000 Loss: 8.548865298507735e-05\n",
            "Epoch: 165 Step: 12000 Loss: 0.0007759909494780004\n",
            "Epoch: 165 Step: 13000 Loss: 8.346598770003766e-05\n",
            "Epoch: 165 Step: 14000 Loss: 0.00018521353194955736\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.0676531302811843\n",
            "Epoch: 166 Step: 0 Loss: 0.104097381234169\n",
            "Epoch: 166 Step: 1000 Loss: 7.613367051817477e-05\n",
            "Epoch: 166 Step: 2000 Loss: 7.539875696238596e-06\n",
            "Epoch: 166 Step: 3000 Loss: 8.702198101673275e-06\n",
            "Epoch: 166 Step: 4000 Loss: 0.0015887166373431683\n",
            "Epoch: 166 Step: 5000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 166 Step: 6000 Loss: 0.00010845720680663362\n",
            "Epoch: 166 Step: 7000 Loss: 1.1324858633088297e-06\n",
            "Epoch: 166 Step: 8000 Loss: 0.0\n",
            "Epoch: 166 Step: 9000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 166 Step: 10000 Loss: 0.00014789616398047656\n",
            "Epoch: 166 Step: 11000 Loss: 1.6987268054435845e-06\n",
            "Epoch: 166 Step: 12000 Loss: 4.023286692245165e-06\n",
            "Epoch: 166 Step: 13000 Loss: 2.208295336458832e-05\n",
            "Epoch: 166 Step: 14000 Loss: 0.6644439101219177\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.07422783165173213\n",
            "Epoch: 167 Step: 0 Loss: 0.004318663384765387\n",
            "Epoch: 167 Step: 1000 Loss: 0.0024102989118546247\n",
            "Epoch: 167 Step: 2000 Loss: 0.13331958651542664\n",
            "Epoch: 167 Step: 3000 Loss: 1.5199161680357065e-06\n",
            "Epoch: 167 Step: 4000 Loss: 8.225350029533729e-06\n",
            "Epoch: 167 Step: 5000 Loss: 9.834756156124058e-07\n",
            "Epoch: 167 Step: 6000 Loss: 0.01147921197116375\n",
            "Epoch: 167 Step: 7000 Loss: 2.056353878288064e-06\n",
            "Epoch: 167 Step: 8000 Loss: 2.1457585717143957e-06\n",
            "Epoch: 167 Step: 9000 Loss: 3.874299636663636e-07\n",
            "Epoch: 167 Step: 10000 Loss: 1.5795202443769085e-06\n",
            "Epoch: 167 Step: 11000 Loss: 0.0046831355430185795\n",
            "Epoch: 167 Step: 12000 Loss: 7.71873146732105e-06\n",
            "Epoch: 167 Step: 13000 Loss: 7.808142072462942e-06\n",
            "Epoch: 167 Step: 14000 Loss: 0.0036389462184160948\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.06669577842012354\n",
            "Epoch: 168 Step: 0 Loss: 5.960463766996327e-08\n",
            "Epoch: 168 Step: 1000 Loss: 1.7881387748275301e-07\n",
            "Epoch: 168 Step: 2000 Loss: 0.00011936830560443923\n",
            "Epoch: 168 Step: 3000 Loss: 0.02331513538956642\n",
            "Epoch: 168 Step: 4000 Loss: 8.940695295223122e-08\n",
            "Epoch: 168 Step: 5000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 168 Step: 6000 Loss: 5.6624367061886e-07\n",
            "Epoch: 168 Step: 7000 Loss: 0.03389369696378708\n",
            "Epoch: 168 Step: 8000 Loss: 5.662438979925355e-07\n",
            "Epoch: 168 Step: 9000 Loss: 6.854527896393847e-07\n",
            "Epoch: 168 Step: 10000 Loss: 1.1384238860046025e-05\n",
            "Epoch: 168 Step: 11000 Loss: 6.55650239878014e-07\n",
            "Epoch: 168 Step: 12000 Loss: 2.2708618416800164e-05\n",
            "Epoch: 168 Step: 13000 Loss: 2.354377556912368e-06\n",
            "Epoch: 168 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.07180818980498631\n",
            "Epoch: 169 Step: 0 Loss: 2.4854431103449315e-05\n",
            "Epoch: 169 Step: 1000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 169 Step: 2000 Loss: 5.364415756048402e-07\n",
            "Epoch: 169 Step: 3000 Loss: 0.00018414050282444805\n",
            "Epoch: 169 Step: 4000 Loss: 1.3112993428876507e-06\n",
            "Epoch: 169 Step: 5000 Loss: 2.8908125386806205e-06\n",
            "Epoch: 169 Step: 6000 Loss: 3.069621698159608e-06\n",
            "Epoch: 169 Step: 7000 Loss: 3.874299636663636e-07\n",
            "Epoch: 169 Step: 8000 Loss: 4.595121936290525e-05\n",
            "Epoch: 169 Step: 9000 Loss: 4.708739652414806e-06\n",
            "Epoch: 169 Step: 10000 Loss: 0.00016689799667801708\n",
            "Epoch: 169 Step: 11000 Loss: 9.834758429860813e-07\n",
            "Epoch: 169 Step: 12000 Loss: 8.940695295223122e-08\n",
            "Epoch: 169 Step: 13000 Loss: 0.004203662276268005\n",
            "Epoch: 169 Step: 14000 Loss: 4.172321723672212e-07\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.06929791724015721\n",
            "Epoch: 170 Step: 0 Loss: 0.01203859318047762\n",
            "Epoch: 170 Step: 1000 Loss: 1.7881392011531716e-07\n",
            "Epoch: 170 Step: 2000 Loss: 2.473580480000237e-06\n",
            "Epoch: 170 Step: 3000 Loss: 5.006755600334145e-06\n",
            "Epoch: 170 Step: 4000 Loss: 0.00020442028471734375\n",
            "Epoch: 170 Step: 5000 Loss: 5.9604641222676946e-08\n",
            "Epoch: 170 Step: 6000 Loss: 0.0001557585346745327\n",
            "Epoch: 170 Step: 7000 Loss: 8.995726238936186e-05\n",
            "Epoch: 170 Step: 8000 Loss: 6.25848201707413e-07\n",
            "Epoch: 170 Step: 9000 Loss: 0.00034925068030133843\n",
            "Epoch: 170 Step: 10000 Loss: 5.2451587180257775e-06\n",
            "Epoch: 170 Step: 11000 Loss: 9.29367815842852e-05\n",
            "Epoch: 170 Step: 12000 Loss: 1.2129255082982127e-05\n",
            "Epoch: 170 Step: 13000 Loss: 4.4703443791149766e-07\n",
            "Epoch: 170 Step: 14000 Loss: 0.0001273427769774571\n",
            "Accuracy: 0.9868\n",
            "Test Loss: 0.0748468492940178\n",
            "Epoch: 171 Step: 0 Loss: 9.566382686898578e-06\n",
            "Epoch: 171 Step: 1000 Loss: 3.510580427246168e-05\n",
            "Epoch: 171 Step: 2000 Loss: 3.874299636663636e-07\n",
            "Epoch: 171 Step: 3000 Loss: 0.052424266934394836\n",
            "Epoch: 171 Step: 4000 Loss: 0.00025975139578804374\n",
            "Epoch: 171 Step: 5000 Loss: 1.1205424925719853e-05\n",
            "Epoch: 171 Step: 6000 Loss: 7.152552825573366e-07\n",
            "Epoch: 171 Step: 7000 Loss: 0.0002971319481730461\n",
            "Epoch: 171 Step: 8000 Loss: 8.940696005765858e-08\n",
            "Epoch: 171 Step: 9000 Loss: 0.0\n",
            "Epoch: 171 Step: 10000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 171 Step: 11000 Loss: 5.4950280173216015e-05\n",
            "Epoch: 171 Step: 12000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 171 Step: 13000 Loss: 1.0728819006544654e-06\n",
            "Epoch: 171 Step: 14000 Loss: 1.7881389169360773e-07\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.06512323599402113\n",
            "Epoch: 172 Step: 0 Loss: 7.68888094171416e-06\n",
            "Epoch: 172 Step: 1000 Loss: 2.0265497369109653e-06\n",
            "Epoch: 172 Step: 2000 Loss: 3.111170371994376e-05\n",
            "Epoch: 172 Step: 3000 Loss: 4.9705351557349786e-05\n",
            "Epoch: 172 Step: 4000 Loss: 1.1920901670237072e-06\n",
            "Epoch: 172 Step: 5000 Loss: 2.9802313861182483e-07\n",
            "Epoch: 172 Step: 6000 Loss: 5.408537617768161e-05\n",
            "Epoch: 172 Step: 7000 Loss: 0.0015690767904743552\n",
            "Epoch: 172 Step: 8000 Loss: 3.0188815799192525e-05\n",
            "Epoch: 172 Step: 9000 Loss: 4.27340783062391e-05\n",
            "Epoch: 172 Step: 10000 Loss: 0.00042319222120568156\n",
            "Epoch: 172 Step: 11000 Loss: 1.311299229200813e-06\n",
            "Epoch: 172 Step: 12000 Loss: 8.940696005765858e-08\n",
            "Epoch: 172 Step: 13000 Loss: 0.00019980431534349918\n",
            "Epoch: 172 Step: 14000 Loss: 8.255106877186336e-06\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.06992045662447913\n",
            "Epoch: 173 Step: 0 Loss: 1.7285286730839289e-06\n",
            "Epoch: 173 Step: 1000 Loss: 8.906330913305283e-05\n",
            "Epoch: 173 Step: 2000 Loss: 1.534777220513206e-05\n",
            "Epoch: 173 Step: 3000 Loss: 5.42399857295095e-06\n",
            "Epoch: 173 Step: 4000 Loss: 9.834586307988502e-06\n",
            "Epoch: 173 Step: 5000 Loss: 1.573539339005947e-05\n",
            "Epoch: 173 Step: 6000 Loss: 0.00022877736773807555\n",
            "Epoch: 173 Step: 7000 Loss: 2.2827545762993395e-05\n",
            "Epoch: 173 Step: 8000 Loss: 6.02000591243268e-06\n",
            "Epoch: 173 Step: 9000 Loss: 0.0\n",
            "Epoch: 173 Step: 10000 Loss: 2.3841853646899835e-07\n",
            "Epoch: 173 Step: 11000 Loss: 0.0\n",
            "Epoch: 173 Step: 12000 Loss: 8.135932148434222e-06\n",
            "Epoch: 173 Step: 13000 Loss: 8.940695295223122e-08\n",
            "Epoch: 173 Step: 14000 Loss: 6.854524485788716e-07\n",
            "Accuracy: 0.9879\n",
            "Test Loss: 0.0606534102148446\n",
            "Epoch: 174 Step: 0 Loss: 8.7320158854709e-06\n",
            "Epoch: 174 Step: 1000 Loss: 1.4901124814059585e-06\n",
            "Epoch: 174 Step: 2000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 174 Step: 3000 Loss: 2.589757787063718e-05\n",
            "Epoch: 174 Step: 4000 Loss: 3.8829621189506724e-05\n",
            "Epoch: 174 Step: 5000 Loss: 9.238709139935963e-07\n",
            "Epoch: 174 Step: 6000 Loss: 1.6271793356281705e-05\n",
            "Epoch: 174 Step: 7000 Loss: 9.536736911286425e-07\n",
            "Epoch: 174 Step: 8000 Loss: 1.162288185696525e-06\n",
            "Epoch: 174 Step: 9000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 174 Step: 10000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 174 Step: 11000 Loss: 7.539916623500176e-06\n",
            "Epoch: 174 Step: 12000 Loss: 4.470346652851731e-07\n",
            "Epoch: 174 Step: 13000 Loss: 5.960463766996327e-08\n",
            "Epoch: 174 Step: 14000 Loss: 1.9669471384986537e-06\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.06691660477234465\n",
            "Epoch: 175 Step: 0 Loss: 2.8610072604351444e-06\n",
            "Epoch: 175 Step: 1000 Loss: 2.8578800993273035e-05\n",
            "Epoch: 175 Step: 2000 Loss: 2.7029926059185527e-05\n",
            "Epoch: 175 Step: 3000 Loss: 1.4901121403454454e-06\n",
            "Epoch: 175 Step: 4000 Loss: 8.165702638507355e-06\n",
            "Epoch: 175 Step: 5000 Loss: 2.801402615659754e-06\n",
            "Epoch: 175 Step: 6000 Loss: 0.02554400824010372\n",
            "Epoch: 175 Step: 7000 Loss: 2.3572529244120233e-05\n",
            "Epoch: 175 Step: 8000 Loss: 4.0618935599923134e-05\n",
            "Epoch: 175 Step: 9000 Loss: 9.44724706641864e-06\n",
            "Epoch: 175 Step: 10000 Loss: 1.233788498211652e-05\n",
            "Epoch: 175 Step: 11000 Loss: 1.0430802603877964e-06\n",
            "Epoch: 175 Step: 12000 Loss: 2.7716050681192428e-06\n",
            "Epoch: 175 Step: 13000 Loss: 1.1920928244535389e-07\n",
            "Epoch: 175 Step: 14000 Loss: 0.0009700983646325767\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.06637472548516013\n",
            "Epoch: 176 Step: 0 Loss: 2.2947729121369775e-06\n",
            "Epoch: 176 Step: 1000 Loss: 2.443778839733568e-06\n",
            "Epoch: 176 Step: 2000 Loss: 5.947835961706005e-05\n",
            "Epoch: 176 Step: 3000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 176 Step: 4000 Loss: 2.747645521594677e-05\n",
            "Epoch: 176 Step: 5000 Loss: 2.6822075938071066e-07\n",
            "Epoch: 176 Step: 6000 Loss: 6.854442744952394e-06\n",
            "Epoch: 176 Step: 7000 Loss: 7.510121577070095e-06\n",
            "Epoch: 176 Step: 8000 Loss: 2.0861622829215776e-07\n",
            "Epoch: 176 Step: 9000 Loss: 0.0018269364954903722\n",
            "Epoch: 176 Step: 10000 Loss: 3.1292361200030427e-06\n",
            "Epoch: 176 Step: 11000 Loss: 1.1265045031905174e-05\n",
            "Epoch: 176 Step: 12000 Loss: 7.450573207279376e-07\n",
            "Epoch: 176 Step: 13000 Loss: 2.9802305334669654e-07\n",
            "Epoch: 176 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9876\n",
            "Test Loss: 0.0681548342132796\n",
            "Epoch: 177 Step: 0 Loss: 3.874300205097825e-07\n",
            "Epoch: 177 Step: 1000 Loss: 7.927327715151478e-06\n",
            "Epoch: 177 Step: 2000 Loss: 2.3185226382338442e-05\n",
            "Epoch: 177 Step: 3000 Loss: 1.4305090871857828e-06\n",
            "Epoch: 177 Step: 4000 Loss: 1.9400684323045425e-05\n",
            "Epoch: 177 Step: 5000 Loss: 1.2010047612420749e-05\n",
            "Epoch: 177 Step: 6000 Loss: 5.453797712107189e-06\n",
            "Epoch: 177 Step: 7000 Loss: 0.0008767062681727111\n",
            "Epoch: 177 Step: 8000 Loss: 0.00024024226877372712\n",
            "Epoch: 177 Step: 9000 Loss: 0.00011677631118800491\n",
            "Epoch: 177 Step: 10000 Loss: 0.0003178781480528414\n",
            "Epoch: 177 Step: 11000 Loss: 1.7881389169360773e-07\n",
            "Epoch: 177 Step: 12000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 177 Step: 13000 Loss: 4.082889972778503e-06\n",
            "Epoch: 177 Step: 14000 Loss: 5.036551556258928e-06\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.07305440469265168\n",
            "Epoch: 178 Step: 0 Loss: 0.00010613607446430251\n",
            "Epoch: 178 Step: 1000 Loss: 0.0005096662789583206\n",
            "Epoch: 178 Step: 2000 Loss: 1.1622794772847556e-05\n",
            "Epoch: 178 Step: 3000 Loss: 3.5762775496550603e-07\n",
            "Epoch: 178 Step: 4000 Loss: 1.4305087461252697e-06\n",
            "Epoch: 178 Step: 5000 Loss: 0.0010503148660063744\n",
            "Epoch: 178 Step: 6000 Loss: 1.0341193046770059e-05\n",
            "Epoch: 178 Step: 7000 Loss: 5.066392532171449e-07\n",
            "Epoch: 178 Step: 8000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 178 Step: 9000 Loss: 1.2814681213058066e-05\n",
            "Epoch: 178 Step: 10000 Loss: 0.14352622628211975\n",
            "Epoch: 178 Step: 11000 Loss: 2.384184938364342e-07\n",
            "Epoch: 178 Step: 12000 Loss: 9.238704024028266e-07\n",
            "Epoch: 178 Step: 13000 Loss: 1.2576341759995557e-05\n",
            "Epoch: 178 Step: 14000 Loss: 4.470346368634637e-07\n",
            "Accuracy: 0.9858\n",
            "Test Loss: 0.07512957224267246\n",
            "Epoch: 179 Step: 0 Loss: 0.11027343571186066\n",
            "Epoch: 179 Step: 1000 Loss: 0.00021637008467223495\n",
            "Epoch: 179 Step: 2000 Loss: 4.470346652851731e-07\n",
            "Epoch: 179 Step: 3000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 179 Step: 4000 Loss: 8.093011274468154e-05\n",
            "Epoch: 179 Step: 5000 Loss: 0.0\n",
            "Epoch: 179 Step: 6000 Loss: 0.00010434613068355247\n",
            "Epoch: 179 Step: 7000 Loss: 5.462182525661774e-05\n",
            "Epoch: 179 Step: 8000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 179 Step: 9000 Loss: 4.410721885506064e-06\n",
            "Epoch: 179 Step: 10000 Loss: 5.36441234544327e-07\n",
            "Epoch: 179 Step: 11000 Loss: 1.0430799193272833e-06\n",
            "Epoch: 179 Step: 12000 Loss: 8.493517270835582e-06\n",
            "Epoch: 179 Step: 13000 Loss: 3.1888353078102227e-06\n",
            "Epoch: 179 Step: 14000 Loss: 1.7881387748275301e-07\n",
            "Accuracy: 0.9862\n",
            "Test Loss: 0.07005977130903686\n",
            "Epoch: 180 Step: 0 Loss: 4.529917532636318e-06\n",
            "Epoch: 180 Step: 1000 Loss: 9.834756156124058e-07\n",
            "Epoch: 180 Step: 2000 Loss: 5.960463766996327e-08\n",
            "Epoch: 180 Step: 3000 Loss: 3.7550792058027582e-06\n",
            "Epoch: 180 Step: 4000 Loss: 9.328035048383754e-06\n",
            "Epoch: 180 Step: 5000 Loss: 1.2457060620363336e-05\n",
            "Epoch: 180 Step: 6000 Loss: 3.397445425434853e-06\n",
            "Epoch: 180 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 180 Step: 8000 Loss: 3.5762778338721546e-07\n",
            "Epoch: 180 Step: 9000 Loss: 0.0007734301616437733\n",
            "Epoch: 180 Step: 10000 Loss: 0.00656919227913022\n",
            "Epoch: 180 Step: 11000 Loss: 2.4973302060971037e-05\n",
            "Epoch: 180 Step: 12000 Loss: 1.3380895325099118e-05\n",
            "Epoch: 180 Step: 13000 Loss: 0.16914786398410797\n",
            "Epoch: 180 Step: 14000 Loss: 2.9802320611338473e-08\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.07761189636476967\n",
            "Epoch: 181 Step: 0 Loss: 5.543197403312661e-06\n",
            "Epoch: 181 Step: 1000 Loss: 0.0\n",
            "Epoch: 181 Step: 2000 Loss: 9.150628466159105e-05\n",
            "Epoch: 181 Step: 3000 Loss: 1.9073413568548858e-06\n",
            "Epoch: 181 Step: 4000 Loss: 0.7863250970840454\n",
            "Epoch: 181 Step: 5000 Loss: 1.5795180843269918e-06\n",
            "Epoch: 181 Step: 6000 Loss: 1.218886154674692e-05\n",
            "Epoch: 181 Step: 7000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 181 Step: 8000 Loss: 8.046615107559774e-07\n",
            "Epoch: 181 Step: 9000 Loss: 2.473580480000237e-06\n",
            "Epoch: 181 Step: 10000 Loss: 1.0043190741271246e-05\n",
            "Epoch: 181 Step: 11000 Loss: 7.390877726720646e-06\n",
            "Epoch: 181 Step: 12000 Loss: 6.794874479965074e-06\n",
            "Epoch: 181 Step: 13000 Loss: 2.801408982122666e-06\n",
            "Epoch: 181 Step: 14000 Loss: 2.0861619987044833e-07\n",
            "Accuracy: 0.9869\n",
            "Test Loss: 0.07559887425434242\n",
            "Epoch: 182 Step: 0 Loss: 0.09206859022378922\n",
            "Epoch: 182 Step: 1000 Loss: 5.662435569320223e-07\n",
            "Epoch: 182 Step: 2000 Loss: 8.940695295223122e-08\n",
            "Epoch: 182 Step: 3000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 182 Step: 4000 Loss: 0.005501998122781515\n",
            "Epoch: 182 Step: 5000 Loss: 0.01776631735265255\n",
            "Epoch: 182 Step: 6000 Loss: 0.08596859872341156\n",
            "Epoch: 182 Step: 7000 Loss: 5.274955583445262e-06\n",
            "Epoch: 182 Step: 8000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 182 Step: 9000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 182 Step: 10000 Loss: 7.622272096341476e-05\n",
            "Epoch: 182 Step: 11000 Loss: 4.380922746349825e-06\n",
            "Epoch: 182 Step: 12000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 182 Step: 13000 Loss: 1.4394112440641038e-05\n",
            "Epoch: 182 Step: 14000 Loss: 1.311300820816541e-06\n",
            "Accuracy: 0.9863\n",
            "Test Loss: 0.07988547334842326\n",
            "Epoch: 183 Step: 0 Loss: 1.6003334167180583e-05\n",
            "Epoch: 183 Step: 1000 Loss: 5.960463766996327e-08\n",
            "Epoch: 183 Step: 2000 Loss: 5.2153668548271526e-06\n",
            "Epoch: 183 Step: 3000 Loss: 0.0\n",
            "Epoch: 183 Step: 4000 Loss: 1.4841129996057134e-05\n",
            "Epoch: 183 Step: 5000 Loss: 0.0\n",
            "Epoch: 183 Step: 6000 Loss: 2.103981387335807e-05\n",
            "Epoch: 183 Step: 7000 Loss: 6.854529601696413e-07\n",
            "Epoch: 183 Step: 8000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 183 Step: 9000 Loss: 2.0861619987044833e-07\n",
            "Epoch: 183 Step: 10000 Loss: 1.7285286730839289e-06\n",
            "Epoch: 183 Step: 11000 Loss: 0.0\n",
            "Epoch: 183 Step: 12000 Loss: 0.005969138350337744\n",
            "Epoch: 183 Step: 13000 Loss: 0.13336586952209473\n",
            "Epoch: 183 Step: 14000 Loss: 5.990230420138687e-06\n",
            "Accuracy: 0.9864\n",
            "Test Loss: 0.0732587928019032\n",
            "Epoch: 184 Step: 0 Loss: 8.940696005765858e-08\n",
            "Epoch: 184 Step: 1000 Loss: 0.0627271756529808\n",
            "Epoch: 184 Step: 2000 Loss: 5.960457656328799e-07\n",
            "Epoch: 184 Step: 3000 Loss: 8.940696005765858e-08\n",
            "Epoch: 184 Step: 4000 Loss: 0.00011477965745143592\n",
            "Epoch: 184 Step: 5000 Loss: 6.94384470989462e-06\n",
            "Epoch: 184 Step: 6000 Loss: 9.74517661234131e-06\n",
            "Epoch: 184 Step: 7000 Loss: 0.010212013497948647\n",
            "Epoch: 184 Step: 8000 Loss: 4.917357728118077e-06\n",
            "Epoch: 184 Step: 9000 Loss: 0.08031600713729858\n",
            "Epoch: 184 Step: 10000 Loss: 8.046615107559774e-07\n",
            "Epoch: 184 Step: 11000 Loss: 7.450476459780475e-06\n",
            "Epoch: 184 Step: 12000 Loss: 5.572971986111952e-06\n",
            "Epoch: 184 Step: 13000 Loss: 7.011936395429075e-05\n",
            "Epoch: 184 Step: 14000 Loss: 2.9802305334669654e-07\n",
            "Accuracy: 0.9877\n",
            "Test Loss: 0.07076465470706883\n",
            "Epoch: 185 Step: 0 Loss: 0.0003476114652585238\n",
            "Epoch: 185 Step: 1000 Loss: 3.9040778574417345e-06\n",
            "Epoch: 185 Step: 2000 Loss: 5.932940621278249e-05\n",
            "Epoch: 185 Step: 3000 Loss: 6.0497991398733575e-06\n",
            "Epoch: 185 Step: 4000 Loss: 3.039818466277211e-06\n",
            "Epoch: 185 Step: 5000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 185 Step: 6000 Loss: 6.765079433534993e-06\n",
            "Epoch: 185 Step: 7000 Loss: 1.8775406260829186e-06\n",
            "Epoch: 185 Step: 8000 Loss: 0.0003886545018758625\n",
            "Epoch: 185 Step: 9000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 185 Step: 10000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 185 Step: 11000 Loss: 0.0006572353886440396\n",
            "Epoch: 185 Step: 12000 Loss: 7.18230830898392e-06\n",
            "Epoch: 185 Step: 13000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 185 Step: 14000 Loss: 3.397442469577072e-06\n",
            "Accuracy: 0.9871\n",
            "Test Loss: 0.0798808071644477\n",
            "Epoch: 186 Step: 0 Loss: 9.23870629776502e-07\n",
            "Epoch: 186 Step: 1000 Loss: 9.308542212238535e-05\n",
            "Epoch: 186 Step: 2000 Loss: 3.087329969275743e-05\n",
            "Epoch: 186 Step: 3000 Loss: 0.0009547767112962902\n",
            "Epoch: 186 Step: 4000 Loss: 2.792344821500592e-05\n",
            "Epoch: 186 Step: 5000 Loss: 5.363846503314562e-05\n",
            "Epoch: 186 Step: 6000 Loss: 6.258479743337375e-07\n",
            "Epoch: 186 Step: 7000 Loss: 5.396704364102334e-05\n",
            "Epoch: 186 Step: 8000 Loss: 0.0014352265279740095\n",
            "Epoch: 186 Step: 9000 Loss: 8.940696005765858e-08\n",
            "Epoch: 186 Step: 10000 Loss: 1.1920901670237072e-06\n",
            "Epoch: 186 Step: 11000 Loss: 3.3974415600823704e-06\n",
            "Epoch: 186 Step: 12000 Loss: 0.004859652370214462\n",
            "Epoch: 186 Step: 13000 Loss: 2.503383711882634e-06\n",
            "Epoch: 186 Step: 14000 Loss: 2.2351664483721834e-06\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.07426474654729484\n",
            "Epoch: 187 Step: 0 Loss: 5.960463766996327e-08\n",
            "Epoch: 187 Step: 1000 Loss: 4.529918896878371e-06\n",
            "Epoch: 187 Step: 2000 Loss: 6.854524485788716e-07\n",
            "Epoch: 187 Step: 3000 Loss: 3.1292261155613232e-06\n",
            "Epoch: 187 Step: 4000 Loss: 2.3334139768849127e-05\n",
            "Epoch: 187 Step: 5000 Loss: 2.5212068067048676e-05\n",
            "Epoch: 187 Step: 6000 Loss: 1.844696635089349e-05\n",
            "Epoch: 187 Step: 7000 Loss: 1.4603122053813422e-06\n",
            "Epoch: 187 Step: 8000 Loss: 4.7683693082944956e-07\n",
            "Epoch: 187 Step: 9000 Loss: 4.172321723672212e-07\n",
            "Epoch: 187 Step: 10000 Loss: 0.0032267875503748655\n",
            "Epoch: 187 Step: 11000 Loss: 2.980231101901154e-07\n",
            "Epoch: 187 Step: 12000 Loss: 4.282242662156932e-05\n",
            "Epoch: 187 Step: 13000 Loss: 2.1755618035967927e-06\n",
            "Epoch: 187 Step: 14000 Loss: 4.449151310836896e-05\n",
            "Accuracy: 0.9863\n",
            "Test Loss: 0.08350290823670449\n",
            "Epoch: 188 Step: 0 Loss: 0.0006410750211216509\n",
            "Epoch: 188 Step: 1000 Loss: 0.00015245217946358025\n",
            "Epoch: 188 Step: 2000 Loss: 0.02827070839703083\n",
            "Epoch: 188 Step: 3000 Loss: 3.7252757465466857e-06\n",
            "Epoch: 188 Step: 4000 Loss: 5.572973350354005e-06\n",
            "Epoch: 188 Step: 5000 Loss: 7.450573207279376e-07\n",
            "Epoch: 188 Step: 6000 Loss: 0.0020973370410501957\n",
            "Epoch: 188 Step: 7000 Loss: 1.1056539733544923e-05\n",
            "Epoch: 188 Step: 8000 Loss: 5.066391395303071e-07\n",
            "Epoch: 188 Step: 9000 Loss: 2.3543766474176664e-06\n",
            "Epoch: 188 Step: 10000 Loss: 1.1920926112907182e-07\n",
            "Epoch: 188 Step: 11000 Loss: 3.159026618959615e-06\n",
            "Epoch: 188 Step: 12000 Loss: 7.152551688704989e-07\n",
            "Epoch: 188 Step: 13000 Loss: 2.0563520592986606e-06\n",
            "Epoch: 188 Step: 14000 Loss: 9.953812877938617e-06\n",
            "Accuracy: 0.9865\n",
            "Test Loss: 0.07860011020588141\n",
            "Epoch: 189 Step: 0 Loss: 2.9802316703353426e-07\n",
            "Epoch: 189 Step: 1000 Loss: 8.106105269689579e-06\n",
            "Epoch: 189 Step: 2000 Loss: 5.960457656328799e-07\n",
            "Epoch: 189 Step: 3000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 189 Step: 4000 Loss: 1.1324859769956674e-06\n",
            "Epoch: 189 Step: 5000 Loss: 0.0004327519563958049\n",
            "Epoch: 189 Step: 6000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 189 Step: 7000 Loss: 0.00010708683839766309\n",
            "Epoch: 189 Step: 8000 Loss: 1.4901117992849322e-06\n",
            "Epoch: 189 Step: 9000 Loss: 6.288219083216973e-06\n",
            "Epoch: 189 Step: 10000 Loss: 8.344636626134161e-07\n",
            "Epoch: 189 Step: 11000 Loss: 8.939100371208042e-05\n",
            "Epoch: 189 Step: 12000 Loss: 0.00017239191220141947\n",
            "Epoch: 189 Step: 13000 Loss: 1.16228943625174e-06\n",
            "Epoch: 189 Step: 14000 Loss: 1.73744629137218e-05\n",
            "Accuracy: 0.9851\n",
            "Test Loss: 0.08403251287967109\n",
            "Epoch: 190 Step: 0 Loss: 0.020654914900660515\n",
            "Epoch: 190 Step: 1000 Loss: 0.0023823059163987637\n",
            "Epoch: 190 Step: 2000 Loss: 1.281497588934144e-06\n",
            "Epoch: 190 Step: 3000 Loss: 3.9455695514334366e-05\n",
            "Epoch: 190 Step: 4000 Loss: 1.4305077229437302e-06\n",
            "Epoch: 190 Step: 5000 Loss: 7.033262590994127e-06\n",
            "Epoch: 190 Step: 6000 Loss: 3.0545568733941764e-05\n",
            "Epoch: 190 Step: 7000 Loss: 3.0247527320170775e-05\n",
            "Epoch: 190 Step: 8000 Loss: 0.0038945877458900213\n",
            "Epoch: 190 Step: 9000 Loss: 7.74859188368282e-07\n",
            "Epoch: 190 Step: 10000 Loss: 0.07518107444047928\n",
            "Epoch: 190 Step: 11000 Loss: 0.00017121530254371464\n",
            "Epoch: 190 Step: 12000 Loss: 2.980215185743873e-06\n",
            "Epoch: 190 Step: 13000 Loss: 8.940695295223122e-08\n",
            "Epoch: 190 Step: 14000 Loss: 1.4751723028894048e-05\n",
            "Accuracy: 0.9857\n",
            "Test Loss: 0.08867897386380928\n",
            "Epoch: 191 Step: 0 Loss: 4.142499619774753e-06\n",
            "Epoch: 191 Step: 1000 Loss: 0.06747313588857651\n",
            "Epoch: 191 Step: 2000 Loss: 5.960462203802308e-07\n",
            "Epoch: 191 Step: 3000 Loss: 1.1682276635838207e-05\n",
            "Epoch: 191 Step: 4000 Loss: 0.000433758192230016\n",
            "Epoch: 191 Step: 5000 Loss: 3.278254325778107e-07\n",
            "Epoch: 191 Step: 6000 Loss: 2.562986537668621e-06\n",
            "Epoch: 191 Step: 7000 Loss: 2.458606286381837e-05\n",
            "Epoch: 191 Step: 8000 Loss: 0.00038945136475376785\n",
            "Epoch: 191 Step: 9000 Loss: 1.7881337726066704e-06\n",
            "Epoch: 191 Step: 10000 Loss: 0.0\n",
            "Epoch: 191 Step: 11000 Loss: 2.6822075938071066e-07\n",
            "Epoch: 191 Step: 12000 Loss: 0.0001519161305623129\n",
            "Epoch: 191 Step: 13000 Loss: 6.0497991398733575e-06\n",
            "Epoch: 191 Step: 14000 Loss: 0.0017373828450217843\n",
            "Accuracy: 0.987\n",
            "Test Loss: 0.07060158781081373\n",
            "Epoch: 192 Step: 0 Loss: 1.931115912157111e-05\n",
            "Epoch: 192 Step: 1000 Loss: 8.046615107559774e-07\n",
            "Epoch: 192 Step: 2000 Loss: 0.0\n",
            "Epoch: 192 Step: 3000 Loss: 3.27825318890973e-07\n",
            "Epoch: 192 Step: 4000 Loss: 9.059765943675302e-06\n",
            "Epoch: 192 Step: 5000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 192 Step: 6000 Loss: 1.1920927533992653e-07\n",
            "Epoch: 192 Step: 7000 Loss: 2.4735863917157985e-06\n",
            "Epoch: 192 Step: 8000 Loss: 3.2782537573439186e-07\n",
            "Epoch: 192 Step: 9000 Loss: 1.6391244344049483e-06\n",
            "Epoch: 192 Step: 10000 Loss: 2.0861619987044833e-07\n",
            "Epoch: 192 Step: 11000 Loss: 8.940695295223122e-08\n",
            "Epoch: 192 Step: 12000 Loss: 0.0\n",
            "Epoch: 192 Step: 13000 Loss: 0.00038171469350345433\n",
            "Epoch: 192 Step: 14000 Loss: 2.920610768342158e-06\n",
            "Accuracy: 0.9861\n",
            "Test Loss: 0.08363538180666669\n",
            "Epoch: 193 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 193 Step: 1000 Loss: 2.050315924861934e-05\n",
            "Epoch: 193 Step: 2000 Loss: 5.900814358028583e-06\n",
            "Epoch: 193 Step: 3000 Loss: 1.6689248241164023e-06\n",
            "Epoch: 193 Step: 4000 Loss: 1.7881390590446244e-07\n",
            "Epoch: 193 Step: 5000 Loss: 3.945528806070797e-05\n",
            "Epoch: 193 Step: 6000 Loss: 5.066391395303071e-07\n",
            "Epoch: 193 Step: 7000 Loss: 1.4901156930591242e-07\n",
            "Epoch: 193 Step: 8000 Loss: 1.883437107608188e-05\n",
            "Epoch: 193 Step: 9000 Loss: 2.139715252269525e-05\n",
            "Epoch: 193 Step: 10000 Loss: 1.2576418157550506e-05\n",
            "Epoch: 193 Step: 11000 Loss: 0.00031133799348026514\n",
            "Epoch: 193 Step: 12000 Loss: 1.6689247104295646e-06\n",
            "Epoch: 193 Step: 13000 Loss: 3.278254041561013e-07\n",
            "Epoch: 193 Step: 14000 Loss: 7.301475307031069e-06\n",
            "Accuracy: 0.9882\n",
            "Test Loss: 0.08102493377970961\n",
            "Epoch: 194 Step: 0 Loss: 0.0018082876922562718\n",
            "Epoch: 194 Step: 1000 Loss: 0.0002091249916702509\n",
            "Epoch: 194 Step: 2000 Loss: 1.1920909628315712e-06\n",
            "Epoch: 194 Step: 3000 Loss: 0.0004725464968942106\n",
            "Epoch: 194 Step: 4000 Loss: 2.3841846541472478e-07\n",
            "Epoch: 194 Step: 5000 Loss: 8.344642310476047e-07\n",
            "Epoch: 194 Step: 6000 Loss: 0.00014155554526951164\n",
            "Epoch: 194 Step: 7000 Loss: 9.000144018500578e-06\n",
            "Epoch: 194 Step: 8000 Loss: 1.698729420240852e-06\n",
            "Epoch: 194 Step: 9000 Loss: 8.94055665412452e-06\n",
            "Epoch: 194 Step: 10000 Loss: 5.960463766996327e-08\n",
            "Epoch: 194 Step: 11000 Loss: 6.55650637781946e-07\n",
            "Epoch: 194 Step: 12000 Loss: 0.0\n",
            "Epoch: 194 Step: 13000 Loss: 5.79588049731683e-05\n",
            "Epoch: 194 Step: 14000 Loss: 1.3112988881402998e-06\n",
            "Accuracy: 0.986\n",
            "Test Loss: 0.08415875737076174\n",
            "Epoch: 195 Step: 0 Loss: 2.9802320611338473e-08\n",
            "Epoch: 195 Step: 1000 Loss: 0.00019134754256810993\n",
            "Epoch: 195 Step: 2000 Loss: 7.450572070410999e-07\n",
            "Epoch: 195 Step: 3000 Loss: 2.8698208552668802e-05\n",
            "Epoch: 195 Step: 4000 Loss: 0.0003937370784115046\n",
            "Epoch: 195 Step: 5000 Loss: 8.940696005765858e-08\n",
            "Epoch: 195 Step: 6000 Loss: 1.084793075278867e-05\n",
            "Epoch: 195 Step: 7000 Loss: 4.172323713191872e-07\n",
            "Epoch: 195 Step: 8000 Loss: 9.804828550841194e-06\n",
            "Epoch: 195 Step: 9000 Loss: 8.940684779190633e-07\n",
            "Epoch: 195 Step: 10000 Loss: 0.043483417481184006\n",
            "Epoch: 195 Step: 11000 Loss: 1.668926984166319e-06\n",
            "Epoch: 195 Step: 12000 Loss: 4.470345800200448e-07\n",
            "Epoch: 195 Step: 13000 Loss: 5.7219835980504286e-06\n",
            "Epoch: 195 Step: 14000 Loss: 0.00025357911363244057\n",
            "Accuracy: 0.9856\n",
            "Test Loss: 0.08375079123391496\n",
            "Epoch: 196 Step: 0 Loss: 0.0006050005904398859\n",
            "Epoch: 196 Step: 1000 Loss: 3.576276128569589e-07\n",
            "Epoch: 196 Step: 2000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 196 Step: 3000 Loss: 1.1324860906825052e-06\n",
            "Epoch: 196 Step: 4000 Loss: 3.3080361845350126e-06\n",
            "Epoch: 196 Step: 5000 Loss: 1.406630235578632e-05\n",
            "Epoch: 196 Step: 6000 Loss: 1.1324856359351543e-06\n",
            "Epoch: 196 Step: 7000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 196 Step: 8000 Loss: 0.005761449225246906\n",
            "Epoch: 196 Step: 9000 Loss: 3.993495738541242e-06\n",
            "Epoch: 196 Step: 10000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 196 Step: 11000 Loss: 1.6987268054435845e-06\n",
            "Epoch: 196 Step: 12000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 196 Step: 13000 Loss: 0.008503129705786705\n",
            "Epoch: 196 Step: 14000 Loss: 1.9669455468829256e-06\n",
            "Accuracy: 0.9873\n",
            "Test Loss: 0.07857623961246218\n",
            "Epoch: 197 Step: 0 Loss: 2.6822075938071066e-07\n",
            "Epoch: 197 Step: 1000 Loss: 2.3841852225814364e-07\n",
            "Epoch: 197 Step: 2000 Loss: 2.9802320611338473e-08\n",
            "Epoch: 197 Step: 3000 Loss: 6.854530170130602e-07\n",
            "Epoch: 197 Step: 4000 Loss: 2.0324405340943485e-05\n",
            "Epoch: 197 Step: 5000 Loss: 9.536734069115482e-07\n",
            "Epoch: 197 Step: 6000 Loss: 2.6822084464583895e-07\n",
            "Epoch: 197 Step: 7000 Loss: 5.364415187614213e-07\n",
            "Epoch: 197 Step: 8000 Loss: 5.781596428278135e-06\n",
            "Epoch: 197 Step: 9000 Loss: 3.84420454793144e-05\n",
            "Epoch: 197 Step: 10000 Loss: 0.003480682848021388\n",
            "Epoch: 197 Step: 11000 Loss: 0.0\n",
            "Epoch: 197 Step: 12000 Loss: 0.0001058362249750644\n",
            "Epoch: 197 Step: 13000 Loss: 0.0001394683204125613\n",
            "Epoch: 197 Step: 14000 Loss: 2.6641857402864844e-05\n",
            "Accuracy: 0.9867\n",
            "Test Loss: 0.07463093636192963\n",
            "Epoch: 198 Step: 0 Loss: 5.960463766996327e-08\n",
            "Epoch: 198 Step: 1000 Loss: 0.005352339241653681\n",
            "Epoch: 198 Step: 2000 Loss: 0.0415753498673439\n",
            "Epoch: 198 Step: 3000 Loss: 2.175562485717819e-06\n",
            "Epoch: 198 Step: 4000 Loss: 1.1026630090782419e-05\n",
            "Epoch: 198 Step: 5000 Loss: 2.5629885840317e-06\n",
            "Epoch: 198 Step: 6000 Loss: 7.74859188368282e-07\n",
            "Epoch: 198 Step: 7000 Loss: 3.4570455227367347e-06\n",
            "Epoch: 198 Step: 8000 Loss: 8.316445018863305e-05\n",
            "Epoch: 198 Step: 9000 Loss: 4.4730470108333975e-05\n",
            "Epoch: 198 Step: 10000 Loss: 5.960463766996327e-08\n",
            "Epoch: 198 Step: 11000 Loss: 8.106119821604807e-06\n",
            "Epoch: 198 Step: 12000 Loss: 2.8549786293297075e-05\n",
            "Epoch: 198 Step: 13000 Loss: 3.486630885163322e-05\n",
            "Epoch: 198 Step: 14000 Loss: 1.7881333178593195e-06\n",
            "Accuracy: 0.9886\n",
            "Test Loss: 0.07540240034723883\n",
            "Epoch: 199 Step: 0 Loss: 2.4735807073739124e-06\n",
            "Epoch: 199 Step: 1000 Loss: 0.02505318820476532\n",
            "Epoch: 199 Step: 2000 Loss: 0.0011100237024948\n",
            "Epoch: 199 Step: 3000 Loss: 0.0\n",
            "Epoch: 199 Step: 4000 Loss: 1.4901158351676713e-07\n",
            "Epoch: 199 Step: 5000 Loss: 2.712001787585905e-06\n",
            "Epoch: 199 Step: 6000 Loss: 2.4436907551717013e-05\n",
            "Epoch: 199 Step: 7000 Loss: 8.940696005765858e-08\n",
            "Epoch: 199 Step: 8000 Loss: 1.7881389169360773e-07\n",
            "Epoch: 199 Step: 9000 Loss: 1.519914349046303e-06\n",
            "Epoch: 199 Step: 10000 Loss: 3.5762769812208717e-07\n",
            "Epoch: 199 Step: 11000 Loss: 5.662389867211459e-06\n",
            "Epoch: 199 Step: 12000 Loss: 3.844470029434888e-06\n",
            "Epoch: 199 Step: 13000 Loss: 8.940696005765858e-08\n",
            "Epoch: 199 Step: 14000 Loss: 0.0\n",
            "Accuracy: 0.9874\n",
            "Test Loss: 0.06953058567988657\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5,self).__init__()\n",
        "        self.conv1=nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
        "        self.sub2=nn.AvgPool2d(kernel_size=2,stride=2)\n",
        "        self.conv3=nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5)\n",
        "        self.sub4=nn.AvgPool2d(kernel_size=2,stride=2)\n",
        "        self.conv5=nn.Conv2d(in_channels=16,out_channels=120,kernel_size=4)\n",
        "        self.fc6=nn.Linear(120,84)\n",
        "        self.fc7=nn.Linear(84,10)\n",
        "        self.tanh=nn.Tanh()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.tanh(x)\n",
        "        x=self.sub2(x)\n",
        "\n",
        "\n",
        "        x=self.conv3(x)\n",
        "        x=self.tanh(x)\n",
        "        x=self.sub4(x)\n",
        "\n",
        "\n",
        "        x=self.conv5(x)\n",
        "        x=self.tanh(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "        x=self.fc6(x)\n",
        "        x=self.tanh(x)\n",
        "        x=self.fc7(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "if __name__=='__main__':\n",
        "    transform=transforms.ToTensor()\n",
        "    train=datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "    test=datasets.MNIST(root='./data',train=False,download=True,transform=transform)\n",
        "    train_dataloader=DataLoader(train,\n",
        "               batch_size=4,\n",
        "               shuffle=True,)\n",
        "    test_dataloader=DataLoader(test,\n",
        "               batch_size=4,\n",
        "               shuffle=True)\n",
        "\n",
        "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model=LeNet5().to(device)\n",
        "    optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "    loss=torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_score=0\n",
        "    best_loss = 100\n",
        "    for epoch in range(200):\n",
        "        for i,(data,label) in enumerate(train_dataloader):\n",
        "            data=data.to(device)\n",
        "            label=label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output=model(data)\n",
        "            loss_val=loss(output,label)\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i%1000==0:\n",
        "                print('Epoch:',epoch,'Step:',i,'Loss:',loss_val.item())\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  #     \n",
        "            for data, labels in test_dataloader:\n",
        "                data=data.to(device)\n",
        "                labels=labels.to(device)\n",
        "                outputs = model(data)\n",
        "                loss_test = loss(outputs, labels)\n",
        "                eval_loss += loss_test.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print('Accuracy:',correct/len(test))\n",
        "            test_loss=eval_loss/len(test_dataloader)\n",
        "            print('Test Loss:',test_loss)\n",
        "            if best_loss>test_loss:\n",
        "              best_loss=test_loss\n",
        "              torch.save(model.state_dict(), './model_best.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozIq7eq7S6JB"
      },
      "outputs": [],
      "source": [
        " model=LeNet5().to(device)\n",
        " model.load_state_dict(torch.load('./model_best.pth'))\n",
        " model.eval()\n",
        "\n",
        " with torch.no_grad():  #     \n",
        "    for data, labels in test_dataloader:\n",
        "        data=data.to(device)\n",
        "        labels=labels.to(device)\n",
        "        outputs = model(data)\n",
        "        loss_test = loss(outputs, labels)\n",
        "        eval_loss += loss_test.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print('Accuracy:',correct/len(test))\n",
        "    test_loss=eval_loss/len(test_dataloader)\n",
        "    print('Test Loss:',test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAjstWPCaV6P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
