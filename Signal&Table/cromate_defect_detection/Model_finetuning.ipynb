{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536c3380",
   "metadata": {},
   "source": [
    "### Best Model 2개\n",
    "- Autoencoder \n",
    "- Deep SVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a81dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from model.AE_anbormaly_detection import Autoencoder\n",
    "from model.Deep_SVDD import DeepSVDD\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c33b84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('E:/glass_git/ML-DL/Signal&Table/data/cromate/preped/df_merged.csv')\n",
    "\n",
    "features=['pH','Temp','Voltage','run_time','month','day','hour','minutes','seconds']\n",
    "target=['class']\n",
    "\n",
    "normal_df=df[df['class']==0]\n",
    "abnormal_df=df[df['class']==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a01c4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)  # For reproducibility\n",
    "def sampling(df,normal_df,abnormal_df):\n",
    "    #test에 사용할 normal data sampling\n",
    "    test_lot_index=random.sample(normal_df.Lot.unique().tolist(),9)\n",
    "    test_lot_index+=abnormal_df.Lot.unique().tolist()\n",
    "\n",
    "    #train test split, test set에 normal Lot 9개, abnormal Lot 9개(전체)\n",
    "    train_df=normal_df[~normal_df['Lot'].isin(test_lot_index)]\n",
    "    test_df=df[df['Lot'].isin(test_lot_index)]\n",
    "\n",
    "    #scaled data\n",
    "    scaler=StandardScaler()\n",
    "    scaled_x_train=scaler.fit_transform(train_df[features])\n",
    "    scaled_x_test=scaler.transform(test_df[features])\n",
    "    return scaled_x_train,scaled_x_test,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b800247d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepsvdd_train(scaled_x_train, scaled_x_test, test_df):\n",
    "    scaled_x_train = torch.tensor(scaled_x_train, dtype=torch.float32)  # 변환\n",
    "    train_loader = DataLoader(TensorDataset(scaled_x_train), batch_size=256, shuffle=False)\n",
    "    X_test = torch.tensor(scaled_x_test, dtype=torch.float32)\n",
    "    # ---- Choose objective ----\n",
    "    deepsvdd = DeepSVDD(in_dim=9, rep_dim=16, objective=\"one-class\")             # hard-boundary\n",
    "    # deepsvdd = DeepSVDD(in_dim=9, rep_dim=16, objective=\"soft-boundary\", nu=0.05)  # soft-boundary\n",
    "\n",
    "    deepsvdd.fit(train_loader, lr=1e-3, weight_decay=1e-6, epochs=50, R_update_freq=5)\n",
    "    if deepsvdd.objective == \"one-class\":\n",
    "        # 학습 분포의 95% 분위수를 임계값으로\n",
    "        with torch.no_grad():\n",
    "                s_train = deepsvdd.score(scaled_x_train)\n",
    "        thr = torch.quantile(s_train, 0.97).item()\n",
    "        y_pred, _ = deepsvdd.predict(X_test, threshold=thr)\n",
    "        deepsvdd_pred= y_pred\n",
    "\n",
    "    deepsvdd_thr_pred = (deepsvdd_pred > thr).int()\n",
    "    acc=accuracy_score(test_df[target], deepsvdd_thr_pred)\n",
    "    return acc*100\n",
    "\n",
    "def autoencoder_train(scaled_x_train, scaled_x_test,test_df):\n",
    "    #Autoencoder\n",
    "    scaled_x_train = torch.tensor(scaled_x_train, dtype=torch.float32)  # 변환\n",
    "    train_loader = DataLoader(TensorDataset(scaled_x_train), batch_size=256, shuffle=False)\n",
    "    model = Autoencoder()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(50):\n",
    "        for i,data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data[0])\n",
    "            loss = criterion(outputs, data[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    #reconstruciont error\n",
    "    X_test = torch.tensor(scaled_x_test, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        recon = model(X_test)\n",
    "        errors = torch.mean((X_test - recon) ** 2, dim=1)  # MSE per sample\n",
    "\n",
    "    # threhold (정상 데이터 기준 99% 분위수)\n",
    "    threshold = np.percentile(errors[:len(scaled_x_train)], 85)\n",
    "    AE_pred = (errors > threshold).int()\n",
    "\n",
    "    acc=accuracy_score(test_df[target], AE_pred)\n",
    "    return acc*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd0397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/50] loss=0.011447\n",
      "[002/50] loss=0.005159\n",
      "[003/50] loss=0.002936\n",
      "[004/50] loss=0.001901\n",
      "[005/50] loss=0.001335\n",
      "[006/50] loss=0.001048\n",
      "[007/50] loss=0.000878\n",
      "[008/50] loss=0.000736\n",
      "[009/50] loss=0.000629\n",
      "[010/50] loss=0.000553\n",
      "[011/50] loss=0.000498\n",
      "[012/50] loss=0.000454\n",
      "[013/50] loss=0.000419\n",
      "[014/50] loss=0.000388\n",
      "[015/50] loss=0.000364\n",
      "[016/50] loss=0.000349\n",
      "[017/50] loss=0.000339\n",
      "[018/50] loss=0.000326\n",
      "[019/50] loss=0.000301\n",
      "[020/50] loss=0.000261\n",
      "[021/50] loss=0.000224\n",
      "[022/50] loss=0.000202\n",
      "[023/50] loss=0.000202\n",
      "[024/50] loss=0.000208\n",
      "[025/50] loss=0.000194\n",
      "[026/50] loss=0.000182\n",
      "[027/50] loss=0.000166\n",
      "[028/50] loss=0.000125\n",
      "[029/50] loss=0.000139\n",
      "[030/50] loss=0.000185\n",
      "[031/50] loss=0.000285\n",
      "[032/50] loss=0.000140\n",
      "[033/50] loss=0.000039\n",
      "[034/50] loss=0.000060\n",
      "[035/50] loss=0.000088\n",
      "[036/50] loss=0.000077\n",
      "[037/50] loss=0.000102\n",
      "[038/50] loss=0.000101\n",
      "[039/50] loss=0.000051\n",
      "[040/50] loss=0.000075\n",
      "[041/50] loss=0.000069\n",
      "[042/50] loss=0.000115\n",
      "[043/50] loss=0.000141\n",
      "[044/50] loss=0.000076\n",
      "[045/50] loss=0.000041\n",
      "[046/50] loss=0.000033\n",
      "[047/50] loss=0.000037\n",
      "[048/50] loss=0.000043\n",
      "[049/50] loss=0.000059\n",
      "[050/50] loss=0.000104\n",
      "[001/50] loss=0.011398\n",
      "[002/50] loss=0.006207\n",
      "[003/50] loss=0.003254\n",
      "[004/50] loss=0.002452\n",
      "[005/50] loss=0.001908\n",
      "[006/50] loss=0.001469\n",
      "[007/50] loss=0.001134\n",
      "[008/50] loss=0.000851\n",
      "[009/50] loss=0.000703\n",
      "[010/50] loss=0.000626\n",
      "[011/50] loss=0.000534\n",
      "[012/50] loss=0.000418\n",
      "[013/50] loss=0.000301\n",
      "[014/50] loss=0.000228\n",
      "[015/50] loss=0.000197\n",
      "[016/50] loss=0.000163\n",
      "[017/50] loss=0.000158\n",
      "[018/50] loss=0.000190\n",
      "[019/50] loss=0.000224\n",
      "[020/50] loss=0.000237\n",
      "[021/50] loss=0.000239\n",
      "[022/50] loss=0.000233\n",
      "[023/50] loss=0.000216\n",
      "[024/50] loss=0.000216\n",
      "[025/50] loss=0.000229\n",
      "[026/50] loss=0.000241\n",
      "[027/50] loss=0.000243\n",
      "[028/50] loss=0.000301\n",
      "[029/50] loss=0.000363\n",
      "[030/50] loss=0.000424\n",
      "[031/50] loss=0.000614\n",
      "[032/50] loss=0.000810\n",
      "[033/50] loss=0.000593\n",
      "[034/50] loss=0.000210\n",
      "[035/50] loss=0.000131\n",
      "[036/50] loss=0.000103\n",
      "[037/50] loss=0.000092\n",
      "[038/50] loss=0.000087\n",
      "[039/50] loss=0.000082\n",
      "[040/50] loss=0.000073\n",
      "[041/50] loss=0.000066\n",
      "[042/50] loss=0.000062\n",
      "[043/50] loss=0.000059\n",
      "[044/50] loss=0.000069\n"
     ]
    }
   ],
   "source": [
    "reuslts={'deepsvdd':[], 'autoencoder':[]}\n",
    "for i in range(10): \n",
    "    scaled_x_train,scaled_x_test,test_df=sampling(df,normal_df,abnormal_df)\n",
    "    deepsvdd_acc=deepsvdd_train(scaled_x_train, scaled_x_test, test_df)\n",
    "    ae_acc=autoencoder_train(scaled_x_train, scaled_x_test,test_df)\n",
    "    reuslts['deepsvdd'].append(deepsvdd_acc)\n",
    "    reuslts['autoencoder'].append(ae_acc)\n",
    "reuslts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glass_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
